{
    "rjhuang__my_wb_preferences_default": {
        "name": "rjhuang/my_wb_preferences",
        "subset": "default",
        "input": "content",
        "label": "response",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 417
    },
    "KBLab__overlim_stsb_da": {
        "name": "KBLab/overlim",
        "subset": "stsb_da",
        "input": "text_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 4312
    },
    "AhmadMustafa__Urdu-Instruct-News-Category-Classification_default": {
        "name": "AhmadMustafa/Urdu-Instruct-News-Category-Classification",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 100674
    },
    "thai_toxicity_tweet_thai_toxicity_tweet": {
        "name": "thai_toxicity_tweet",
        "subset": "thai_toxicity_tweet",
        "input": "tweet_text",
        "label": "is_toxic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3300
    },
    "DBQ__Gucci.Product.prices.Bulgaria_default": {
        "name": "DBQ/Gucci.Product.prices.Bulgaria",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5170
    },
    "senti_lex_km": {
        "name": "senti_lex",
        "subset": "km",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 956
    },
    "emo_emo2019": {
        "name": "emo",
        "subset": "emo2019",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 30160
    },
    "AmazonScience__massive_jv-ID": {
        "name": "AmazonScience/massive",
        "subset": "jv-ID",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "kuroneko5943__jd21_\u6d77\u9c9c": {
        "name": "kuroneko5943/jd21",
        "subset": "\u6d77\u9c9c",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3495
    },
    "hlgd_default": {
        "name": "hlgd",
        "subset": "default",
        "input": "headline_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15492
    },
    "kuroneko5943__amz20_Bag": {
        "name": "kuroneko5943/amz20",
        "subset": "Bag",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "masakhane__masakhanews_fra": {
        "name": "masakhane/masakhanews",
        "subset": "fra",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1476
    },
    "mlsum_de": {
        "name": "mlsum",
        "subset": "de",
        "input": "text",
        "label": "date",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 220887
    },
    "nguha__legalbench_oral_argument_question_purpose": {
        "name": "nguha/legalbench",
        "subset": "oral_argument_question_purpose",
        "input": "question",
        "label": "Docket No.",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7
    },
    "tasksource__mmlu_high_school_statistics": {
        "name": "tasksource/mmlu",
        "subset": "high_school_statistics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 216
    },
    "glombardo__misogynistic-statements-classification_default": {
        "name": "glombardo/misogynistic-statements-classification",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 127
    },
    "schema_guided_dstc8_schema": {
        "name": "schema_guided_dstc8",
        "subset": "schema",
        "input": "description",
        "label": "service_name",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 26
    },
    "kuroneko5943__stock11_wine": {
        "name": "kuroneko5943/stock11",
        "subset": "wine",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9683
    },
    "shmuhammad__AfriSenti-twitter-sentiment_arq": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "arq",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1651
    },
    "AmazonScience__massive_cy-GB": {
        "name": "AmazonScience/massive",
        "subset": "cy-GB",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "joelniklaus__lextreme_swiss_criticality_prediction_bge_considerations": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_criticality_prediction_bge_considerations",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 91075
    },
    "amphora__korfin-asc_default": {
        "name": "amphora/korfin-asc",
        "subset": "default",
        "input": "SRC",
        "label": "SENTIMENT",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8818
    },
    "DBQ__Gucci.Product.prices.Canada_default": {
        "name": "DBQ/Gucci.Product.prices.Canada",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5185
    },
    "evaluate__glue-ci_sst2": {
        "name": "evaluate/glue-ci",
        "subset": "sst2",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 67349
    },
    "nguha__legalbench_diversity_2": {
        "name": "nguha/legalbench",
        "subset": "diversity_2",
        "input": "text",
        "label": "aic_is_met",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "hope_edi_tamil": {
        "name": "hope_edi",
        "subset": "tamil",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 16160
    },
    "ufukhaman__uspto_balanced_200k_ipc_classification_default": {
        "name": "ufukhaman/uspto_balanced_200k_ipc_classification",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 180342
    },
    "tasksource__mmlu_high_school_psychology": {
        "name": "tasksource/mmlu",
        "subset": "high_school_psychology",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 545
    },
    "nguha__legalbench_cuad_source_code_escrow": {
        "name": "nguha/legalbench",
        "subset": "cuad_source_code_escrow",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "DBQ__Bottega.Veneta.Product.prices.South.Korea_default": {
        "name": "DBQ/Bottega.Veneta.Product.prices.South.Korea",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4463
    },
    "offenseval_dravidian_kannada": {
        "name": "offenseval_dravidian",
        "subset": "kannada",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6217
    },
    "taishi-i__awesome-japanese-nlp-classification-dataset_awesome-japanese-nlp-classification-dataset": {
        "name": "taishi-i/awesome-japanese-nlp-classification-dataset",
        "subset": "awesome-japanese-nlp-classification-dataset",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5496
    },
    "DFKI-SLT__knowledge_net_knet_re": {
        "name": "DFKI-SLT/knowledge_net",
        "subset": "knet_re",
        "input": "passageText",
        "label": "subjectType",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10895
    },
    "SKT27182__Preprocessed_OpenOrca_default": {
        "name": "SKT27182/Preprocessed_OpenOrca",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2872771
    },
    "tyqiangz__multilingual-sentiments_indonesian": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "indonesian",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11000
    },
    "sbx__superlim-2_swediagnostics": {
        "name": "sbx/superlim-2",
        "subset": "swediagnostics",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1104
    },
    "indonli_indonli": {
        "name": "indonli",
        "subset": "indonli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10330
    },
    "clinc_oos_plus": {
        "name": "clinc_oos",
        "subset": "plus",
        "input": "text",
        "label": "intent",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15250
    },
    "CATIE-AQ__amazon_massive_intent_fr_prompt_intent_classification_default": {
        "name": "CATIE-AQ/amazon_massive_intent_fr_prompt_intent_classification",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 345420
    },
    "ejschwartz__oo-method-test-split_combined": {
        "name": "ejschwartz/oo-method-test-split",
        "subset": "combined",
        "input": "Disassembly",
        "label": "Type",
        "splits": {
            "train": "combined"
        },
        "task_type": "classification",
        "train_split_size": 83776
    },
    "blog_authorship_corpus_blog_authorship_corpus": {
        "name": "blog_authorship_corpus",
        "subset": "blog_authorship_corpus",
        "input": "text",
        "label": "gender",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 689793
    },
    "Genius1237__TyDiP_vi": {
        "name": "Genius1237/TyDiP",
        "subset": "vi",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "shahules786__prosocial_augmented_default": {
        "name": "shahules786/prosocial_augmented",
        "subset": "default",
        "input": "Assistant",
        "label": "safety_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10722
    },
    "KBLab__overlim_sst_nb": {
        "name": "KBLab/overlim",
        "subset": "sst_nb",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 66486
    },
    "SachinPatel248__mqnli_default": {
        "name": "SachinPatel248/mqnli",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 103059
    },
    "scaredmeow__shopee-reviews-tl-stars_default": {
        "name": "scaredmeow/shopee-reviews-tl-stars",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10500
    },
    "swahili_news_swahili_news": {
        "name": "swahili_news",
        "subset": "swahili_news",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 22207
    },
    "kuroneko5943__stock11_airline": {
        "name": "kuroneko5943/stock11",
        "subset": "airline",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25431
    },
    "akhtet__myXNLI_default": {
        "name": "akhtet/myXNLI",
        "subset": "default",
        "input": "sentence1_en",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "RussianNLP__tape_chegeka.episodes": {
        "name": "RussianNLP/tape",
        "subset": "chegeka.episodes",
        "input": "question",
        "label": "author",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49
    },
    "nguha__legalbench_overruling": {
        "name": "nguha/legalbench",
        "subset": "overruling",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--mustapha--QuranExe": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--mustapha--QuranExe",
        "input": "text",
        "label": "resource_name",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49888
    },
    "kuroneko5943__amz20_Video_Games": {
        "name": "kuroneko5943/amz20",
        "subset": "Video_Games",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "DBQ__Prada.Product.prices.Germany_default": {
        "name": "DBQ/Prada.Product.prices.Germany",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2588
    },
    "webis__Touche23-ValueEval_nyt-meta": {
        "name": "webis/Touche23-ValueEval",
        "subset": "nyt-meta",
        "input": "Argument ID",
        "label": "URL",
        "splits": {
            "train": "meta"
        },
        "task_type": "classification",
        "train_split_size": 80
    },
    "AmazonScience__massive_de-DE": {
        "name": "AmazonScience/massive",
        "subset": "de-DE",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "rexarski__climate_fever_fixed_default": {
        "name": "rexarski/climate_fever_fixed",
        "subset": "default",
        "input": "evidence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4298
    },
    "x_stance_default": {
        "name": "x_stance",
        "subset": "default",
        "input": "comment",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 45640
    },
    "AmazonScience__massive_da-DK": {
        "name": "AmazonScience/massive",
        "subset": "da-DK",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "DBQ__Ounass.Product.prices.Oman_default": {
        "name": "DBQ/Ounass.Product.prices.Oman",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 71958
    },
    "classla__copa_hr_copa_hr": {
        "name": "classla/copa_hr",
        "subset": "copa_hr",
        "input": "premise",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 400
    },
    "senti_lex_sk": {
        "name": "senti_lex",
        "subset": "sk",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2428
    },
    "CATIE-AQ__ling_fr_prompt_textual_entailment_default": {
        "name": "CATIE-AQ/ling_fr_prompt_textual_entailment",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 110000
    },
    "se2p__code-readability-merged_default": {
        "name": "se2p/code-readability-merged",
        "subset": "default",
        "input": "code_snippet",
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 421
    },
    "tasksource__mmlu_high_school_mathematics": {
        "name": "tasksource/mmlu",
        "subset": "high_school_mathematics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 270
    },
    "indic_glue_wnli.en": {
        "name": "indic_glue",
        "subset": "wnli.en",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 635
    },
    "KETI-AIR__kor_snli_default": {
        "name": "KETI-AIR/kor_snli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550152
    },
    "mwong__climatetext-claim-related-evaluation_default": {
        "name": "mwong/climatetext-claim-related-evaluation",
        "subset": "default",
        "input": "claim",
        "label": "evidence",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1978000
    },
    "kuroneko5943__amz20_Gloves": {
        "name": "kuroneko5943/amz20",
        "subset": "Gloves",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "kuroneko5943__snap21_Luxury_Beauty_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Luxury_Beauty_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6954
    },
    "super_glue_wsc": {
        "name": "super_glue",
        "subset": "wsc",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 554
    },
    "THUDM__LongBench_lcc": {
        "name": "THUDM/LongBench",
        "subset": "lcc",
        "input": "input",
        "label": "language",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "kuroneko5943__snap21_Home_and_Kitchen_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Home_and_Kitchen_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7016
    },
    "tasksource__mmlu_professional_accounting": {
        "name": "tasksource/mmlu",
        "subset": "professional_accounting",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 282
    },
    "senti_lex_zh": {
        "name": "senti_lex",
        "subset": "zh",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1879
    },
    "tillschwoerer__tagesschau_default": {
        "name": "tillschwoerer/tagesschau",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "DBQ__Net.a.Porter.Product.prices.United.States_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.United.States",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 48920
    },
    "guardian_authorship_cross_topic_10": {
        "name": "guardian_authorship",
        "subset": "cross_topic_10",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 117
    },
    "masakhane__masakhanews_yor": {
        "name": "masakhane/masakhanews",
        "subset": "yor",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1433
    },
    "evaluate__glue-ci_qqp": {
        "name": "evaluate/glue-ci",
        "subset": "qqp",
        "input": [
            "question1",
            "question2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 363846
    },
    "saattrupdan__womens-clothing-ecommerce-reviews_default": {
        "name": "saattrupdan/womens-clothing-ecommerce-reviews",
        "subset": "default",
        "input": "review_text",
        "label": "recommended_ind",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20641
    },
    "THUDM__LongBench_qmsum": {
        "name": "THUDM/LongBench",
        "subset": "qmsum",
        "input": "input",
        "label": "context",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 200
    },
    "maveriq__bigbenchhard_causal_judgement": {
        "name": "maveriq/bigbenchhard",
        "subset": "causal_judgement",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 187
    },
    "nguha__legalbench_cuad_license_grant": {
        "name": "nguha/legalbench",
        "subset": "cuad_license_grant",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "TheBritishLibrary__blbooksgenre_raw": {
        "name": "TheBritishLibrary/blbooksgenre",
        "subset": "raw",
        "input": "Title",
        "label": "Type of resource",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 55343
    },
    "indonlp__NusaX-senti_jav": {
        "name": "indonlp/NusaX-senti",
        "subset": "jav",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "MichiganNLP__TID-8_pejorative-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "pejorative-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1535
    },
    "DBQ__Mr.Porter.Product.prices.Kazakhstan_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Kazakhstan",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25806
    },
    "KBLab__overlim_copa_da": {
        "name": "KBLab/overlim",
        "subset": "copa_da",
        "input": "premise",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 321
    },
    "DBQ__Gucci.Product.prices.Australia_default": {
        "name": "DBQ/Gucci.Product.prices.Australia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2972
    },
    "indic_glue_wstp.or": {
        "name": "indic_glue",
        "subset": "wstp.or",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4015
    },
    "senti_lex_eo": {
        "name": "senti_lex",
        "subset": "eo",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2604
    },
    "clue_cluewsc2020": {
        "name": "clue",
        "subset": "cluewsc2020",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1244
    },
    "senti_lex_cs": {
        "name": "senti_lex",
        "subset": "cs",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2599
    },
    "pragmeval_persuasiveness-claimtype": {
        "name": "pragmeval",
        "subset": "persuasiveness-claimtype",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 160
    },
    "nguha__legalbench_proa": {
        "name": "nguha/legalbench",
        "subset": "proa",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5
    },
    "metaeval__ethics_justice": {
        "name": "metaeval/ethics",
        "subset": "justice",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 21791
    },
    "KBLab__overlim_copa_nb": {
        "name": "KBLab/overlim",
        "subset": "copa_nb",
        "input": "premise",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 321
    },
    "AmazonScience__massive_sw-KE": {
        "name": "AmazonScience/massive",
        "subset": "sw-KE",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "Deysi__sentences-and-emotions_default": {
        "name": "Deysi/sentences-and-emotions",
        "subset": "default",
        "input": "utterance",
        "label": "emotion",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2405
    },
    "md_gender_bias_funpedia": {
        "name": "md_gender_bias",
        "subset": "funpedia",
        "input": "text",
        "label": "gender",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 23897
    },
    "MilaNLProc__honest_en_queer_nonqueer": {
        "name": "MilaNLProc/honest",
        "subset": "en_queer_nonqueer",
        "input": "template_masked",
        "label": "type",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 705
    },
    "tasksource__mmlu_electrical_engineering": {
        "name": "tasksource/mmlu",
        "subset": "electrical_engineering",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 145
    },
    "AmazonScience__massive_is-IS": {
        "name": "AmazonScience/massive",
        "subset": "is-IS",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nala-cub__americas_nli_shp": {
        "name": "nala-cub/americas_nli",
        "subset": "shp",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "clue_tnews": {
        "name": "clue",
        "subset": "tnews",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 53360
    },
    "HausaNLP__NaijaSenti-Twitter_yor": {
        "name": "HausaNLP/NaijaSenti-Twitter",
        "subset": "yor",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8522
    },
    "indic_glue_inltkh.ta": {
        "name": "indic_glue",
        "subset": "inltkh.ta",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5346
    },
    "financial_phrasebank_sentences_allagree": {
        "name": "financial_phrasebank",
        "subset": "sentences_allagree",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2264
    },
    "KBLab__overlim_mrpc_sv": {
        "name": "KBLab/overlim",
        "subset": "mrpc_sv",
        "input": "text_a",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3261
    },
    "THUDM__LongBench_repobench-p_e": {
        "name": "THUDM/LongBench",
        "subset": "repobench-p_e",
        "input": "input",
        "label": "language",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 300
    },
    "cassandra-themis__QR-AN_qran_full": {
        "name": "cassandra-themis/QR-AN",
        "subset": "qran_full",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63683
    },
    "pragmeval_mrda": {
        "name": "pragmeval",
        "subset": "mrda",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 14484
    },
    "nguha__legalbench_cuad_third_party_beneficiary": {
        "name": "nguha/legalbench",
        "subset": "cuad_third_party_beneficiary",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "kuroneko5943__jd21_iPhone": {
        "name": "kuroneko5943/jd21",
        "subset": "iPhone",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3729
    },
    "tasksource__lonli_default": {
        "name": "tasksource/lonli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 363000
    },
    "strombergnlp__offenseval_2020_ar": {
        "name": "strombergnlp/offenseval_2020",
        "subset": "ar",
        "input": "text",
        "label": "subtask_a",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7839
    },
    "adv_glue_adv_rte": {
        "name": "adv_glue",
        "subset": "adv_rte",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 81
    },
    "fake-news-UFG__FakeNewsSet_default": {
        "name": "fake-news-UFG/FakeNewsSet",
        "subset": "default",
        "input": "title",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 600
    },
    "indonlp__NusaX-senti_bbc": {
        "name": "indonlp/NusaX-senti",
        "subset": "bbc",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "allenai__scifact_entailment_default": {
        "name": "allenai/scifact_entailment",
        "subset": "default",
        "input": "title",
        "label": "verdict",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 919
    },
    "cdt_default": {
        "name": "cdt",
        "subset": "default",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10041
    },
    "shunk031__JGLUE_JCommonsenseQA": {
        "name": "shunk031/JGLUE",
        "subset": "JCommonsenseQA",
        "input": "question",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8939
    },
    "DBQ__Mr.Porter.Product.prices.Germany_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Germany",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27744
    },
    "CATIE-AQ__amazon_reviews_multi_fr_prompt_classes_classification_default": {
        "name": "CATIE-AQ/amazon_reviews_multi_fr_prompt_classes_classification",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4200000
    },
    "AmazonScience__massive_tl-PH": {
        "name": "AmazonScience/massive",
        "subset": "tl-PH",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "miam_vm2": {
        "name": "miam",
        "subset": "vm2",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25060
    },
    "mwong__climatetext-claim-evidence-pair-related-evaluation_default": {
        "name": "mwong/climatetext-claim-evidence-pair-related-evaluation",
        "subset": "default",
        "input": "claim",
        "label": "evidence",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1978000
    },
    "KBLab__overlim_qnli_sv": {
        "name": "KBLab/overlim",
        "subset": "qnli_sv",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 99506
    },
    "sagnikrayc__snli-cf-kaushik_plain_text": {
        "name": "sagnikrayc/snli-cf-kaushik",
        "subset": "plain_text",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8330
    },
    "kuroneko5943__weibo16_\u4e2d\u7f8e": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u4e2d\u7f8e",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 577
    },
    "joelniklaus__lextreme_turkish_constitutional_court_decisions_judgment": {
        "name": "joelniklaus/lextreme",
        "subset": "turkish_constitutional_court_decisions_judgment",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 902
    },
    "maveriq__bigbenchhard_temporal_sequences": {
        "name": "maveriq/bigbenchhard",
        "subset": "temporal_sequences",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "sihaochen__propsegment_nli": {
        "name": "sihaochen/propsegment",
        "subset": "nli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 23835
    },
    "kuroneko5943__amz20_Baby": {
        "name": "kuroneko5943/amz20",
        "subset": "Baby",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "senti_lex_ga": {
        "name": "senti_lex",
        "subset": "ga",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1073
    },
    "senti_lex_eu": {
        "name": "senti_lex",
        "subset": "eu",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1979
    },
    "RussianNLP__tape_ru_worldtree.episodes": {
        "name": "RussianNLP/tape",
        "subset": "ru_worldtree.episodes",
        "input": "question",
        "label": "school_grade",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 47
    },
    "nala-cub__americas_nli_oto": {
        "name": "nala-cub/americas_nli",
        "subset": "oto",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 748
    },
    "DFKI-SLT__knowledge_net_knet": {
        "name": "DFKI-SLT/knowledge_net",
        "subset": "knet",
        "input": "documentText",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3977
    },
    "kuroneko5943__snap21_Automotive_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Automotive_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7020
    },
    "nguha__legalbench_cuad_cap_on_liability": {
        "name": "nguha/legalbench",
        "subset": "cuad_cap_on_liability",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "sdadas__ppc_default": {
        "name": "sdadas/ppc",
        "subset": "default",
        "input": "sentence_B",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5000
    },
    "ma2za__many_emotions_raw": {
        "name": "ma2za/many_emotions",
        "subset": "raw",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "en"
        },
        "task_type": "classification",
        "train_split_size": 598298
    },
    "indic_glue_csqa.te": {
        "name": "indic_glue",
        "subset": "csqa.te",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 41338
    },
    "kuroneko5943__weibo16_\u963f\u91cc\u5df4\u5df4": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u963f\u91cc\u5df4\u5df4",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 420
    },
    "cjvt__sloie_default": {
        "name": "cjvt/sloie",
        "subset": "default",
        "input": "sentence",
        "label": "expression",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 29399
    },
    "kor_qpair_default": {
        "name": "kor_qpair",
        "subset": "default",
        "input": [
            "question1",
            "question2"
        ],
        "label": "is_duplicate",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6136
    },
    "nguha__legalbench_ssla_individual_defendants": {
        "name": "nguha/legalbench",
        "subset": "ssla_individual_defendants",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3
    },
    "aav-ds__Israel-HAMAS_war_news_default": {
        "name": "aav-ds/Israel-HAMAS_war_news",
        "subset": "default",
        "input": "text",
        "label": "provider",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13103
    },
    "tyqiangz__multilingual-sentiments_hindi": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "hindi",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "kuroneko5943__jd21_\u5c0f\u7c73\u624b\u673a": {
        "name": "kuroneko5943/jd21",
        "subset": "\u5c0f\u7c73\u624b\u673a",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7348
    },
    "maveriq__bigbenchhard_hyperbaton": {
        "name": "maveriq/bigbenchhard",
        "subset": "hyperbaton",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "kuroneko5943__weibo16_\u84ec\u4f69\u5965\u75c5\u6bd2": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u84ec\u4f69\u5965\u75c5\u6bd2",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 521
    },
    "erickrribeiro__gender-by-name_default": {
        "name": "erickrribeiro/gender-by-name",
        "subset": "default",
        "input": "Name",
        "label": "Gender",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 117815
    },
    "tasksource__crowdflower_sentiment_nuclear_power": {
        "name": "tasksource/crowdflower",
        "subset": "sentiment_nuclear_power",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 190
    },
    "tasksource__scone_default": {
        "name": "tasksource/scone",
        "subset": "default",
        "input": "sentence1_edited",
        "label": "gold_label_edited",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5010
    },
    "indonlp__NusaX-senti_mad": {
        "name": "indonlp/NusaX-senti",
        "subset": "mad",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "csebuetnlp__xnli_bn_xnli_bn": {
        "name": "csebuetnlp/xnli_bn",
        "subset": "xnli_bn",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 381449
    },
    "strombergnlp__offenseval_2020_gr": {
        "name": "strombergnlp/offenseval_2020",
        "subset": "gr",
        "input": "text",
        "label": "subtask_a",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8743
    },
    "almanach__hc3_french_ood_hc3_fr_qa": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_fr_qa",
        "input": "answer",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 68283
    },
    "ethos_binary": {
        "name": "ethos",
        "subset": "binary",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 998
    },
    "AmazonScience__massive_sq-AL": {
        "name": "AmazonScience/massive",
        "subset": "sq-AL",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "shunk031__JGLUE_JNLI": {
        "name": "shunk031/JGLUE",
        "subset": "JNLI",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20073
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--Goud--Goud-sum": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--Goud--Goud-sum",
        "input": "article",
        "label": "categories",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 139288
    },
    "indic_glue_actsa-sc.te": {
        "name": "indic_glue",
        "subset": "actsa-sc.te",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4328
    },
    "newspop_default": {
        "name": "newspop",
        "subset": "default",
        "input": "headline",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 93239
    },
    "sileod__probability_words_nli_reasoning_2hop": {
        "name": "sileod/probability_words_nli",
        "subset": "reasoning_2hop",
        "input": "context",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4000
    },
    "pragmeval_emobank-arousal": {
        "name": "pragmeval",
        "subset": "emobank-arousal",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5470
    },
    "financial_phrasebank_sentences_50agree": {
        "name": "financial_phrasebank",
        "subset": "sentences_50agree",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4846
    },
    "maveriq__bigbenchhard_penguins_in_a_table": {
        "name": "maveriq/bigbenchhard",
        "subset": "penguins_in_a_table",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 146
    },
    "davebulaval__CSMD_meaning_with_data_augmentation": {
        "name": "davebulaval/CSMD",
        "subset": "meaning_with_data_augmentation",
        "input": "simplification",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 2560
    },
    "senti_lex_rm": {
        "name": "senti_lex",
        "subset": "rm",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 116
    },
    "fake-news-UFG__fakebr_full_texts": {
        "name": "fake-news-UFG/fakebr",
        "subset": "full_texts",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7200
    },
    "evaluate__glue-ci_wnli": {
        "name": "evaluate/glue-ci",
        "subset": "wnli",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 635
    },
    "cassandra-themis__QR-AN_qran_question": {
        "name": "cassandra-themis/QR-AN",
        "subset": "qran_question",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63683
    },
    "fake-news-UFG__FactChecksbr_central_de_fatos": {
        "name": "fake-news-UFG/FactChecksbr",
        "subset": "central_de_fatos",
        "input": "review_text",
        "label": "is_fake",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10461
    },
    "pragmeval_persuasiveness-specificity": {
        "name": "pragmeval",
        "subset": "persuasiveness-specificity",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 504
    },
    "ought__raft_systematic_review_inclusion": {
        "name": "ought/raft",
        "subset": "systematic_review_inclusion",
        "input": "Abstract",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "DBQ__Net.a.Porter.Product.prices.Italy_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Italy",
        "subset": "default",
        "input": "Lace-trimmed silk-satin midi dress",
        "label": "CLOTHING",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 43148
    },
    "factckbr_default": {
        "name": "factckbr",
        "subset": "default",
        "input": "review",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1313
    },
    "dair-ai__emotion_unsplit": {
        "name": "dair-ai/emotion",
        "subset": "unsplit",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 416809
    },
    "indonlp__NusaX-senti_nij": {
        "name": "indonlp/NusaX-senti",
        "subset": "nij",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "claudios__cubert_ETHPy150Open_variable_misuse_datasets": {
        "name": "claudios/cubert_ETHPy150Open",
        "subset": "variable_misuse_datasets",
        "input": "function",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 700708
    },
    "climatebert__environmental_claims_default": {
        "name": "climatebert/environmental_claims",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2117
    },
    "Divyanshu__indicxnli_te": {
        "name": "Divyanshu/indicxnli",
        "subset": "te",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "AmazonScience__massive_ca-ES": {
        "name": "AmazonScience/massive",
        "subset": "ca-ES",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "sdadas__8tags_default": {
        "name": "sdadas/8tags",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 40001
    },
    "nguha__legalbench_cuad_no-solicit_of_employees": {
        "name": "nguha/legalbench",
        "subset": "cuad_no-solicit_of_employees",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "swag_regular": {
        "name": "swag",
        "subset": "regular",
        "input": "startphrase",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 73546
    },
    "DBQ__Net.a.Porter.Product.prices.Tunisia_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Tunisia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 42405
    },
    "metaeval__ethics_deontology": {
        "name": "metaeval/ethics",
        "subset": "deontology",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 18164
    },
    "nguha__legalbench_supply_chain_disclosure_best_practice_accountability": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_best_practice_accountability",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "kuroneko5943__snap21_Toys_and_Games_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Toys_and_Games_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7008
    },
    "hard_plain_text": {
        "name": "hard",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 105698
    },
    "ejschwartz__oo-method-test-split_byfuncname": {
        "name": "ejschwartz/oo-method-test-split",
        "subset": "byfuncname",
        "input": "Disassembly",
        "label": "Type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 75478
    },
    "MichiganNLP__TID-8_commitmentbank-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "commitmentbank-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7816
    },
    "indic_glue_csqa.ta": {
        "name": "indic_glue",
        "subset": "csqa.ta",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 38590
    },
    "tasksource__zero-shot-label-nli_default": {
        "name": "tasksource/zero-shot-label-nli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1090333
    },
    "fake-news-UFG__FactChecksbr_FakeNewsSet": {
        "name": "fake-news-UFG/FactChecksbr",
        "subset": "FakeNewsSet",
        "input": "review_id",
        "label": "is_fake",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 598
    },
    "alsubari__Israel-palestine-war_default": {
        "name": "alsubari/Israel-palestine-war",
        "subset": "default",
        "input": "title",
        "label": "publish_channel",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 310
    },
    "wisesight_sentiment_wisesight_sentiment": {
        "name": "wisesight_sentiment",
        "subset": "wisesight_sentiment",
        "input": "texts",
        "label": "category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 21628
    },
    "senti_lex_cy": {
        "name": "senti_lex",
        "subset": "cy",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1647
    },
    "miam_dihana": {
        "name": "miam",
        "subset": "dihana",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 19063
    },
    "senti_lex_la": {
        "name": "senti_lex",
        "subset": "la",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2033
    },
    "bgglue__bgglue_ct21t1": {
        "name": "bgglue/bgglue",
        "subset": "ct21t1",
        "input": "tweet_text",
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3000
    },
    "nguha__legalbench_cuad_liquidated_damages": {
        "name": "nguha/legalbench",
        "subset": "cuad_liquidated_damages",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_ssla_plaintiff": {
        "name": "nguha/legalbench",
        "subset": "ssla_plaintiff",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3
    },
    "KBLab__overlim_rte_da": {
        "name": "KBLab/overlim",
        "subset": "rte_da",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2214
    },
    "strombergnlp__x-stance_fr": {
        "name": "strombergnlp/x-stance",
        "subset": "fr",
        "input": "comment",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11790
    },
    "sagteam__author_profiling_main": {
        "name": "sagteam/author_profiling",
        "subset": "main",
        "input": "text",
        "label": "gender",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9564
    },
    "webis__Touche23-ValueEval_nahjalbalagha-level1": {
        "name": "webis/Touche23-ValueEval",
        "subset": "nahjalbalagha-level1",
        "input": "Premise",
        "label": "Stance",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 279
    },
    "shunk031__JGLUE_JSTS": {
        "name": "shunk031/JGLUE",
        "subset": "JSTS",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 12451
    },
    "yahoo_answers_topics_yahoo_answers_topics": {
        "name": "yahoo_answers_topics",
        "subset": "yahoo_answers_topics",
        "input": "best_answer",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1400000
    },
    "silicone_sem": {
        "name": "silicone",
        "subset": "sem",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4264
    },
    "KBLab__overlim_stsb_nb": {
        "name": "KBLab/overlim",
        "subset": "stsb_nb",
        "input": "text_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 4312
    },
    "cardiffnlp__tweet_sentiment_multilingual_portuguese": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "portuguese",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "SIA86__TechnicalSupportCalls_default": {
        "name": "SIA86/TechnicalSupportCalls",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 921
    },
    "nguha__legalbench_citation_prediction_classification": {
        "name": "nguha/legalbench",
        "subset": "citation_prediction_classification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2
    },
    "tweet_eval_stance_feminist": {
        "name": "tweet_eval",
        "subset": "stance_feminist",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 597
    },
    "shmuhammad__AfriSenti-twitter-sentiment_twi": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "twi",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3481
    },
    "McGill-NLP__stereoset_intersentence": {
        "name": "McGill-NLP/stereoset",
        "subset": "intersentence",
        "input": "context",
        "label": "target",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 2123
    },
    "pragmeval_stac": {
        "name": "pragmeval",
        "subset": "stac",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11230
    },
    "nguha__legalbench_opp115_first_party_collection_use": {
        "name": "nguha/legalbench",
        "subset": "opp115_first_party_collection_use",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "kuroneko5943__amz20_Vacuum": {
        "name": "kuroneko5943/amz20",
        "subset": "Vacuum",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "muchocine_default": {
        "name": "muchocine",
        "subset": "default",
        "input": "review_body",
        "label": "star_rating",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3872
    },
    "RussianNLP__tape_sit_ethics.episodes": {
        "name": "RussianNLP/tape",
        "subset": "sit_ethics.episodes",
        "input": "text",
        "label": "sit_virtue",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 59
    },
    "nguha__legalbench_definition_classification": {
        "name": "nguha/legalbench",
        "subset": "definition_classification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "kuroneko5943__jd21_MacBook": {
        "name": "kuroneko5943/jd21",
        "subset": "MacBook",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3160
    },
    "ar_res_reviews_default": {
        "name": "ar_res_reviews",
        "subset": "default",
        "input": "text",
        "label": "polarity",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8364
    },
    "hebrew_sentiment_morph": {
        "name": "hebrew_sentiment",
        "subset": "morph",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10221
    },
    "kuroneko5943__stock11_finance": {
        "name": "kuroneko5943/stock11",
        "subset": "finance",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6869
    },
    "nguha__legalbench_diversity_5": {
        "name": "nguha/legalbench",
        "subset": "diversity_5",
        "input": "text",
        "label": "aic_is_met",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "Divyanshu__indicxnli_kn": {
        "name": "Divyanshu/indicxnli",
        "subset": "kn",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "DBQ__Ounass.Product.prices.Qatar_default": {
        "name": "DBQ/Ounass.Product.prices.Qatar",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 69623
    },
    "amazon_polarity_amazon_polarity": {
        "name": "amazon_polarity",
        "subset": "amazon_polarity",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3600000
    },
    "kuroneko5943__weibo16_\u5916\u56fd\u4eba": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u5916\u56fd\u4eba",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 559
    },
    "cedr_enriched": {
        "name": "cedr",
        "subset": "enriched",
        "input": "text",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7528
    },
    "AmazonScience__massive_vi-VN": {
        "name": "AmazonScience/massive",
        "subset": "vi-VN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "tweet_eval_stance_hillary": {
        "name": "tweet_eval",
        "subset": "stance_hillary",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 620
    },
    "sbx__superlim-2_swewinograd": {
        "name": "sbx/superlim-2",
        "subset": "swewinograd",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 721
    },
    "chrisvoncsefalvay__vaers-outcomes_default": {
        "name": "chrisvoncsefalvay/vaers-outcomes",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1270444
    },
    "polemo2_in": {
        "name": "polemo2",
        "subset": "in",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5783
    },
    "mlsum_fr": {
        "name": "mlsum",
        "subset": "fr",
        "input": "text",
        "label": "date",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392902
    },
    "ScandEval__angry-tweets-mini_default": {
        "name": "ScandEval/angry-tweets-mini",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "joelniklaus__lextreme_swiss_law_area_prediction_sub_area_considerations": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_law_area_prediction_sub_area_considerations",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10475
    },
    "matthewfranglen__aste-v2_2014-restaurant-aste-v2": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2014-restaurant-aste-v2",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2335
    },
    "ought__raft_ade_corpus_v2": {
        "name": "ought/raft",
        "subset": "ade_corpus_v2",
        "input": "Sentence",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "labr_plain_text": {
        "name": "labr",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11760
    },
    "kuroneko5943__snap21_Pet_Supplies_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Pet_Supplies_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7033
    },
    "super_glue_axg": {
        "name": "super_glue",
        "subset": "axg",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 356
    },
    "senti_lex_tk": {
        "name": "senti_lex",
        "subset": "tk",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 78
    },
    "imppres_implicature_numerals_10_100": {
        "name": "imppres",
        "subset": "implicature_numerals_10_100",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "numerals_10_100"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "super_glue_wic": {
        "name": "super_glue",
        "subset": "wic",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5428
    },
    "masakhane__masakhanews_sna": {
        "name": "masakhane/masakhanews",
        "subset": "sna",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1288
    },
    "DBQ__Burberry.Product.prices.Italy_default": {
        "name": "DBQ/Burberry.Product.prices.Italy",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2691
    },
    "datacommons_factcheck_fctchk_politifact_wapo": {
        "name": "datacommons_factcheck",
        "subset": "fctchk_politifact_wapo",
        "input": "claim_text",
        "label": "reviewer_name",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5632
    },
    "nguha__legalbench_cuad_change_of_control": {
        "name": "nguha/legalbench",
        "subset": "cuad_change_of_control",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "maveriq__bigbenchhard_tracking_shuffled_objects_three_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "tracking_shuffled_objects_three_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "nguha__legalbench_canada_tax_court_outcomes": {
        "name": "nguha/legalbench",
        "subset": "canada_tax_court_outcomes",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "CATIE-AQ__anli_fr_prompt_textual_entailment_default": {
        "name": "CATIE-AQ/anli_fr_prompt_textual_entailment",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550000
    },
    "cjvt__si_nli_public": {
        "name": "cjvt/si_nli",
        "subset": "public",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4392
    },
    "kuroneko5943__amz20_Jewelry": {
        "name": "kuroneko5943/amz20",
        "subset": "Jewelry",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "senti_lex_mt": {
        "name": "senti_lex",
        "subset": "mt",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 863
    },
    "masakhane__masakhanews_orm": {
        "name": "masakhane/masakhanews",
        "subset": "orm",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1128
    },
    "nguha__legalbench_cuad_revenue-profit_sharing": {
        "name": "nguha/legalbench",
        "subset": "cuad_revenue-profit_sharing",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "sileod__probability_words_nli_reasoning_1hop": {
        "name": "sileod/probability_words_nli",
        "subset": "reasoning_1hop",
        "input": "context",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4000
    },
    "omp_posts_unlabeled": {
        "name": "omp",
        "subset": "posts_unlabeled",
        "input": "Body",
        "label": "Status",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1000000
    },
    "yangwang825__tnews_default": {
        "name": "yangwang825/tnews",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 53360
    },
    "vadis__sv-ident_default": {
        "name": "vadis/sv-ident",
        "subset": "default",
        "input": "sentence",
        "label": "is_variable",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3823
    },
    "cardiffnlp__tweet_sentiment_multilingual_english": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "english",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "tasksource__mmlu_human_aging": {
        "name": "tasksource/mmlu",
        "subset": "human_aging",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 223
    },
    "AmazonScience__massive_it-IT": {
        "name": "AmazonScience/massive",
        "subset": "it-IT",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nguha__legalbench_learned_hands_immigration": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_immigration",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "kuroneko5943__amz20_Headphone": {
        "name": "kuroneko5943/amz20",
        "subset": "Headphone",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "MuhammadHelmy__nafsy_default": {
        "name": "MuhammadHelmy/nafsy",
        "subset": "default",
        "input": "content",
        "label": "prob",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 1884
    },
    "tasksource__mmlu_econometrics": {
        "name": "tasksource/mmlu",
        "subset": "econometrics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 114
    },
    "kuroneko5943__snap21_CDs_and_Vinyl_5": {
        "name": "kuroneko5943/snap21",
        "subset": "CDs_and_Vinyl_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6974
    },
    "ajgt_twitter_ar_plain_text": {
        "name": "ajgt_twitter_ar",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1800
    },
    "biglam__contentious_contexts_default": {
        "name": "biglam/contentious_contexts",
        "subset": "default",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2720
    },
    "RussianNLP__tape_sit_ethics.raw": {
        "name": "RussianNLP/tape",
        "subset": "sit_ethics.raw",
        "input": "text",
        "label": "sit_virtue",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 254
    },
    "kor_nli_xnli": {
        "name": "kor_nli",
        "subset": "xnli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 5010
    },
    "AI-team-UoA__greek_legal_code_volume": {
        "name": "AI-team-UoA/greek_legal_code",
        "subset": "volume",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28536
    },
    "DBQ__Louis.Vuitton.Product.prices.Canada_default": {
        "name": "DBQ/Louis.Vuitton.Product.prices.Canada",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8151
    },
    "bias-amplified-splits__anli_partial_input": {
        "name": "bias-amplified-splits/anli",
        "subset": "partial_input",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train.biased"
        },
        "task_type": "classification",
        "train_split_size": 134068
    },
    "glue_qnli": {
        "name": "glue",
        "subset": "qnli",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 104743
    },
    "tasksource__mmlu_clinical_knowledge": {
        "name": "tasksource/mmlu",
        "subset": "clinical_knowledge",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 265
    },
    "DBQ__Net.a.Porter.Product.prices.Japan_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Japan",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 51262
    },
    "pietrolesci__DBPedia_Classes_indexed_default": {
        "name": "pietrolesci/DBPedia_Classes_indexed",
        "subset": "default",
        "input": "text",
        "label": "l1",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 240942
    },
    "clue_csl": {
        "name": "clue",
        "subset": "csl",
        "input": "abst",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20000
    },
    "imppres_presupposition_all_n_presupposition": {
        "name": "imppres",
        "subset": "presupposition_all_n_presupposition",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "all_n_presupposition"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "djstrong__8tags_default": {
        "name": "djstrong/8tags",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 40001
    },
    "matthewfranglen__aste-v2_2014-laptop-aste-v2": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2014-laptop-aste-v2",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1450
    },
    "kuroneko5943__weibo16_HM": {
        "name": "kuroneko5943/weibo16",
        "subset": "HM",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 832
    },
    "ejschwartz__oo-method-test-split_bylibrarydedupall": {
        "name": "ejschwartz/oo-method-test-split",
        "subset": "bylibrarydedupall",
        "input": "Disassembly",
        "label": "Type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1433
    },
    "shmuhammad__AfriSenti-twitter-sentiment_hau": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "hau",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 14172
    },
    "swag_full": {
        "name": "swag",
        "subset": "full",
        "input": "startphrase",
        "label": "gold-type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 73546
    },
    "DBQ__Gucci.Product.prices.Germany_default": {
        "name": "DBQ/Gucci.Product.prices.Germany",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5418
    },
    "lex_glue_scotus": {
        "name": "lex_glue",
        "subset": "scotus",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5000
    },
    "llm-book__JGLUE_JSQuAD": {
        "name": "llm-book/JGLUE",
        "subset": "JSQuAD",
        "input": "context",
        "label": "title",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 62859
    },
    "kuroneko5943__amz20_GPS": {
        "name": "kuroneko5943/amz20",
        "subset": "GPS",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "hackathon-somos-nlp-2023__DiagTrast_default": {
        "name": "hackathon-somos-nlp-2023/DiagTrast",
        "subset": "default",
        "input": "Sintoma",
        "label": "Padecimiento",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1333
    },
    "ccdv__patent-classification_abstract": {
        "name": "ccdv/patent-classification",
        "subset": "abstract",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25000
    },
    "discovery_discovery": {
        "name": "discovery",
        "subset": "discovery",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1566000
    },
    "nguha__legalbench_cuad_warranty_duration": {
        "name": "nguha/legalbench",
        "subset": "cuad_warranty_duration",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "MilaNLProc__honest_es_binary": {
        "name": "MilaNLProc/honest",
        "subset": "es_binary",
        "input": "template_masked",
        "label": "number",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 810
    },
    "davanstrien__test_imdb_embedd2_default": {
        "name": "davanstrien/test_imdb_embedd2",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25000
    },
    "tasksource__mmlu_college_medicine": {
        "name": "tasksource/mmlu",
        "subset": "college_medicine",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 173
    },
    "nguha__legalbench_cuad_covenant_not_to_sue": {
        "name": "nguha/legalbench",
        "subset": "cuad_covenant_not_to_sue",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "stsb_multi_mt_fr": {
        "name": "stsb_multi_mt",
        "subset": "fr",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "glue_cola": {
        "name": "glue",
        "subset": "cola",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8551
    },
    "kuroneko5943__stock11_car": {
        "name": "kuroneko5943/stock11",
        "subset": "car",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 26727
    },
    "senti_lex_lv": {
        "name": "senti_lex",
        "subset": "lv",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1938
    },
    "tasksource__crowdflower_text_emotion": {
        "name": "tasksource/crowdflower",
        "subset": "text_emotion",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 39998
    },
    "murodbek__uz-text-classification_default": {
        "name": "murodbek/uz-text-classification",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 410200
    },
    "classla__FRENK-hate-hr_multiclass": {
        "name": "classla/FRENK-hate-hr",
        "subset": "multiclass",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7964
    },
    "aarnow__auditory-skills-test2_default": {
        "name": "aarnow/auditory-skills-test2",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 275
    },
    "kuroneko5943__stock11_energy": {
        "name": "kuroneko5943/stock11",
        "subset": "energy",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6166
    },
    "tasksource__mmlu_medical_genetics": {
        "name": "tasksource/mmlu",
        "subset": "medical_genetics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "joelniklaus__lextreme_greek_legal_code_chapter": {
        "name": "joelniklaus/lextreme",
        "subset": "greek_legal_code_chapter",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28536
    },
    "djstrong__ppc_default": {
        "name": "djstrong/ppc",
        "subset": "default",
        "input": "sentence_B",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5000
    },
    "senti_lex_nn": {
        "name": "senti_lex",
        "subset": "nn",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1894
    },
    "nala-cub__americas_nli_nah": {
        "name": "nala-cub/americas_nli",
        "subset": "nah",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 738
    },
    "derenrich__wikidata-enwiki-categories-and-statements_default": {
        "name": "derenrich/wikidata-enwiki-categories-and-statements",
        "subset": "default",
        "input": "text",
        "label": "relation",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6534445
    },
    "m-ric__amazon_product_reviews_datafiniti_default": {
        "name": "m-ric/amazon_product_reviews_datafiniti",
        "subset": "default",
        "input": "reviews.text",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6000
    },
    "nguha__legalbench_contract_nli_limited_use": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_limited_use",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "movie_rationales_default": {
        "name": "movie_rationales",
        "subset": "default",
        "input": "review",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1600
    },
    "joelniklaus__lextreme_swiss_criticality_prediction_bge_facts": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_criticality_prediction_bge_facts",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 91075
    },
    "senti_lex_kn": {
        "name": "senti_lex",
        "subset": "kn",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2173
    },
    "conv_ai_3_conv_ai_3": {
        "name": "conv_ai_3",
        "subset": "conv_ai_3",
        "input": "topic_desc",
        "label": "initial_request",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9176
    },
    "swedish_reviews_plain_text": {
        "name": "swedish_reviews",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 62089
    },
    "tasksource__mmlu_professional_psychology": {
        "name": "tasksource/mmlu",
        "subset": "professional_psychology",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 612
    },
    "AmazonScience__massive_my-MM": {
        "name": "AmazonScience/massive",
        "subset": "my-MM",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "maveriq__bigbenchhard_tracking_shuffled_objects_seven_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "tracking_shuffled_objects_seven_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "senti_lex_ku": {
        "name": "senti_lex",
        "subset": "ku",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 145
    },
    "senti_lex_ht": {
        "name": "senti_lex",
        "subset": "ht",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 472
    },
    "pragmeval_persuasiveness-premisetype": {
        "name": "pragmeval",
        "subset": "persuasiveness-premisetype",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 566
    },
    "tasksource__mmlu_philosophy": {
        "name": "tasksource/mmlu",
        "subset": "philosophy",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 311
    },
    "DBQ__Loro.Piana.Product.prices.China_default": {
        "name": "DBQ/Loro.Piana.Product.prices.China",
        "subset": "default",
        "input": "category2_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 817
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--cardiffnlp--tweet_sentiment_multilingual": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--cardiffnlp--tweet_sentiment_multilingual",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "tasksource__mmlu_professional_law": {
        "name": "tasksource/mmlu",
        "subset": "professional_law",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1534
    },
    "Genius1237__TyDiP_ko": {
        "name": "Genius1237/TyDiP",
        "subset": "ko",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "fake-news-UFG__FactChecksbr_fact_check_tweet_pt": {
        "name": "fake-news-UFG/FactChecksbr",
        "subset": "fact_check_tweet_pt",
        "input": "review_id",
        "label": "is_fake",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 656
    },
    "maveriq__bigbenchhard_disambiguation_qa": {
        "name": "maveriq/bigbenchhard",
        "subset": "disambiguation_qa",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "RussianNLP__tape_ru_openbook.raw": {
        "name": "RussianNLP/tape",
        "subset": "ru_openbook.raw",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2339
    },
    "indic_glue_csqa.gu": {
        "name": "indic_glue",
        "subset": "csqa.gu",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 22861
    },
    "akuysal__turkishSMS-ds_default": {
        "name": "akuysal/turkishSMS-ds",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 765
    },
    "AmazonScience__massive_kn-IN": {
        "name": "AmazonScience/massive",
        "subset": "kn-IN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nguha__legalbench_legal_reasoning_causality": {
        "name": "nguha/legalbench",
        "subset": "legal_reasoning_causality",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "kuroneko5943__amz20_Magazine_Subscriptions": {
        "name": "kuroneko5943/amz20",
        "subset": "Magazine_Subscriptions",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "JanosAudran__financial-reports-sec_large_lite": {
        "name": "JanosAudran/financial-reports-sec",
        "subset": "large_lite",
        "input": "sentence",
        "label": "section",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15983000
    },
    "kinnews_kirnews_kinnews_raw": {
        "name": "kinnews_kirnews",
        "subset": "kinnews_raw",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 17014
    },
    "indic_glue_wnli.hi": {
        "name": "indic_glue",
        "subset": "wnli.hi",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 635
    },
    "KBLab__overlim_qqp_nb": {
        "name": "KBLab/overlim",
        "subset": "qqp_nb",
        "input": "text_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 323419
    },
    "senti_lex_be": {
        "name": "senti_lex",
        "subset": "be",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1526
    },
    "DDSC__angry-tweets_default": {
        "name": "DDSC/angry-tweets",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2411
    },
    "CATIE-AQ__french_book_reviews_fr_prompt_sentiment_analysis_default": {
        "name": "CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 270424
    },
    "Genius1237__TyDiP_ta": {
        "name": "Genius1237/TyDiP",
        "subset": "ta",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "lex_glue_ledgar": {
        "name": "lex_glue",
        "subset": "ledgar",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 60000
    },
    "metooma_default": {
        "name": "metooma",
        "subset": "default",
        "input": "TweetId",
        "label": "Text_Only_Informative",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7978
    },
    "indic_glue_sna.bn": {
        "name": "indic_glue",
        "subset": "sna.bn",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11284
    },
    "vibhorag101__suicide_prediction_dataset_phr_default": {
        "name": "vibhorag101/suicide_prediction_dataset_phr",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 185574
    },
    "JanosAudran__financial-reports-sec_small_lite": {
        "name": "JanosAudran/financial-reports-sec",
        "subset": "small_lite",
        "input": "sentence",
        "label": "section",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 200000
    },
    "Aniemore__cedr-m7_default": {
        "name": "Aniemore/cedr-m7",
        "subset": "default",
        "input": "text",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7528
    },
    "symanto__autextification2023_attribution_es": {
        "name": "symanto/autextification2023",
        "subset": "attribution_es",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 21935
    },
    "hope_edi_malayalam": {
        "name": "hope_edi",
        "subset": "malayalam",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8564
    },
    "tyqiangz__multilingual-sentiments_english": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "english",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "pragmeval_emobank-valence": {
        "name": "pragmeval",
        "subset": "emobank-valence",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5150
    },
    "stsb_multi_mt_it": {
        "name": "stsb_multi_mt",
        "subset": "it",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "tasksource__crowdflower_political-media-bias": {
        "name": "tasksource/crowdflower",
        "subset": "political-media-bias",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4999
    },
    "Divyanshu__indicxnli_bn": {
        "name": "Divyanshu/indicxnli",
        "subset": "bn",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "kuroneko5943__amz20_AlarmClock": {
        "name": "kuroneko5943/amz20",
        "subset": "AlarmClock",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "nanaaaa__emotion_chinese_english_default": {
        "name": "nanaaaa/emotion_chinese_english",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 416
    },
    "RamAnanth1__talkrl-podcast_default": {
        "name": "RamAnanth1/talkrl-podcast",
        "subset": "default",
        "input": "transcript",
        "label": "title",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 39
    },
    "DBQ__Louis.Vuitton.Product.prices.France_default": {
        "name": "DBQ/Louis.Vuitton.Product.prices.France",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7806
    },
    "nguha__legalbench_function_of_decision_section": {
        "name": "nguha/legalbench",
        "subset": "function_of_decision_section",
        "input": "Paragraph",
        "label": "Citation",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7
    },
    "nguha__legalbench_cuad_rofr-rofo-rofn": {
        "name": "nguha/legalbench",
        "subset": "cuad_rofr-rofo-rofn",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "wrbsc_default": {
        "name": "wrbsc",
        "subset": "default",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "relationship",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2827
    },
    "has_part_default": {
        "name": "has_part",
        "subset": "default",
        "input": "arg2",
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 49848
    },
    "senti_lex_az": {
        "name": "senti_lex",
        "subset": "az",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1979
    },
    "offenseval_dravidian_malayalam": {
        "name": "offenseval_dravidian",
        "subset": "malayalam",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 16010
    },
    "art_anli": {
        "name": "art",
        "subset": "anli",
        "input": "observation_2",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 169654
    },
    "nguha__legalbench_cuad_termination_for_convenience": {
        "name": "nguha/legalbench",
        "subset": "cuad_termination_for_convenience",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "tasksource__mmlu_virology": {
        "name": "tasksource/mmlu",
        "subset": "virology",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 166
    },
    "nguha__legalbench_supply_chain_disclosure_disclosed_certification": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_disclosed_certification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "DBQ__Mr.Porter.Product.prices.Czech.Republic_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Czech.Republic",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27800
    },
    "indic_glue_wnli.mr": {
        "name": "indic_glue",
        "subset": "wnli.mr",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 635
    },
    "RussianNLP__russian_super_glue_danetqa": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "danetqa",
        "input": "passage",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1749
    },
    "imppres_presupposition_possessed_definites_existence": {
        "name": "imppres",
        "subset": "presupposition_possessed_definites_existence",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "possessed_definites_existence"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "kuroneko5943__jd21_\u7ea2\u7c73\u624b\u673a": {
        "name": "kuroneko5943/jd21",
        "subset": "\u7ea2\u7c73\u624b\u673a",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3981
    },
    "symanto__autextification2023_detection_en": {
        "name": "symanto/autextification2023",
        "subset": "detection_en",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 33845
    },
    "app_reviews_default": {
        "name": "app_reviews",
        "subset": "default",
        "input": "review",
        "label": "package_name",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 288065
    },
    "kuroneko5943__jd21_\u4fee\u590d\u971c": {
        "name": "kuroneko5943/jd21",
        "subset": "\u4fee\u590d\u971c",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4364
    },
    "nguha__legalbench_learned_hands_housing": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_housing",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_learned_hands_courts": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_courts",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "fernandoperes__py_legislation_raw_text": {
        "name": "fernandoperes/py_legislation",
        "subset": "raw_text",
        "input": "text",
        "label": "extension",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 867
    },
    "Divyanshu__indicxnli_or": {
        "name": "Divyanshu/indicxnli",
        "subset": "or",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "BDas__EnglishNLPDataset_EnglishData": {
        "name": "BDas/EnglishNLPDataset",
        "subset": "EnglishData",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 80616
    },
    "AmazonScience__massive_ta-IN": {
        "name": "AmazonScience/massive",
        "subset": "ta-IN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "indic_glue_csqa.pa": {
        "name": "indic_glue",
        "subset": "csqa.pa",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 5667
    },
    "pragmeval_squinky-formality": {
        "name": "pragmeval",
        "subset": "squinky-formality",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3622
    },
    "nguha__legalbench_diversity_6": {
        "name": "nguha/legalbench",
        "subset": "diversity_6",
        "input": "text",
        "label": "aic_is_met",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "cardiffnlp__tweet_sentiment_multilingual_italian": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "italian",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "ought__raft_terms_of_service": {
        "name": "ought/raft",
        "subset": "terms_of_service",
        "input": "Sentence",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "kinnews_kirnews_kirnews_cleaned": {
        "name": "kinnews_kirnews",
        "subset": "kirnews_cleaned",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3689
    },
    "AmazonScience__massive_af-ZA": {
        "name": "AmazonScience/massive",
        "subset": "af-ZA",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "senti_lex_sr": {
        "name": "senti_lex",
        "subset": "sr",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2034
    },
    "matthewfranglen__aste-v2_2014-restaurant-sem-eval": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2014-restaurant-sem-eval",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2335
    },
    "senti_lex_ur": {
        "name": "senti_lex",
        "subset": "ur",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1347
    },
    "tasksource__mmlu_sociology": {
        "name": "tasksource/mmlu",
        "subset": "sociology",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 201
    },
    "webis__Touche23-ValueEval_nahjalbalagha": {
        "name": "webis/Touche23-ValueEval",
        "subset": "nahjalbalagha",
        "input": "Premise",
        "label": "Stance",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 279
    },
    "tyqiangz__multilingual-sentiments_all": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "all",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 270399
    },
    "indic_glue_inltkh.te": {
        "name": "indic_glue",
        "subset": "inltkh.te",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4328
    },
    "indonlp__NusaX-senti_bjn": {
        "name": "indonlp/NusaX-senti",
        "subset": "bjn",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "nguha__legalbench_nys_judicial_ethics": {
        "name": "nguha/legalbench",
        "subset": "nys_judicial_ethics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "myanmar_news_default": {
        "name": "myanmar_news",
        "subset": "default",
        "input": "text",
        "label": "category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8116
    },
    "xtreme_PAWS-X.es": {
        "name": "xtreme",
        "subset": "PAWS-X.es",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "guardian_authorship_cross_genre_4": {
        "name": "guardian_authorship",
        "subset": "cross_genre_4",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63
    },
    "senti_lex_fo": {
        "name": "senti_lex",
        "subset": "fo",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 123
    },
    "conceptnet5_omcs_sentences_more": {
        "name": "conceptnet5",
        "subset": "omcs_sentences_more",
        "input": "sentence",
        "label": "lang",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2001735
    },
    "nguha__legalbench_cuad_exclusivity": {
        "name": "nguha/legalbench",
        "subset": "cuad_exclusivity",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "climatebert__climate_commitments_actions_default": {
        "name": "climatebert/climate_commitments_actions",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1000
    },
    "tasksource__mmlu_public_relations": {
        "name": "tasksource/mmlu",
        "subset": "public_relations",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 110
    },
    "ejschwartz__oo-method-test_default": {
        "name": "ejschwartz/oo-method-test",
        "subset": "default",
        "input": "Disassembly",
        "label": "Binary",
        "splits": {
            "train": "combined"
        },
        "task_type": "classification",
        "train_split_size": 83776
    },
    "BerMaker__test_plain_text": {
        "name": "BerMaker/test",
        "subset": "plain_text",
        "input": "context",
        "label": "title",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 87599
    },
    "indic_glue_csqa.ml": {
        "name": "indic_glue",
        "subset": "csqa.ml",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 26537
    },
    "nala-cub__americas_nli_tar": {
        "name": "nala-cub/americas_nli",
        "subset": "tar",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "senti_lex_pt": {
        "name": "senti_lex",
        "subset": "pt",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3953
    },
    "ScandEval__scala-fo_default": {
        "name": "ScandEval/scala-fo",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "ought__raft_neurips_impact_statement_risks": {
        "name": "ought/raft",
        "subset": "neurips_impact_statement_risks",
        "input": "Impact statement",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "ScandEval__scala-nn_default": {
        "name": "ScandEval/scala-nn",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "tasksource__mtop_mtop": {
        "name": "tasksource/mtop",
        "subset": "mtop",
        "input": "logical_form",
        "label": "domain",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 73928
    },
    "senti_lex_tr": {
        "name": "senti_lex",
        "subset": "tr",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2500
    },
    "imppres_presupposition_cleft_existence": {
        "name": "imppres",
        "subset": "presupposition_cleft_existence",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "cleft_existence"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "mohammadjavadpirhadi__fake-news-detection-dataset-english_default": {
        "name": "mohammadjavadpirhadi/fake-news-detection-dataset-english",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35918
    },
    "KBLab__overlim_mnli_sv": {
        "name": "KBLab/overlim",
        "subset": "mnli_sv",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 383124
    },
    "DBQ__Mr.Porter.Product.prices.United.States_default": {
        "name": "DBQ/Mr.Porter.Product.prices.United.States",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35741
    },
    "fake_news_filipino_default": {
        "name": "fake_news_filipino",
        "subset": "default",
        "input": "article",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3206
    },
    "clarin-pl__polemo2-official_medicine_text": {
        "name": "clarin-pl/polemo2-official",
        "subset": "medicine_text",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2618
    },
    "tasksource__mmlu_college_computer_science": {
        "name": "tasksource/mmlu",
        "subset": "college_computer_science",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "tunizi_default": {
        "name": "tunizi",
        "subset": "default",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3000
    },
    "bias-amplified-splits__anli_minority_examples": {
        "name": "bias-amplified-splits/anli",
        "subset": "minority_examples",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train.biased"
        },
        "task_type": "classification",
        "train_split_size": 134068
    },
    "hate_speech_filipino_default": {
        "name": "hate_speech_filipino",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10000
    },
    "KBLab__overlim_boolq_da": {
        "name": "KBLab/overlim",
        "subset": "boolq_da",
        "input": "passage",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6285
    },
    "davebulaval__CSMD_meaning_holdout_unrelated": {
        "name": "davebulaval/CSMD",
        "subset": "meaning_holdout_unrelated",
        "input": "simplification",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "regression",
        "train_split_size": 359
    },
    "senti_lex_no": {
        "name": "senti_lex",
        "subset": "no",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3089
    },
    "imppres_presupposition_cleft_uniqueness": {
        "name": "imppres",
        "subset": "presupposition_cleft_uniqueness",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "cleft_uniqueness"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "AmazonScience__massive_km-KH": {
        "name": "AmazonScience/massive",
        "subset": "km-KH",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "Genius1237__TyDiP_hi": {
        "name": "Genius1237/TyDiP",
        "subset": "hi",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "DBQ__Bottega.Veneta.Product.prices.Italy_default": {
        "name": "DBQ/Bottega.Veneta.Product.prices.Italy",
        "subset": "default",
        "input": "Portachiavi Intreccio con gancio a goccia",
        "label": "N.A..1",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4394
    },
    "senti_lex_uz": {
        "name": "senti_lex",
        "subset": "uz",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 111
    },
    "senti_lex_yi": {
        "name": "senti_lex",
        "subset": "yi",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 395
    },
    "DBQ__Burberry.Product.prices.France_default": {
        "name": "DBQ/Burberry.Product.prices.France",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3298
    },
    "climatebert__climate_sentiment_default": {
        "name": "climatebert/climate_sentiment",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1000
    },
    "KBLab__overlim_mnli_da": {
        "name": "KBLab/overlim",
        "subset": "mnli_da",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 383124
    },
    "tweets_hate_speech_detection_default": {
        "name": "tweets_hate_speech_detection",
        "subset": "default",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 31962
    },
    "biglam__old_bailey_proceedings_default": {
        "name": "biglam/old_bailey_proceedings",
        "subset": "default",
        "input": "text",
        "label": "type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2638
    },
    "go_emotions_raw": {
        "name": "go_emotions",
        "subset": "raw",
        "input": "text",
        "label": "created_utc",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 211225
    },
    "nguha__legalbench_definition_extraction": {
        "name": "nguha/legalbench",
        "subset": "definition_extraction",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "ade_corpus_v2_Ade_corpus_v2_classification": {
        "name": "ade_corpus_v2",
        "subset": "Ade_corpus_v2_classification",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 23516
    },
    "glue_stsb": {
        "name": "glue",
        "subset": "stsb",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "kuroneko5943__amz20_CableModem": {
        "name": "kuroneko5943/amz20",
        "subset": "CableModem",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "joelniklaus__lextreme_swiss_criticality_prediction_citation_considerations": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_criticality_prediction_citation_considerations",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2523
    },
    "masakhane__masakhanews_hau": {
        "name": "masakhane/masakhanews",
        "subset": "hau",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2219
    },
    "senti_lex_hi": {
        "name": "senti_lex",
        "subset": "hi",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3640
    },
    "kuroneko5943__amz20_Movies_TV": {
        "name": "kuroneko5943/amz20",
        "subset": "Movies_TV",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "Tidrael__tsl_news_plain_text": {
        "name": "Tidrael/tsl_news",
        "subset": "plain_text",
        "input": "title",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 443
    },
    "kuroneko5943__stock11_Real_estate": {
        "name": "kuroneko5943/stock11",
        "subset": "Real_estate",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6707
    },
    "DBQ__Burberry.Product.prices.United.States_default": {
        "name": "DBQ/Burberry.Product.prices.United.States",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3038
    },
    "DBQ__Net.a.Porter.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "Portofino 20 suede sandals",
        "label": "SHOES",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 51659
    },
    "KBLab__overlim_boolq_sv": {
        "name": "KBLab/overlim",
        "subset": "boolq_sv",
        "input": "passage",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6285
    },
    "xtreme_XNLI": {
        "name": "xtreme",
        "subset": "XNLI",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "gold_label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 75150
    },
    "imppres_implicature_gradable_verb": {
        "name": "imppres",
        "subset": "implicature_gradable_verb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "gradable_verb"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "indonlp__NusaX-senti_ban": {
        "name": "indonlp/NusaX-senti",
        "subset": "ban",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "tasksource__mmlu_business_ethics": {
        "name": "tasksource/mmlu",
        "subset": "business_ethics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "AI-Sweden__SuperLim_swepar": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swepar",
        "input": "sentence_2",
        "label": "similarity_score",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 165
    },
    "imppres_presupposition_possessed_definites_uniqueness": {
        "name": "imppres",
        "subset": "presupposition_possessed_definites_uniqueness",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "possessed_definites_uniqueness"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "senti_lex_sv": {
        "name": "senti_lex",
        "subset": "sv",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3722
    },
    "nguha__legalbench_cuad_ip_ownership_assignment": {
        "name": "nguha/legalbench",
        "subset": "cuad_ip_ownership_assignment",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "d0rj__OpenOrca-ru_default": {
        "name": "d0rj/OpenOrca-ru",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4233923
    },
    "maveriq__bigbenchhard_ruin_names": {
        "name": "maveriq/bigbenchhard",
        "subset": "ruin_names",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "maveriq__bigbenchhard_geometric_shapes": {
        "name": "maveriq/bigbenchhard",
        "subset": "geometric_shapes",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "kuroneko5943__jd21_\u8fd0\u52a8\u978b": {
        "name": "kuroneko5943/jd21",
        "subset": "\u8fd0\u52a8\u978b",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3337
    },
    "sbx__superlim-2_sweana": {
        "name": "sbx/superlim-2",
        "subset": "sweana",
        "input": "pair1_element2",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2045
    },
    "md_gender_bias_wizard": {
        "name": "md_gender_bias",
        "subset": "wizard",
        "input": "text",
        "label": "gender",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10449
    },
    "hope_edi_english": {
        "name": "hope_edi",
        "subset": "english",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 22762
    },
    "ejschwartz__oo-method-test-split_bylibrarydedup": {
        "name": "ejschwartz/oo-method-test-split",
        "subset": "bylibrarydedup",
        "input": "Disassembly",
        "label": "Type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7910
    },
    "RussianNLP__tape_per_ethics.episodes": {
        "name": "RussianNLP/tape",
        "subset": "per_ethics.episodes",
        "input": "text",
        "label": "per_virtue",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 58
    },
    "KBLab__overlim_wnli_sv": {
        "name": "KBLab/overlim",
        "subset": "wnli_sv",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 565
    },
    "maveriq__bigbenchhard_sports_understanding": {
        "name": "maveriq/bigbenchhard",
        "subset": "sports_understanding",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "google__civil_comments_default": {
        "name": "google/civil_comments",
        "subset": "default",
        "input": "text",
        "label": "toxicity",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 1804874
    },
    "evaluate__glue-ci_cola": {
        "name": "evaluate/glue-ci",
        "subset": "cola",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8551
    },
    "guardian_authorship_cross_topic_7": {
        "name": "guardian_authorship",
        "subset": "cross_topic_7",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 90
    },
    "maveriq__bigbenchhard_logical_deduction_three_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "logical_deduction_three_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "AmazonScience__massive_pl-PL": {
        "name": "AmazonScience/massive",
        "subset": "pl-PL",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "DBQ__Gucci.Product.prices.Hungary_default": {
        "name": "DBQ/Gucci.Product.prices.Hungary",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4976
    },
    "nguha__legalbench_international_citizenship_questions": {
        "name": "nguha/legalbench",
        "subset": "international_citizenship_questions",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "snips_built_in_intents_default": {
        "name": "snips_built_in_intents",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 328
    },
    "maveriq__bigbenchhard_snarks": {
        "name": "maveriq/bigbenchhard",
        "subset": "snarks",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 178
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--J-Mourad--MNAD.v1": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--J-Mourad--MNAD.v1",
        "input": "Body",
        "label": "Category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 418563
    },
    "nguha__legalbench_personal_jurisdiction": {
        "name": "nguha/legalbench",
        "subset": "personal_jurisdiction",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "DBQ__Mr.Porter.Product.prices.United.Arab.Emirates_default": {
        "name": "DBQ/Mr.Porter.Product.prices.United.Arab.Emirates",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27033
    },
    "AmazonScience__massive_ko-KR": {
        "name": "AmazonScience/massive",
        "subset": "ko-KR",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "bbc_hindi_nli_bbc hindi nli": {
        "name": "bbc_hindi_nli",
        "subset": "bbc hindi nli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15552
    },
    "AmazonScience__massive_ro-RO": {
        "name": "AmazonScience/massive",
        "subset": "ro-RO",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "senti_lex_lb": {
        "name": "senti_lex",
        "subset": "lb",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 224
    },
    "senti_lex_io": {
        "name": "senti_lex",
        "subset": "io",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 183
    },
    "llm-book__JGLUE_JCommonsenseQA": {
        "name": "llm-book/JGLUE",
        "subset": "JCommonsenseQA",
        "input": "question",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8939
    },
    "bgglue__bgglue_examsbg": {
        "name": "bgglue/bgglue",
        "subset": "examsbg",
        "input": "id",
        "label": "answerKey",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1512
    },
    "biglam__on_the_books_default": {
        "name": "biglam/on_the_books",
        "subset": "default",
        "input": "section_text",
        "label": "jim_crow",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1785
    },
    "glue_qqp": {
        "name": "glue",
        "subset": "qqp",
        "input": [
            "question1",
            "question2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 363846
    },
    "senti_lex_vi": {
        "name": "senti_lex",
        "subset": "vi",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1016
    },
    "senti_lex_bs": {
        "name": "senti_lex",
        "subset": "bs",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2020
    },
    "mwong__climatetext-climate_evidence-claim-related-evaluation_default": {
        "name": "mwong/climatetext-climate_evidence-claim-related-evaluation",
        "subset": "default",
        "input": "claim",
        "label": "evidence",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1978000
    },
    "shmuhammad__AfriSenti-twitter-sentiment_ibo": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "ibo",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10192
    },
    "DBQ__Net.a.Porter.Product.prices.Russia_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Russia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 41393
    },
    "guardian_authorship_cross_topic_8": {
        "name": "guardian_authorship",
        "subset": "cross_topic_8",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 90
    },
    "ScandEval__scala-da_default": {
        "name": "ScandEval/scala-da",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "sbx__superlim-2_swesim_relatedness": {
        "name": "sbx/superlim-2",
        "subset": "swesim_relatedness",
        "input": "word_1",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 131
    },
    "sst_default": {
        "name": "sst",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 8544
    },
    "tasksource__mmlu_elementary_mathematics": {
        "name": "tasksource/mmlu",
        "subset": "elementary_mathematics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 378
    },
    "tasksource__mmlu_human_sexuality": {
        "name": "tasksource/mmlu",
        "subset": "human_sexuality",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 131
    },
    "yoruba_bbc_topics_default": {
        "name": "yoruba_bbc_topics",
        "subset": "default",
        "input": "news_title",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1340
    },
    "allegro_reviews_default": {
        "name": "allegro_reviews",
        "subset": "default",
        "input": "text",
        "label": "rating",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 9577
    },
    "DBQ__Loro.Piana.Product.prices.Canada_default": {
        "name": "DBQ/Loro.Piana.Product.prices.Canada",
        "subset": "default",
        "input": "website_name",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 698
    },
    "ltg__norec_default": {
        "name": "ltg/norec",
        "subset": "default",
        "input": "text",
        "label": "language",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 34749
    },
    "tdavidson__hate_speech_offensive_default": {
        "name": "tdavidson/hate_speech_offensive",
        "subset": "default",
        "input": "tweet",
        "label": "class",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 24783
    },
    "DDSC__twitter-sent_default": {
        "name": "DDSC/twitter-sent",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1007
    },
    "nguha__legalbench_cuad_expiration_date": {
        "name": "nguha/legalbench",
        "subset": "cuad_expiration_date",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "maveriq__bigbenchhard_formal_fallacies": {
        "name": "maveriq/bigbenchhard",
        "subset": "formal_fallacies",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "kuroneko5943__weibo16_\u9a6c\u4e91\u8c08996": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u9a6c\u4e91\u8c08996",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 419
    },
    "nguha__legalbench_learned_hands_crime": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_crime",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "tasksource__mmlu_high_school_microeconomics": {
        "name": "tasksource/mmlu",
        "subset": "high_school_microeconomics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 238
    },
    "GePaSud__TROPICAL_original_dataset": {
        "name": "GePaSud/TROPICAL",
        "subset": "original_dataset",
        "input": "id_comment",
        "label": "general_polarity",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1114
    },
    "Divyanshu__indicxnli_pa": {
        "name": "Divyanshu/indicxnli",
        "subset": "pa",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "RussianNLP__russian_super_glue_lidirus": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "lidirus",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1104
    },
    "senti_lex_ca": {
        "name": "senti_lex",
        "subset": "ca",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3204
    },
    "cassandra-themis__QR-AN_qran_answer": {
        "name": "cassandra-themis/QR-AN",
        "subset": "qran_answer",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63683
    },
    "glue_wnli": {
        "name": "glue",
        "subset": "wnli",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 635
    },
    "claudios__cubert_ETHPy150Open_wrong_binary_operator_datasets": {
        "name": "claudios/cubert_ETHPy150Open",
        "subset": "wrong_binary_operator_datasets",
        "input": "function",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 459400
    },
    "nguha__legalbench_successor_liability": {
        "name": "nguha/legalbench",
        "subset": "successor_liability",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3
    },
    "DFKI-SLT__gids_gids": {
        "name": "DFKI-SLT/gids",
        "subset": "gids",
        "input": "sentence",
        "label": "relation",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11297
    },
    "tasksource__crowdflower_political-media-audience": {
        "name": "tasksource/crowdflower",
        "subset": "political-media-audience",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4999
    },
    "classla__FRENK-hate-en_multiclass": {
        "name": "classla/FRENK-hate-en",
        "subset": "multiclass",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8404
    },
    "tab_fact_tab_fact": {
        "name": "tab_fact",
        "subset": "tab_fact",
        "input": "table_text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 92283
    },
    "CATIE-AQ__mtop_domain_intent_fr_prompt_intent_classification_default": {
        "name": "CATIE-AQ/mtop_domain_intent_fr_prompt_intent_classification",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 354420
    },
    "indic_glue_wstp.as": {
        "name": "indic_glue",
        "subset": "wstp.as",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5000
    },
    "nguha__legalbench_supply_chain_disclosure_best_practice_audits": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_best_practice_audits",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "nguha__legalbench_supply_chain_disclosure_disclosed_verification": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_disclosed_verification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "CATIE-AQ__wanli_fr_prompt_textual_entailment_default": {
        "name": "CATIE-AQ/wanli_fr_prompt_textual_entailment",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550000
    },
    "kor_nli_multi_nli": {
        "name": "kor_nli",
        "subset": "multi_nli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "AmazonScience__massive_az-AZ": {
        "name": "AmazonScience/massive",
        "subset": "az-AZ",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "silicone_meld_s": {
        "name": "silicone",
        "subset": "meld_s",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9989
    },
    "hindi_discourse_default": {
        "name": "hindi_discourse",
        "subset": "default",
        "input": "Sentence",
        "label": "Discourse Mode",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9968
    },
    "davanstrien__test_imdb_embedd_default": {
        "name": "davanstrien/test_imdb_embedd",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25000
    },
    "AmazonScience__massive_ms-MY": {
        "name": "AmazonScience/massive",
        "subset": "ms-MY",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "evaluate__glue-ci_stsb": {
        "name": "evaluate/glue-ci",
        "subset": "stsb",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "shmuhammad__AfriSenti-twitter-sentiment_swa": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "swa",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1810
    },
    "AmazonScience__massive_ka-GE": {
        "name": "AmazonScience/massive",
        "subset": "ka-GE",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nli_tr_multinli_tr": {
        "name": "nli_tr",
        "subset": "multinli_tr",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "AmazonScience__massive_fi-FI": {
        "name": "AmazonScience/massive",
        "subset": "fi-FI",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nguha__legalbench_learned_hands_education": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_education",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_corporate_lobbying": {
        "name": "nguha/legalbench",
        "subset": "corporate_lobbying",
        "input": "company_description",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10
    },
    "MichiganNLP__TID-8_md-agreement-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "md-agreement-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 32960
    },
    "stsb_multi_mt_ru": {
        "name": "stsb_multi_mt",
        "subset": "ru",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "paws_labeled_final": {
        "name": "paws",
        "subset": "labeled_final",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "Patt__ReCoRD_TH_drop_default": {
        "name": "Patt/ReCoRD_TH_drop",
        "subset": "default",
        "input": "passage",
        "label": "score_passage",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 57811
    },
    "qanastek__Biosses-BLUE_biosses": {
        "name": "qanastek/Biosses-BLUE",
        "subset": "biosses",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 64
    },
    "nguha__legalbench_abercrombie": {
        "name": "nguha/legalbench",
        "subset": "abercrombie",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5
    },
    "DFKI-SLT__kbp37_kbp37": {
        "name": "DFKI-SLT/kbp37",
        "subset": "kbp37",
        "input": "sentence",
        "label": "relation",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15917
    },
    "Jasontth__climate_fever_plus_default": {
        "name": "Jasontth/climate_fever_plus",
        "subset": "default",
        "input": "evidence",
        "label": "evidence_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6580
    },
    "imppres_implicature_modals": {
        "name": "imppres",
        "subset": "implicature_modals",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "modals"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "d0rj__rudetoxifier_data_default": {
        "name": "d0rj/rudetoxifier_data",
        "subset": "default",
        "input": "text",
        "label": "toxic",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 163187
    },
    "evaluate__glue-ci_mnli": {
        "name": "evaluate/glue-ci",
        "subset": "mnli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "joelniklaus__lextreme_german_argument_mining": {
        "name": "joelniklaus/lextreme",
        "subset": "german_argument_mining",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 19271
    },
    "assin_ptpt": {
        "name": "assin",
        "subset": "ptpt",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "entailment_judgment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2500
    },
    "pragmeval_emergent": {
        "name": "pragmeval",
        "subset": "emergent",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2076
    },
    "AI-Sweden__SuperLim_swewsc": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swewsc",
        "input": "passage",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 275
    },
    "dell-research-harvard__AmericanStories_subset_years": {
        "name": "dell-research-harvard/AmericanStories",
        "subset": "subset_years",
        "input": "article",
        "label": "byline",
        "splits": {
            "train": "1804"
        },
        "task_type": "classification",
        "train_split_size": 103
    },
    "DBQ__Gucci.Product.prices.South.Korea_default": {
        "name": "DBQ/Gucci.Product.prices.South.Korea",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3796
    },
    "AmazonScience__massive_lv-LV": {
        "name": "AmazonScience/massive",
        "subset": "lv-LV",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "AmazonScience__massive_all_1.1": {
        "name": "AmazonScience/massive",
        "subset": "all_1.1",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 598728
    },
    "sms_spam_plain_text": {
        "name": "sms_spam",
        "subset": "plain_text",
        "input": "sms",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5574
    },
    "senti_lex_ru": {
        "name": "senti_lex",
        "subset": "ru",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2914
    },
    "AmazonScience__massive_nl-NL": {
        "name": "AmazonScience/massive",
        "subset": "nl-NL",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "xtreme_PAWS-X.en": {
        "name": "xtreme",
        "subset": "PAWS-X.en",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49175
    },
    "kuroneko5943__snap21_Patio_Lawn_and_Garden_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Patio_Lawn_and_Garden_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7012
    },
    "Deysi__spam-detection-dataset_default": {
        "name": "Deysi/spam-detection-dataset",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8175
    },
    "senti_lex_nl": {
        "name": "senti_lex",
        "subset": "nl",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3976
    },
    "DBQ__Net.a.Porter.Product.prices.South.Korea_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.South.Korea",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 51265
    },
    "clinc_oos_imbalanced": {
        "name": "clinc_oos",
        "subset": "imbalanced",
        "input": "text",
        "label": "intent",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10625
    },
    "DBQ__Mr.Porter.Product.prices.Hungary_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Hungary",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27686
    },
    "nguha__legalbench_supply_chain_disclosure_disclosed_accountability": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_disclosed_accountability",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "pietrolesci__wikitoxic_default": {
        "name": "pietrolesci/wikitoxic",
        "subset": "default",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 127656
    },
    "DBQ__Gucci.Product.prices.Romania_default": {
        "name": "DBQ/Gucci.Product.prices.Romania",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5169
    },
    "senti_lex_gu": {
        "name": "senti_lex",
        "subset": "gu",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2145
    },
    "DBQ__Chloe.Product.prices.France_default": {
        "name": "DBQ/Chloe.Product.prices.France",
        "subset": "default",
        "input": "title",
        "label": "category2_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2459
    },
    "ihassan1__auditor-sentiment_default": {
        "name": "ihassan1/auditor-sentiment",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 3877
    },
    "senti_lex_an": {
        "name": "senti_lex",
        "subset": "an",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 97
    },
    "philschmid__emotion_split": {
        "name": "philschmid/emotion",
        "subset": "split",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 16000
    },
    "senti_lex_ka": {
        "name": "senti_lex",
        "subset": "ka",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2202
    },
    "indic_glue_wstp.pa": {
        "name": "indic_glue",
        "subset": "wstp.pa",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8772
    },
    "cardiffnlp__tweet_sentiment_multilingual_arabic": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "arabic",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "AmazonScience__massive_fr-FR": {
        "name": "AmazonScience/massive",
        "subset": "fr-FR",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "tasksource__mmlu_machine_learning": {
        "name": "tasksource/mmlu",
        "subset": "machine_learning",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 112
    },
    "senti_lex_ia": {
        "name": "senti_lex",
        "subset": "ia",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 326
    },
    "AI-team-UoA__greek_legal_code_chapter": {
        "name": "AI-team-UoA/greek_legal_code",
        "subset": "chapter",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28536
    },
    "DBQ__Burberry.Product.prices.Japan_default": {
        "name": "DBQ/Burberry.Product.prices.Japan",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2950
    },
    "DBQ__Chloe.Product.prices.United.States_default": {
        "name": "DBQ/Chloe.Product.prices.United.States",
        "subset": "default",
        "input": "Hana mini bag",
        "label": "NEW ARRIVALS",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2569
    },
    "indic_glue_iitp-pr.hi": {
        "name": "indic_glue",
        "subset": "iitp-pr.hi",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4182
    },
    "KBLab__overlim_wnli_da": {
        "name": "KBLab/overlim",
        "subset": "wnli_da",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 565
    },
    "DBQ__Prada.Product.prices.Sweden_default": {
        "name": "DBQ/Prada.Product.prices.Sweden",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2548
    },
    "guardian_authorship_cross_topic_6": {
        "name": "guardian_authorship",
        "subset": "cross_topic_6",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 62
    },
    "glue_sst2": {
        "name": "glue",
        "subset": "sst2",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 67349
    },
    "stsb_multi_mt_pt": {
        "name": "stsb_multi_mt",
        "subset": "pt",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "climatebert__tcfd_recommendations_default": {
        "name": "climatebert/tcfd_recommendations",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1300
    },
    "AmazonScience__massive_te-IN": {
        "name": "AmazonScience/massive",
        "subset": "te-IN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "imppres_implicature_connectives": {
        "name": "imppres",
        "subset": "implicature_connectives",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "connectives"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "indic_glue_inltkh.mr": {
        "name": "indic_glue",
        "subset": "inltkh.mr",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9672
    },
    "klue_nli": {
        "name": "klue",
        "subset": "nli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 24998
    },
    "miam_maptask": {
        "name": "miam",
        "subset": "maptask",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25382
    },
    "assin_full": {
        "name": "assin",
        "subset": "full",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "entailment_judgment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5000
    },
    "sem_eval_2014_task_1_default": {
        "name": "sem_eval_2014_task_1",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "entailment_judgment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4500
    },
    "MilaNLProc__honest_pt_binary": {
        "name": "MilaNLProc/honest",
        "subset": "pt_binary",
        "input": "template_masked",
        "label": "number",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 840
    },
    "wongnai_reviews_default": {
        "name": "wongnai_reviews",
        "subset": "default",
        "input": "review_body",
        "label": "star_rating",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 40000
    },
    "ought__raft_twitter_complaints": {
        "name": "ought/raft",
        "subset": "twitter_complaints",
        "input": "Tweet text",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "Divyanshu__indicxnli_hi": {
        "name": "Divyanshu/indicxnli",
        "subset": "hi",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "tweet_eval_stance_climate": {
        "name": "tweet_eval",
        "subset": "stance_climate",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 355
    },
    "nala-cub__americas_nli_all_languages": {
        "name": "nala-cub/americas_nli",
        "subset": "all_languages",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 7486
    },
    "tweet_eval_stance_atheism": {
        "name": "tweet_eval",
        "subset": "stance_atheism",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 461
    },
    "tasksource__mmlu_nutrition": {
        "name": "tasksource/mmlu",
        "subset": "nutrition",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 306
    },
    "AmazonScience__massive_ar-SA": {
        "name": "AmazonScience/massive",
        "subset": "ar-SA",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "DDSC__europarl_default": {
        "name": "DDSC/europarl",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 669
    },
    "t0mmy__livedoor_news_corpus_default": {
        "name": "t0mmy/livedoor_news_corpus",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7367
    },
    "AmazonScience__massive_hi-IN": {
        "name": "AmazonScience/massive",
        "subset": "hi-IN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "KBLab__overlim_qqp_da": {
        "name": "KBLab/overlim",
        "subset": "qqp_da",
        "input": "text_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 323419
    },
    "tasksource__mmlu_high_school_world_history": {
        "name": "tasksource/mmlu",
        "subset": "high_school_world_history",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 237
    },
    "KBLab__overlim_mrpc_da": {
        "name": "KBLab/overlim",
        "subset": "mrpc_da",
        "input": "text_a",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3261
    },
    "kuroneko5943__snap21_Cell_Phones_and_Accessories_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Cell_Phones_and_Accessories_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6998
    },
    "klue_re": {
        "name": "klue",
        "subset": "re",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 32470
    },
    "adv_glue_adv_mnli_mismatched": {
        "name": "adv_glue",
        "subset": "adv_mnli_mismatched",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 162
    },
    "nguha__legalbench_supply_chain_disclosure_best_practice_verification": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_best_practice_verification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "lama_conceptnet": {
        "name": "lama",
        "subset": "conceptnet",
        "input": "masked_sentence",
        "label": "pred",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 29774
    },
    "clarin-pl__polemo2-official_hotels_sentence": {
        "name": "clarin-pl/polemo2-official",
        "subset": "hotels_sentence",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 19881
    },
    "senti_lex_hr": {
        "name": "senti_lex",
        "subset": "hr",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2208
    },
    "piuba-bigdata__contextualized_hate_speech_default": {
        "name": "piuba-bigdata/contextualized_hate_speech",
        "subset": "default",
        "input": "text",
        "label": "HATEFUL",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 36420
    },
    "md_gender_bias_opensubtitles_inferred": {
        "name": "md_gender_bias",
        "subset": "opensubtitles_inferred",
        "input": "text",
        "label": "binary_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 351036
    },
    "AmazonScience__massive_ru-RU": {
        "name": "AmazonScience/massive",
        "subset": "ru-RU",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "dlb__plue_mnli": {
        "name": "dlb/plue",
        "subset": "mnli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "DBQ__Net.a.Porter.Product.prices.India_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.India",
        "subset": "default",
        "input": "Janna strapless duchesse cotton-blend satin mini dress",
        "label": "CLOTHING",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 44420
    },
    "nguha__legalbench_privacy_policy_qa": {
        "name": "nguha/legalbench",
        "subset": "privacy_policy_qa",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "KBLab__overlim_copa_sv": {
        "name": "KBLab/overlim",
        "subset": "copa_sv",
        "input": "premise",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 321
    },
    "AmazonScience__massive_tr-TR": {
        "name": "AmazonScience/massive",
        "subset": "tr-TR",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "kuroneko5943__snap21_Video_Games_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Video_Games_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6997
    },
    "tasksource__mmlu_high_school_physics": {
        "name": "tasksource/mmlu",
        "subset": "high_school_physics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 151
    },
    "MilaNLProc__honest_it_binary": {
        "name": "MilaNLProc/honest",
        "subset": "it_binary",
        "input": "template_masked",
        "label": "number",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 810
    },
    "maveriq__bigbenchhard_multistep_arithmetic_two": {
        "name": "maveriq/bigbenchhard",
        "subset": "multistep_arithmetic_two",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "kor_nlu_nli": {
        "name": "kor_nlu",
        "subset": "nli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550146
    },
    "claudios__cubert_ETHPy150Open_swapped_operands_datasets": {
        "name": "claudios/cubert_ETHPy150Open",
        "subset": "swapped_operands_datasets",
        "input": "function",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 236246
    },
    "swda_default": {
        "name": "swda",
        "subset": "default",
        "input": "text",
        "label": "damsl_act_tag",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 213543
    },
    "discovery_discoverysmall": {
        "name": "discovery",
        "subset": "discoverysmall",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15662
    },
    "tasksource__mmlu_abstract_algebra": {
        "name": "tasksource/mmlu",
        "subset": "abstract_algebra",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "threite__Bundestag-v2_default": {
        "name": "threite/Bundestag-v2",
        "subset": "default",
        "input": "text",
        "label": "party",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 143406
    },
    "indonlp__NusaX-senti_ind": {
        "name": "indonlp/NusaX-senti",
        "subset": "ind",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "tasksource__crowdflower_corporate-messaging": {
        "name": "tasksource/crowdflower",
        "subset": "corporate-messaging",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3118
    },
    "alexandrainst__ddisco_default": {
        "name": "alexandrainst/ddisco",
        "subset": "default",
        "input": "text",
        "label": "domain",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 801
    },
    "DBQ__Blickers.Product.prices.Italy_default": {
        "name": "DBQ/Blickers.Product.prices.Italy",
        "subset": "default",
        "input": "category3_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 29548
    },
    "nguha__legalbench_cuad_irrevocable_or_perpetual_license": {
        "name": "nguha/legalbench",
        "subset": "cuad_irrevocable_or_perpetual_license",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "JanosAudran__financial-reports-sec_small_full": {
        "name": "JanosAudran/financial-reports-sec",
        "subset": "small_full",
        "input": "sentence",
        "label": "section",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 200000
    },
    "senti_lex_es": {
        "name": "senti_lex",
        "subset": "es",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4275
    },
    "DBQ__Mr.Porter.Product.prices.Brazil_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Brazil",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 30793
    },
    "GePaSud__TROPICAL_no_overlapping_subset": {
        "name": "GePaSud/TROPICAL",
        "subset": "no_overlapping_subset",
        "input": "id_comment",
        "label": "general_polarity",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 326
    },
    "pragmeval_pdtb": {
        "name": "pragmeval",
        "subset": "pdtb",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 12907
    },
    "humicroedit_subtask-2": {
        "name": "humicroedit",
        "subset": "subtask-2",
        "input": "original1",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9381
    },
    "almanach__hc3_french_ood_hc3_fr_full": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_fr_full",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 68283
    },
    "ttxy__emotion_default": {
        "name": "ttxy/emotion",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 16000
    },
    "tweet_eval_offensive": {
        "name": "tweet_eval",
        "subset": "offensive",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11916
    },
    "glue_mrpc": {
        "name": "glue",
        "subset": "mrpc",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3668
    },
    "cointegrated__nli-rus-translated-v2021_default": {
        "name": "cointegrated/nli-rus-translated-v2021",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1756548
    },
    "kuroneko5943__amz20_HomeTheaterSystem": {
        "name": "kuroneko5943/amz20",
        "subset": "HomeTheaterSystem",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "assin_ptbr": {
        "name": "assin",
        "subset": "ptbr",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "entailment_judgment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2500
    },
    "dair-ai__emotion_split": {
        "name": "dair-ai/emotion",
        "subset": "split",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 16000
    },
    "nala-cub__americas_nli_gn": {
        "name": "nala-cub/americas_nli",
        "subset": "gn",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "joelniklaus__lextreme_greek_legal_code_volume": {
        "name": "joelniklaus/lextreme",
        "subset": "greek_legal_code_volume",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28536
    },
    "RussianNLP__russian_super_glue_rcb": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "rcb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 438
    },
    "MichiganNLP__TID-8_goemotions-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "goemotions-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 131395
    },
    "CATIE-AQ__xnli_fr_prompt_textual_entailment_default": {
        "name": "CATIE-AQ/xnli_fr_prompt_textual_entailment",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8639444
    },
    "journalists_questions_plain_text": {
        "name": "journalists_questions",
        "subset": "plain_text",
        "input": "tweet_id",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10077
    },
    "kuroneko5943__stock11_manufacture": {
        "name": "kuroneko5943/stock11",
        "subset": "manufacture",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 36283
    },
    "kuroneko5943__stock11_communication": {
        "name": "kuroneko5943/stock11",
        "subset": "communication",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 42269
    },
    "indic_glue_csqa.bn": {
        "name": "indic_glue",
        "subset": "csqa.bn",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 38845
    },
    "cjvt__sentinews_paragraph_level": {
        "name": "cjvt/sentinews",
        "subset": "paragraph_level",
        "input": "content",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 89999
    },
    "DBQ__Net.a.Porter.Product.prices.Bulgaria_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Bulgaria",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 42495
    },
    "fake_news_english_default": {
        "name": "fake_news_english",
        "subset": "default",
        "input": "url_of_rebutting_article",
        "label": "fake_or_satire",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 492
    },
    "kan_hope_default": {
        "name": "kan_hope",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4940
    },
    "d0rj__conv_ai_3_ru_default": {
        "name": "d0rj/conv_ai_3_ru",
        "subset": "default",
        "input": "topic_desc",
        "label": "initial_request",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9176
    },
    "nguha__legalbench_contract_nli_no_licensing": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_no_licensing",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "llm-book__JGLUE_JNLI": {
        "name": "llm-book/JGLUE",
        "subset": "JNLI",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20073
    },
    "cedr_main": {
        "name": "cedr",
        "subset": "main",
        "input": "text",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7528
    },
    "crows_pairs_crows_pairs": {
        "name": "crows_pairs",
        "subset": "crows_pairs",
        "input": "sent_less",
        "label": "stereo_antistereo",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1508
    },
    "AmazonScience__massive_fa-IR": {
        "name": "AmazonScience/massive",
        "subset": "fa-IR",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "xtreme_PAWS-X.ja": {
        "name": "xtreme",
        "subset": "PAWS-X.ja",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "imppres_presupposition_change_of_state": {
        "name": "imppres",
        "subset": "presupposition_change_of_state",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "change_of_state"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "nguha__legalbench_ucc_v_common_law": {
        "name": "nguha/legalbench",
        "subset": "ucc_v_common_law",
        "input": "contract",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "AmazonScience__massive_pt-PT": {
        "name": "AmazonScience/massive",
        "subset": "pt-PT",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "senti_lex_he": {
        "name": "senti_lex",
        "subset": "he",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2533
    },
    "joelniklaus__lextreme_swiss_criticality_prediction_citation_facts": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_criticality_prediction_citation_facts",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2523
    },
    "paws_labeled_swap": {
        "name": "paws",
        "subset": "labeled_swap",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 30397
    },
    "joelniklaus__lextreme_greek_legal_code_subject": {
        "name": "joelniklaus/lextreme",
        "subset": "greek_legal_code_subject",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28536
    },
    "coarse_discourse_default": {
        "name": "coarse_discourse",
        "subset": "default",
        "input": "title",
        "label": "majority_type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 116357
    },
    "adv_glue_adv_sst2": {
        "name": "adv_glue",
        "subset": "adv_sst2",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 148
    },
    "tapaco_all_languages": {
        "name": "tapaco",
        "subset": "all_languages",
        "input": "paraphrase",
        "label": "language",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1926192
    },
    "Genius1237__TyDiP_en": {
        "name": "Genius1237/TyDiP",
        "subset": "en",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1927
    },
    "DBQ__Chanel.Product.prices.United.States_default": {
        "name": "DBQ/Chanel.Product.prices.United.States",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1452
    },
    "tasksource__mmlu_high_school_us_history": {
        "name": "tasksource/mmlu",
        "subset": "high_school_us_history",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 204
    },
    "kuroneko5943__jd21_\u65e0\u7ebf\u8033\u673a": {
        "name": "kuroneko5943/jd21",
        "subset": "\u65e0\u7ebf\u8033\u673a",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3963
    },
    "tasksource__mmlu_college_biology": {
        "name": "tasksource/mmlu",
        "subset": "college_biology",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 144
    },
    "DBQ__Gucci.Product.prices.Sweden_default": {
        "name": "DBQ/Gucci.Product.prices.Sweden",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4916
    },
    "climatebert__climate_specificity_default": {
        "name": "climatebert/climate_specificity",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1000
    },
    "multi_nli_mismatch_plain_text": {
        "name": "multi_nli_mismatch",
        "subset": "plain_text",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "ought__raft_one_stop_english": {
        "name": "ought/raft",
        "subset": "one_stop_english",
        "input": "Article",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "DBQ__Loro.Piana.Product.prices.France_default": {
        "name": "DBQ/Loro.Piana.Product.prices.France",
        "subset": "default",
        "input": "website_name",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1128
    },
    "clue_iflytek": {
        "name": "clue",
        "subset": "iflytek",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 12133
    },
    "covid_tweets_japanese_default": {
        "name": "covid_tweets_japanese",
        "subset": "default",
        "input": "tweet_id",
        "label": "assessment_option_id",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 53639
    },
    "cardiffnlp__tweet_sentiment_multilingual_german": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "german",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "nguha__legalbench_privacy_policy_entailment": {
        "name": "nguha/legalbench",
        "subset": "privacy_policy_entailment",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "strombergnlp__x-stance_de": {
        "name": "strombergnlp/x-stance",
        "subset": "de",
        "input": "comment",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 33850
    },
    "paws-x_es": {
        "name": "paws-x",
        "subset": "es",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "RussianNLP__russian_super_glue_russe": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "russe",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 19845
    },
    "AmazonScience__massive_ml-IN": {
        "name": "AmazonScience/massive",
        "subset": "ml-IN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "ccdv__patent-classification_patent": {
        "name": "ccdv/patent-classification",
        "subset": "patent",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25000
    },
    "nguha__legalbench_cuad_unlimited-all-you-can-eat-license": {
        "name": "nguha/legalbench",
        "subset": "cuad_unlimited-all-you-can-eat-license",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "maveriq__bigbenchhard_word_sorting": {
        "name": "maveriq/bigbenchhard",
        "subset": "word_sorting",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "senti_lex_uk": {
        "name": "senti_lex",
        "subset": "uk",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2827
    },
    "DBQ__Louis.Vuitton.Product.prices.Singapore_default": {
        "name": "DBQ/Louis.Vuitton.Product.prices.Singapore",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6576
    },
    "tyqiangz__multilingual-sentiments_french": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "french",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "senti_lex_bg": {
        "name": "senti_lex",
        "subset": "bg",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2847
    },
    "super_glue_wsc.fixed": {
        "name": "super_glue",
        "subset": "wsc.fixed",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 554
    },
    "told-br_multilabel": {
        "name": "told-br",
        "subset": "multilabel",
        "input": "text",
        "label": "homophobia",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 21000
    },
    "pragmeval_sarcasm": {
        "name": "pragmeval",
        "subset": "sarcasm",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3754
    },
    "tasksource__mmlu_college_mathematics": {
        "name": "tasksource/mmlu",
        "subset": "college_mathematics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "cardiffnlp__tweet_sentiment_multilingual_all": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "all",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 14712
    },
    "matthewfranglen__aste-v2_2016-restaurant-aste-v2": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2016-restaurant-aste-v2",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1394
    },
    "nguha__legalbench_cuad_affiliate_license-licensee": {
        "name": "nguha/legalbench",
        "subset": "cuad_affiliate_license-licensee",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "shmuhammad__AfriSenti-twitter-sentiment_amh": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "amh",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5984
    },
    "jlh-ibm__earnings_call_transcripts": {
        "name": "jlh-ibm/earnings_call",
        "subset": "transcripts",
        "input": "transcript",
        "label": "company",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 150
    },
    "md_gender_bias_convai2_inferred": {
        "name": "md_gender_bias",
        "subset": "convai2_inferred",
        "input": "text",
        "label": "binary_label",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "classification",
        "train_split_size": 131438
    },
    "nguha__legalbench_cuad_renewal_term": {
        "name": "nguha/legalbench",
        "subset": "cuad_renewal_term",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "ucberkeley-dlab__measuring-hate-speech_default": {
        "name": "ucberkeley-dlab/measuring-hate-speech",
        "subset": "default",
        "input": "text",
        "label": "annotator_trans",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 135556
    },
    "kuroneko5943__weibo16_\u4ee3\u5b55": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u4ee3\u5b55",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 822
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--JihadZa--IADD": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--JihadZa--IADD",
        "input": "Sentence",
        "label": "DataSource",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 135804
    },
    "ccdv__arxiv-classification_no_ref": {
        "name": "ccdv/arxiv-classification",
        "subset": "no_ref",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28388
    },
    "senti_lex_gd": {
        "name": "senti_lex",
        "subset": "gd",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 345
    },
    "tyqiangz__multilingual-sentiments_arabic": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "arabic",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "ought__raft_tweet_eval_hate": {
        "name": "ought/raft",
        "subset": "tweet_eval_hate",
        "input": "Tweet",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "DBQ__Mr.Porter.Product.prices.Mexico_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Mexico",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 30806
    },
    "AI-Sweden__SuperLim_swefracas": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swefracas",
        "input": "premiss_1",
        "label": "premiss_5",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 304
    },
    "Genius1237__TyDiP_es": {
        "name": "Genius1237/TyDiP",
        "subset": "es",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "tyqiangz__multilingual-sentiments_japanese": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "japanese",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120000
    },
    "Genius1237__TyDiP_hu": {
        "name": "Genius1237/TyDiP",
        "subset": "hu",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "cjvt__sentinews_sentence_level": {
        "name": "cjvt/sentinews",
        "subset": "sentence_level",
        "input": "content",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 168899
    },
    "gutenberg_time_gutenberg": {
        "name": "gutenberg_time",
        "subset": "gutenberg",
        "input": "tok_context",
        "label": "hour_reference",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120694
    },
    "shmuhammad__AfriSenti-twitter-sentiment_por": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "por",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3063
    },
    "CATIE-AQ__french_book_reviews_fr_prompt_stars_classification_default": {
        "name": "CATIE-AQ/french_book_reviews_fr_prompt_stars_classification",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 212476
    },
    "offenseval2020_tr_offenseval2020-turkish": {
        "name": "offenseval2020_tr",
        "subset": "offenseval2020-turkish",
        "input": "tweet",
        "label": "subtask_a",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 31756
    },
    "statworx__swiss-dialects_default": {
        "name": "statworx/swiss-dialects",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4743
    },
    "nguha__legalbench_contract_nli_permissible_development_of_similar_information": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_permissible_development_of_similar_information",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "tweet_eval_stance_abortion": {
        "name": "tweet_eval",
        "subset": "stance_abortion",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 587
    },
    "ro_sent_default": {
        "name": "ro_sent",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 17941
    },
    "BEE-spoke-data__yahoo_answers_topics-long-text_default": {
        "name": "BEE-spoke-data/yahoo_answers_topics-long-text",
        "subset": "default",
        "input": "text",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3352
    },
    "kor_3i4k_default": {
        "name": "kor_3i4k",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 55134
    },
    "multi_nli_default": {
        "name": "multi_nli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "tweet_eval_hate": {
        "name": "tweet_eval",
        "subset": "hate",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9000
    },
    "lex_glue_case_hold": {
        "name": "lex_glue",
        "subset": "case_hold",
        "input": "context",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 45000
    },
    "kuroneko5943__jd21_\u7535\u89c6": {
        "name": "kuroneko5943/jd21",
        "subset": "\u7535\u89c6",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3617
    },
    "kuroneko5943__stock11_tech": {
        "name": "kuroneko5943/stock11",
        "subset": "tech",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 18069
    },
    "DBQ__Burberry.Product.prices.United.Arab.Emirates_default": {
        "name": "DBQ/Burberry.Product.prices.United.Arab.Emirates",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2554
    },
    "HausaNLP__NaijaSenti-Twitter_ibo": {
        "name": "HausaNLP/NaijaSenti-Twitter",
        "subset": "ibo",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10192
    },
    "nguha__legalbench_opp115_user_access,_edit_and_deletion": {
        "name": "nguha/legalbench",
        "subset": "opp115_user_access,_edit_and_deletion",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "DBQ__Mr.Porter.Product.prices.South.Korea_default": {
        "name": "DBQ/Mr.Porter.Product.prices.South.Korea",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27053
    },
    "matthewfranglen__aste-v2_2016-restaurant-sem-eval": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2016-restaurant-sem-eval",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1394
    },
    "kuroneko5943__jd21_\u667a\u80fd\u624b\u8868": {
        "name": "kuroneko5943/jd21",
        "subset": "\u667a\u80fd\u624b\u8868",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4368
    },
    "DBQ__Prada.Product.prices.United.Kingdom_default": {
        "name": "DBQ/Prada.Product.prices.United.Kingdom",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2180
    },
    "tasksource__mmlu_us_foreign_policy": {
        "name": "tasksource/mmlu",
        "subset": "us_foreign_policy",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "jpcorb20__multidogo_default": {
        "name": "jpcorb20/multidogo",
        "subset": "default",
        "input": "slot-labels",
        "label": "intent",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 170592
    },
    "indonlp__NusaX-senti_bug": {
        "name": "indonlp/NusaX-senti",
        "subset": "bug",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "super_glue_multirc": {
        "name": "super_glue",
        "subset": "multirc",
        "input": "paragraph",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27243
    },
    "silicone_maptask": {
        "name": "silicone",
        "subset": "maptask",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20905
    },
    "DBQ__Burberry.Product.prices.China_default": {
        "name": "DBQ/Burberry.Product.prices.China",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2014
    },
    "classla__FRENK-hate-sl_multiclass": {
        "name": "classla/FRENK-hate-sl",
        "subset": "multiclass",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7189
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--OpenAssistant--oasst1": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--OpenAssistant--oasst1",
        "input": "text",
        "label": "role",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 56
    },
    "davanstrien__test1_default": {
        "name": "davanstrien/test1",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25000
    },
    "AmazonScience__massive_th-TH": {
        "name": "AmazonScience/massive",
        "subset": "th-TH",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "AmazonScience__massive_am-ET": {
        "name": "AmazonScience/massive",
        "subset": "am-ET",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "asoria__draft-list-column_main": {
        "name": "asoria/draft-list-column",
        "subset": "main",
        "input": "text",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7528
    },
    "pragmeval_verifiability": {
        "name": "pragmeval",
        "subset": "verifiability",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5712
    },
    "hpprc__jsick_base": {
        "name": "hpprc/jsick",
        "subset": "base",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4500
    },
    "DBQ__Mr.Porter.Product.prices.Denmark_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Denmark",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27793
    },
    "nguha__legalbench_cuad_no-solicit_of_customers": {
        "name": "nguha/legalbench",
        "subset": "cuad_no-solicit_of_customers",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "cardiffnlp__tweet_topic_single_tweet_topic_single": {
        "name": "cardiffnlp/tweet_topic_single",
        "subset": "tweet_topic_single",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train_all"
        },
        "task_type": "classification",
        "train_split_size": 4374
    },
    "Areeb123__drug_reviews_default": {
        "name": "Areeb123/drug_reviews",
        "subset": "default",
        "input": "review",
        "label": "rating",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 110811
    },
    "matthewfranglen__aste-v2_2015-restaurant-sem-eval": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2015-restaurant-sem-eval",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1013
    },
    "nguha__legalbench_opp115_data_security": {
        "name": "nguha/legalbench",
        "subset": "opp115_data_security",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "asoria__draft-list-column_enriched": {
        "name": "asoria/draft-list-column",
        "subset": "enriched",
        "input": "text",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7528
    },
    "RuterNorway__OpenOrcaNo-15k_default": {
        "name": "RuterNorway/OpenOrcaNo-15k",
        "subset": "default",
        "input": "input",
        "label": "instruction",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15000
    },
    "sbx__superlim-2_argumentation_sent": {
        "name": "sbx/superlim-2",
        "subset": "argumentation_sent",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3450
    },
    "KETI-AIR__kor_anli_default": {
        "name": "KETI-AIR/kor_anli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train_r3"
        },
        "task_type": "classification",
        "train_split_size": 100459
    },
    "tasksource__mmlu_miscellaneous": {
        "name": "tasksource/mmlu",
        "subset": "miscellaneous",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 783
    },
    "kuroneko5943__amz20_GraphicsCard": {
        "name": "kuroneko5943/amz20",
        "subset": "GraphicsCard",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "DBQ__Net.a.Porter.Product.prices.Singapore_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Singapore",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 51236
    },
    "KBLab__overlim_sst_da": {
        "name": "KBLab/overlim",
        "subset": "sst_da",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 66486
    },
    "CATIE-AQ__allocine_fr_prompt_sentiment_analysis_default": {
        "name": "CATIE-AQ/allocine_fr_prompt_sentiment_analysis",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4480000
    },
    "launch__open_question_type_default": {
        "name": "launch/open_question_type",
        "subset": "default",
        "input": "question",
        "label": "resolve_type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3716
    },
    "senti_lex_mk": {
        "name": "senti_lex",
        "subset": "mk",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2965
    },
    "CATIE-AQ__stsb_multi_mt_fr_prompt_sentence_similarity_default": {
        "name": "CATIE-AQ/stsb_multi_mt_fr_prompt_sentence_similarity",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 103482
    },
    "liar_default": {
        "name": "liar",
        "subset": "default",
        "input": "statement",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10269
    },
    "kejian__codeparrot-train-more-filter-3.3b-cleaned_default": {
        "name": "kejian/codeparrot-train-more-filter-3.3b-cleaned",
        "subset": "default",
        "input": "org_text",
        "label": "avg_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 1496692
    },
    "BDas__ArabicNLPDataset_ArabicData": {
        "name": "BDas/ArabicNLPDataset",
        "subset": "ArabicData",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 87334
    },
    "clue_afqmc": {
        "name": "clue",
        "subset": "afqmc",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 34334
    },
    "clickbait_news_bg_default": {
        "name": "clickbait_news_bg",
        "subset": "default",
        "input": "content",
        "label": "fake_news_score",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2815
    },
    "tasksource__mmlu_college_chemistry": {
        "name": "tasksource/mmlu",
        "subset": "college_chemistry",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "masakhane__masakhanews_xho": {
        "name": "masakhane/masakhanews",
        "subset": "xho",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1032
    },
    "Open-Orca__OpenOrca_default": {
        "name": "Open-Orca/OpenOrca",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2914896
    },
    "time_dial_default": {
        "name": "time_dial",
        "subset": "default",
        "input": "correct1",
        "label": "incorrect1_rule",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1446
    },
    "nguha__legalbench_opp115_do_not_track": {
        "name": "nguha/legalbench",
        "subset": "opp115_do_not_track",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "indic_glue_inltkh.ml": {
        "name": "indic_glue",
        "subset": "inltkh.ml",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5036
    },
    "senti_lex_fr": {
        "name": "senti_lex",
        "subset": "fr",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4653
    },
    "ejschwartz__oo-method-test-split_byrow": {
        "name": "ejschwartz/oo-method-test-split",
        "subset": "byrow",
        "input": "Disassembly",
        "label": "Type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 75398
    },
    "UMCU__PubMedCausal_Dutch_translated_with_MariaNMT_default": {
        "name": "UMCU/PubMedCausal_Dutch_translated_with_MariaNMT",
        "subset": "default",
        "input": "input",
        "label": "output",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2368
    },
    "Sangjeong__TestData2_default": {
        "name": "Sangjeong/TestData2",
        "subset": "default",
        "input": "localization",
        "label": "dx",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9577
    },
    "shmuhammad__AfriSenti-twitter-sentiment_pcm": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "pcm",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5121
    },
    "silicone_meld_e": {
        "name": "silicone",
        "subset": "meld_e",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9989
    },
    "tasksource__mmlu_world_religions": {
        "name": "tasksource/mmlu",
        "subset": "world_religions",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 171
    },
    "metaeval__ethics_commonsense": {
        "name": "metaeval/ethics",
        "subset": "commonsense",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13910
    },
    "nguha__legalbench_textualism_tool_plain": {
        "name": "nguha/legalbench",
        "subset": "textualism_tool_plain",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "clarin-pl__polemo2-official_medicine_sentence": {
        "name": "clarin-pl/polemo2-official",
        "subset": "medicine_sentence",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 18126
    },
    "kuroneko5943__amz20_Flashlight": {
        "name": "kuroneko5943/amz20",
        "subset": "Flashlight",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "maveriq__bigbenchhard_date_understanding": {
        "name": "maveriq/bigbenchhard",
        "subset": "date_understanding",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "nguha__legalbench_learned_hands_estates": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_estates",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "masakhane__masakhanews_som": {
        "name": "masakhane/masakhanews",
        "subset": "som",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1021
    },
    "newsph_nli_default": {
        "name": "newsph_nli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 420000
    },
    "strombergnlp__nlpcc-stance_task_a": {
        "name": "strombergnlp/nlpcc-stance",
        "subset": "task_a",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2986
    },
    "DBQ__Mr.Porter.Product.prices.Taiwan_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Taiwan",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27315
    },
    "metaeval__defeasible-nli_atomic": {
        "name": "metaeval/defeasible-nli",
        "subset": "atomic",
        "input": "Update",
        "label": "UpdateType",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35002
    },
    "tasksource__crowdflower_economic-news": {
        "name": "tasksource/crowdflower",
        "subset": "economic-news",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7606
    },
    "silicone_oasis": {
        "name": "silicone",
        "subset": "oasis",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 12076
    },
    "Genius1237__TyDiP_af": {
        "name": "Genius1237/TyDiP",
        "subset": "af",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "PlanTL-GOB-ES__sts-es_STS": {
        "name": "PlanTL-GOB-ES/sts-es",
        "subset": "STS",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 1320
    },
    "fancyzhx__dbpedia_14_dbpedia_14": {
        "name": "fancyzhx/dbpedia_14",
        "subset": "dbpedia_14",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 560000
    },
    "senti_lex_af": {
        "name": "senti_lex",
        "subset": "af",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2299
    },
    "nguha__legalbench_learned_hands_employment": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_employment",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "maveriq__bigbenchhard_object_counting": {
        "name": "maveriq/bigbenchhard",
        "subset": "object_counting",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "yelp_review_full_yelp_review_full": {
        "name": "yelp_review_full",
        "subset": "yelp_review_full",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 650000
    },
    "kuroneko5943__snap21_Sports_and_Outdoors_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Sports_and_Outdoors_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6983
    },
    "ejschwartz__oo-method-test-split_bylibrary": {
        "name": "ejschwartz/oo-method-test-split",
        "subset": "bylibrary",
        "input": "Disassembly",
        "label": "Type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 80940
    },
    "kuroneko5943__snap21_Digital_Music_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Digital_Music_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7002
    },
    "sst2_default": {
        "name": "sst2",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 67349
    },
    "tyqiangz__multilingual-sentiments_german": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "german",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "masakhane__masakhanews_lin": {
        "name": "masakhane/masakhanews",
        "subset": "lin",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 608
    },
    "TimKoornstra__synthetic-financial-tweets-sentiment_default": {
        "name": "TimKoornstra/synthetic-financial-tweets-sentiment",
        "subset": "default",
        "input": "tweet",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 831530
    },
    "Genius1237__TyDiP_ru": {
        "name": "Genius1237/TyDiP",
        "subset": "ru",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "indonlp__NusaX-senti_ace": {
        "name": "indonlp/NusaX-senti",
        "subset": "ace",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "pietrolesci__pubmed-20k-rct_default": {
        "name": "pietrolesci/pubmed-20k-rct",
        "subset": "default",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 176642
    },
    "seara__ru_go_emotions_raw": {
        "name": "seara/ru_go_emotions",
        "subset": "raw",
        "input": "text",
        "label": "created_utc",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 211225
    },
    "senti_lex_ta": {
        "name": "senti_lex",
        "subset": "ta",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2057
    },
    "kuroneko5943__snap21_Musical_Instruments_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Musical_Instruments_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7043
    },
    "sbx__superlim-2_swesat": {
        "name": "sbx/superlim-2",
        "subset": "swesat",
        "input": "item",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 83
    },
    "strombergnlp__nordic_langid_10k": {
        "name": "strombergnlp/nordic_langid",
        "subset": "10k",
        "input": "sentence",
        "label": "language",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 56985
    },
    "KBLab__overlim_wnli_nb": {
        "name": "KBLab/overlim",
        "subset": "wnli_nb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 565
    },
    "GePaSud__TROPICAL_overlapping_subset": {
        "name": "GePaSud/TROPICAL",
        "subset": "overlapping_subset",
        "input": "id_comment",
        "label": "general_polarity",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 787
    },
    "RussianNLP__tape_winograd.raw": {
        "name": "RussianNLP/tape",
        "subset": "winograd.raw",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 804
    },
    "ought__raft_semiconductor_org_types": {
        "name": "ought/raft",
        "subset": "semiconductor_org_types",
        "input": "Paper title",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "webis__Touche23-ValueEval_zhihu": {
        "name": "webis/Touche23-ValueEval",
        "subset": "zhihu",
        "input": "Premise",
        "label": "Stance",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "mlsum_ru": {
        "name": "mlsum",
        "subset": "ru",
        "input": "text",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25556
    },
    "md_gender_bias_new_data": {
        "name": "md_gender_bias",
        "subset": "new_data",
        "input": "text",
        "label": "class_type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2345
    },
    "nguha__legalbench_learned_hands_divorce": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_divorce",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "tyqiangz__multilingual-sentiments_malay": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "malay",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4687
    },
    "nguha__legalbench_cuad_non-compete": {
        "name": "nguha/legalbench",
        "subset": "cuad_non-compete",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "pragmeval_squinky-informativeness": {
        "name": "pragmeval",
        "subset": "squinky-informativeness",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3719
    },
    "nala-cub__americas_nli_hch": {
        "name": "nala-cub/americas_nli",
        "subset": "hch",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "yangwang825__klue-ynat_default": {
        "name": "yangwang825/klue-ynat",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 45678
    },
    "silicone_dyda_da": {
        "name": "silicone",
        "subset": "dyda_da",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 87170
    },
    "paws-x_ko": {
        "name": "paws-x",
        "subset": "ko",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "silicone_iemocap": {
        "name": "silicone",
        "subset": "iemocap",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7213
    },
    "SkAndMl__CPTDS-3_default": {
        "name": "SkAndMl/CPTDS-3",
        "subset": "default",
        "input": "question",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3012
    },
    "nadhifikbarw__id_ohsuhmed_default": {
        "name": "nadhifikbarw/id_ohsuhmed",
        "subset": "default",
        "input": "sentence1",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 53624
    },
    "DBQ__Prada.Product.prices.Portugal_default": {
        "name": "DBQ/Prada.Product.prices.Portugal",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2548
    },
    "kinnews_kirnews_kinnews_cleaned": {
        "name": "kinnews_kirnews",
        "subset": "kinnews_cleaned",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 17014
    },
    "imdb_urdu_reviews_default": {
        "name": "imdb_urdu_reviews",
        "subset": "default",
        "input": "sentence",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50000
    },
    "senti_lex_lt": {
        "name": "senti_lex",
        "subset": "lt",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2190
    },
    "tasksource__mmlu_high_school_biology": {
        "name": "tasksource/mmlu",
        "subset": "high_school_biology",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 310
    },
    "kuroneko5943__weibo16_\u7ea2\u9ec4\u84dd": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u7ea2\u9ec4\u84dd",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 667
    },
    "kuroneko5943__jd21_\u7535\u52a8\u7259\u5237": {
        "name": "kuroneko5943/jd21",
        "subset": "\u7535\u52a8\u7259\u5237",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3556
    },
    "offcombr_offcombr-2": {
        "name": "offcombr",
        "subset": "offcombr-2",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1250
    },
    "kuroneko5943__amz20_Projector": {
        "name": "kuroneko5943/amz20",
        "subset": "Projector",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "AI-Sweden__SuperLim_swesim_similarity": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swesim_similarity",
        "input": "word_1",
        "label": "similarity",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1360
    },
    "AmazonScience__massive_zh-TW": {
        "name": "AmazonScience/massive",
        "subset": "zh-TW",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "snli_plain_text": {
        "name": "snli",
        "subset": "plain_text",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550152
    },
    "xtreme_SQuAD": {
        "name": "xtreme",
        "subset": "SQuAD",
        "input": "context",
        "label": "title",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 87599
    },
    "mwong__climatetext-evidence-related-evaluation_default": {
        "name": "mwong/climatetext-evidence-related-evaluation",
        "subset": "default",
        "input": "claim",
        "label": "evidence",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1978000
    },
    "stsb_mt_sv_plain_text": {
        "name": "stsb_mt_sv",
        "subset": "plain_text",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "sbx__superlim-2_swepar": {
        "name": "sbx/superlim-2",
        "subset": "swepar",
        "input": "sentence_2",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5715
    },
    "nguha__legalbench_cuad_uncapped_liability": {
        "name": "nguha/legalbench",
        "subset": "cuad_uncapped_liability",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "almanach__hc3_french_ood_hc3_en_sentence": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_en_sentence",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 455320
    },
    "grammarly__detexd-benchmark_default": {
        "name": "grammarly/detexd-benchmark",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1023
    },
    "hpprc__janli_original": {
        "name": "hpprc/janli",
        "subset": "original",
        "input": "sentence_A_Ja",
        "label": "entailment_label_Ja",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13680
    },
    "tamilmixsentiment_default": {
        "name": "tamilmixsentiment",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11335
    },
    "nala-cub__americas_nli_bzd": {
        "name": "nala-cub/americas_nli",
        "subset": "bzd",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "rubrix__wildfire_tweets_default": {
        "name": "rubrix/wildfire_tweets",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 104
    },
    "maveriq__bigbenchhard_boolean_expressions": {
        "name": "maveriq/bigbenchhard",
        "subset": "boolean_expressions",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "nguha__legalbench_diversity_1": {
        "name": "nguha/legalbench",
        "subset": "diversity_1",
        "input": "text",
        "label": "aic_is_met",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "hpprc__jsick_stress-original": {
        "name": "hpprc/jsick",
        "subset": "stress-original",
        "input": "pair_ID",
        "label": "entailment_label_Ja",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 900
    },
    "senti_lex_hu": {
        "name": "senti_lex",
        "subset": "hu",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3522
    },
    "laugustyniak__abusive-clauses-pl_abusive-clauses-pl": {
        "name": "laugustyniak/abusive-clauses-pl",
        "subset": "abusive-clauses-pl",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4284
    },
    "evaluate__glue-ci_mrpc": {
        "name": "evaluate/glue-ci",
        "subset": "mrpc",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3668
    },
    "clue_ocnli": {
        "name": "clue",
        "subset": "ocnli",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50437
    },
    "imppres_implicature_quantifiers": {
        "name": "imppres",
        "subset": "implicature_quantifiers",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "quantifiers"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "nguha__legalbench_hearsay": {
        "name": "nguha/legalbench",
        "subset": "hearsay",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5
    },
    "DBQ__Net.a.Porter.Product.prices.Poland_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Poland",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 43226
    },
    "saier__unarXive_imrad_clf_default": {
        "name": "saier/unarXive_imrad_clf",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 520053
    },
    "Sharathhebbar24__app_reviews_modded_default": {
        "name": "Sharathhebbar24/app_reviews_modded",
        "subset": "default",
        "input": "review",
        "label": "star",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 259258
    },
    "nguha__legalbench_learned_hands_domestic_violence": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_domestic_violence",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "kuroneko5943__snap21_Grocery_and_Gourmet_Food_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Grocery_and_Gourmet_Food_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6991
    },
    "kuroneko5943__amz20_Keyboard": {
        "name": "kuroneko5943/amz20",
        "subset": "Keyboard",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "AI-Sweden__SuperLim_swesim_relatedness": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swesim_relatedness",
        "input": "word_1",
        "label": "relatedness",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1360
    },
    "zeio__auto-pale_vanilla": {
        "name": "zeio/auto-pale",
        "subset": "vanilla",
        "input": "text",
        "label": "champion",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 67575
    },
    "fever_v2.0": {
        "name": "fever",
        "subset": "v2.0",
        "input": "claim",
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 2384
    },
    "clarin-pl__polemo2-official_hotels_text": {
        "name": "clarin-pl/polemo2-official",
        "subset": "hotels_text",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3165
    },
    "biglam__lampeter_corpus_default": {
        "name": "biglam/lampeter_corpus",
        "subset": "default",
        "input": "text",
        "label": "genre",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120
    },
    "masakhane__masakhanews_amh": {
        "name": "masakhane/masakhanews",
        "subset": "amh",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1311
    },
    "per_sent_default": {
        "name": "per_sent",
        "subset": "default",
        "input": "DOCUMENT",
        "label": "TRUE_SENTIMENT",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3355
    },
    "nguha__legalbench_learned_hands_health": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_health",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_supply_chain_disclosure_best_practice_training": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_best_practice_training",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "sbx__superlim-2_swenli": {
        "name": "sbx/superlim-2",
        "subset": "swenli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "kor_nli_snli": {
        "name": "kor_nli",
        "subset": "snli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550152
    },
    "nguha__legalbench_cuad_effective_date": {
        "name": "nguha/legalbench",
        "subset": "cuad_effective_date",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_sara_numeric": {
        "name": "nguha/legalbench",
        "subset": "sara_numeric",
        "input": "text",
        "label": "question",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "AmazonScience__massive_hu-HU": {
        "name": "AmazonScience/massive",
        "subset": "hu-HU",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "DBQ__Mr.Porter.Product.prices.Slovakia_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Slovakia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27826
    },
    "tasksource__mmlu_prehistory": {
        "name": "tasksource/mmlu",
        "subset": "prehistory",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 324
    },
    "indic_glue_wstp.bn": {
        "name": "indic_glue",
        "subset": "wstp.bn",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 47580
    },
    "RussianNLP__russian_super_glue_parus": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "parus",
        "input": "premise",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 400
    },
    "senti_lex_sq": {
        "name": "senti_lex",
        "subset": "sq",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2076
    },
    "miam_ilisten": {
        "name": "miam",
        "subset": "ilisten",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1986
    },
    "indic_glue_csqa.kn": {
        "name": "indic_glue",
        "subset": "csqa.kn",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 13666
    },
    "uitnlp__vietnamese_students_feedback_default": {
        "name": "uitnlp/vietnamese_students_feedback",
        "subset": "default",
        "input": "sentence",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11426
    },
    "tasksource__mmlu_moral_scenarios": {
        "name": "tasksource/mmlu",
        "subset": "moral_scenarios",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 895
    },
    "Divyanshu__indicxnli_gu": {
        "name": "Divyanshu/indicxnli",
        "subset": "gu",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "tasksource__mmlu_professional_medicine": {
        "name": "tasksource/mmlu",
        "subset": "professional_medicine",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 272
    },
    "PIXIU-fin__en-fpb_default": {
        "name": "PIXIU-fin/en-fpb",
        "subset": "default",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3100
    },
    "kuroneko5943__snap21_Movies_and_TV_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Movies_and_TV_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6989
    },
    "bing_coronavirus_query_set_country_2020-09-01_2020-09-30": {
        "name": "bing_coronavirus_query_set",
        "subset": "country_2020-09-01_2020-09-30",
        "input": "Query",
        "label": "IsImplicitIntent",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 317856
    },
    "shmuhammad__AfriSenti-twitter-sentiment_tso": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "tso",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 804
    },
    "DBQ__Prada.Product.prices.Italy_default": {
        "name": "DBQ/Prada.Product.prices.Italy",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2533
    },
    "kuroneko5943__weibo16_\u5b59\u5c0f\u679c": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u5b59\u5c0f\u679c",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 605
    },
    "KETI-AIR__kor_amazon_polarity_default": {
        "name": "KETI-AIR/kor_amazon_polarity",
        "subset": "default",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3600000
    },
    "cardiffnlp__tweet_sentiment_multilingual_hindi": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "hindi",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "RussianNLP__russian_super_glue_terra": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "terra",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2616
    },
    "jakartaresearch__indonews_default": {
        "name": "jakartaresearch/indonews",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6127
    },
    "turkish_product_reviews_default": {
        "name": "turkish_product_reviews",
        "subset": "default",
        "input": "sentence",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 235165
    },
    "climatebert__climate_detection_default": {
        "name": "climatebert/climate_detection",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1300
    },
    "nguha__legalbench_learned_hands_torts": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_torts",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "CATIE-AQ__mnli_fr_prompt_textual_entailment_default": {
        "name": "CATIE-AQ/mnli_fr_prompt_textual_entailment",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550000
    },
    "imppres_presupposition_only_presupposition": {
        "name": "imppres",
        "subset": "presupposition_only_presupposition",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "only_presupposition"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "roman_urdu_hate_speech_Coarse_Grained": {
        "name": "roman_urdu_hate_speech",
        "subset": "Coarse_Grained",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7208
    },
    "tasksource__crowdflower_tweet_global_warming": {
        "name": "tasksource/crowdflower",
        "subset": "tweet_global_warming",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4223
    },
    "neurae__dnd_style_intents_default": {
        "name": "neurae/dnd_style_intents",
        "subset": "default",
        "input": "examples",
        "label": "label_names",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 130570
    },
    "yys__OpenOrca-Chinese_default": {
        "name": "yys/OpenOrca-Chinese",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3254888
    },
    "symanto__autextification2023_detection_es": {
        "name": "symanto/autextification2023",
        "subset": "detection_es",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 32062
    },
    "nlu_evaluation_data_default": {
        "name": "nlu_evaluation_data",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 25715
    },
    "KBLab__overlim_rte_nb": {
        "name": "KBLab/overlim",
        "subset": "rte_nb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2214
    },
    "nguha__legalbench_opp115_third_party_sharing_collection": {
        "name": "nguha/legalbench",
        "subset": "opp115_third_party_sharing_collection",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "nguha__legalbench_supply_chain_disclosure_disclosed_training": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_disclosed_training",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "metaeval__defeasible-nli_snli": {
        "name": "metaeval/defeasible-nli",
        "subset": "snli",
        "input": "Premise",
        "label": "UpdateType",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 88674
    },
    "conceptnet5_omcs_sentences_free": {
        "name": "conceptnet5",
        "subset": "omcs_sentences_free",
        "input": "sentence",
        "label": "lang",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 898160
    },
    "health_fact_default": {
        "name": "health_fact",
        "subset": "default",
        "input": "main_text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9832
    },
    "sbx__superlim-2_dalaj-ged": {
        "name": "sbx/superlim-2",
        "subset": "dalaj-ged",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35581
    },
    "bgglue__bgglue_cinexio": {
        "name": "bgglue/bgglue",
        "subset": "cinexio",
        "input": "Comment",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 8155
    },
    "financial_phrasebank_sentences_66agree": {
        "name": "financial_phrasebank",
        "subset": "sentences_66agree",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4217
    },
    "tyqiangz__multilingual-sentiments_spanish": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "spanish",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "bgglue__bgglue_xnlibg": {
        "name": "bgglue/bgglue",
        "subset": "xnlibg",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "AmazonScience__massive_nb-NO": {
        "name": "AmazonScience/massive",
        "subset": "nb-NO",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "kuroneko5943__jd21_\u892a\u9ed1\u7d20": {
        "name": "kuroneko5943/jd21",
        "subset": "\u892a\u9ed1\u7d20",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4268
    },
    "sileod__mindgames_default": {
        "name": "sileod/mindgames",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11174
    },
    "MLNTeam-Unical__NFT-70M_transactions_default": {
        "name": "MLNTeam-Unical/NFT-70M_transactions",
        "subset": "default",
        "input": "tx_timestamp",
        "label": "asset_contract_type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 70972143
    },
    "slava-medvedev__zelensky-speeches_default": {
        "name": "slava-medvedev/zelensky-speeches",
        "subset": "default",
        "input": "full_text",
        "label": "lang",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2044
    },
    "sbx__superlim-2_absabank-imm": {
        "name": "sbx/superlim-2",
        "subset": "absabank-imm",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 3898
    },
    "almanach__hc3_french_ood_hc3_fr_chatgpt_qa": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_fr_chatgpt_qa",
        "input": "chatgpt_answer",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 113
    },
    "indic_glue_csqa.hi": {
        "name": "indic_glue",
        "subset": "csqa.hi",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 35140
    },
    "guardian_authorship_cross_topic_9": {
        "name": "guardian_authorship",
        "subset": "cross_topic_9",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 90
    },
    "ought__raft_overruling": {
        "name": "ought/raft",
        "subset": "overruling",
        "input": "Sentence",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "roman_urdu_default": {
        "name": "roman_urdu",
        "subset": "default",
        "input": "sentence",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20229
    },
    "indonlp__NusaX-senti_sun": {
        "name": "indonlp/NusaX-senti",
        "subset": "sun",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "tasksource__mmlu_jurisprudence": {
        "name": "tasksource/mmlu",
        "subset": "jurisprudence",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 108
    },
    "DBQ__Matches.Fashion.Product.prices.France_default": {
        "name": "DBQ/Matches.Fashion.Product.prices.France",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 44840
    },
    "indic_glue_csqa.as": {
        "name": "indic_glue",
        "subset": "csqa.as",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 2942
    },
    "nguha__legalbench_cuad_governing_law": {
        "name": "nguha/legalbench",
        "subset": "cuad_governing_law",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "kuroneko5943__weibo16_\u6587\u79d1\u751f": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u6587\u79d1\u751f",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 798
    },
    "Genius1237__TyDiP_fr": {
        "name": "Genius1237/TyDiP",
        "subset": "fr",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "McGill-NLP__stereoset_intrasentence": {
        "name": "McGill-NLP/stereoset",
        "subset": "intrasentence",
        "input": "context",
        "label": "target",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 2106
    },
    "assin2_default": {
        "name": "assin2",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "entailment_judgment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6500
    },
    "senti_lex_hy": {
        "name": "senti_lex",
        "subset": "hy",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1657
    },
    "wmt20_mlqe_task2_en-zh": {
        "name": "wmt20_mlqe_task2",
        "subset": "en-zh",
        "input": "pe",
        "label": "hter",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 7000
    },
    "tasksource__mmlu_astronomy": {
        "name": "tasksource/mmlu",
        "subset": "astronomy",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 152
    },
    "classla__FRENK-hate-en_binary": {
        "name": "classla/FRENK-hate-en",
        "subset": "binary",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8404
    },
    "claudios__cubert_ETHPy150Open_function_docstring_datasets": {
        "name": "claudios/cubert_ETHPy150Open",
        "subset": "function_docstring_datasets",
        "input": "docstring",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 340846
    },
    "matthewfranglen__aste-v2_2015-restaurant-aste-v2": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2015-restaurant-aste-v2",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1013
    },
    "mwong__climatetext-claim-climate_evidence-related-evaluation_default": {
        "name": "mwong/climatetext-claim-climate_evidence-related-evaluation",
        "subset": "default",
        "input": "claim",
        "label": "evidence",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1978000
    },
    "ma2za__many_emotions_split": {
        "name": "ma2za/many_emotions",
        "subset": "split",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2439666
    },
    "indic_glue_csqa.or": {
        "name": "indic_glue",
        "subset": "csqa.or",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1975
    },
    "maveriq__bigbenchhard_salient_translation_error_detection": {
        "name": "maveriq/bigbenchhard",
        "subset": "salient_translation_error_detection",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "AmazonScience__massive_ur-PK": {
        "name": "AmazonScience/massive",
        "subset": "ur-PK",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "DBQ__Dior.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Dior.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4426
    },
    "laurievb__open-lid-dataset_default": {
        "name": "laurievb/open-lid-dataset",
        "subset": "default",
        "input": "text",
        "label": "language",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 118296182
    },
    "masakhane__masakhanews_ibo": {
        "name": "masakhane/masakhanews",
        "subset": "ibo",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1356
    },
    "senti_lex_ms": {
        "name": "senti_lex",
        "subset": "ms",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2934
    },
    "silicone_mrda": {
        "name": "silicone",
        "subset": "mrda",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 83943
    },
    "silicone_dyda_e": {
        "name": "silicone",
        "subset": "dyda_e",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 87170
    },
    "kuroneko5943__jd21_\u5e73\u677f\u7535\u8111": {
        "name": "kuroneko5943/jd21",
        "subset": "\u5e73\u677f\u7535\u8111",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3048
    },
    "DBQ__Celine.Product.prices.Germany_default": {
        "name": "DBQ/Celine.Product.prices.Germany",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 655
    },
    "RussianNLP__tape_ru_worldtree.raw": {
        "name": "RussianNLP/tape",
        "subset": "ru_worldtree.raw",
        "input": "question",
        "label": "school_grade",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 118
    },
    "imppres_implicature_numerals_2_3": {
        "name": "imppres",
        "subset": "implicature_numerals_2_3",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "numerals_2_3"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "MichiganNLP__TID-8_humor-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "humor-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 98735
    },
    "DBQ__Net.a.Porter.Product.prices.Portugal_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Portugal",
        "subset": "default",
        "input": "Loulou Toy quilted leather shoulder bag",
        "label": "BAGS",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 44280
    },
    "MichiganNLP__TID-8_hs_brexit-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "hs_brexit-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4480
    },
    "MichiganNLP__TID-8_humor-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "humor-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 97410
    },
    "kuroneko5943__snap21_Arts_Crafts_and_Sewing_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Arts_Crafts_and_Sewing_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6976
    },
    "TimKoornstra__financial-tweets-sentiment_default": {
        "name": "TimKoornstra/financial-tweets-sentiment",
        "subset": "default",
        "input": "tweet",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 38091
    },
    "tsac_default": {
        "name": "tsac",
        "subset": "default",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13669
    },
    "DBQ__Gucci.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Gucci.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4398
    },
    "AmazonScience__massive_ja-JP": {
        "name": "AmazonScience/massive",
        "subset": "ja-JP",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "hezarai__sentiment-dksf_default": {
        "name": "hezarai/sentiment-dksf",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28602
    },
    "kuroneko5943__weibo16_\u97e9\u56fd\u5403\u64ad\u9053\u6b49": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u97e9\u56fd\u5403\u64ad\u9053\u6b49",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 763
    },
    "CATIE-AQ__paws-x_fr_prompt_paraphrase_detection_default": {
        "name": "CATIE-AQ/paws-x_fr_prompt_paraphrase_detection",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1086822
    },
    "joelniklaus__lextreme_swiss_law_area_prediction_sub_area_facts": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_law_area_prediction_sub_area_facts",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10475
    },
    "tasksource__mmlu_high_school_macroeconomics": {
        "name": "tasksource/mmlu",
        "subset": "high_school_macroeconomics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 390
    },
    "senti_lex_fi": {
        "name": "senti_lex",
        "subset": "fi",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3295
    },
    "M-A-D__Mixed-Arabic-Datasets-Repo_Ara--saudinewsnet": {
        "name": "M-A-D/Mixed-Arabic-Datasets-Repo",
        "subset": "Ara--saudinewsnet",
        "input": "content",
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 31030
    },
    "DFKI-SLT__kbp37_kbp37_formatted": {
        "name": "DFKI-SLT/kbp37",
        "subset": "kbp37_formatted",
        "input": "id",
        "label": "relation",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 15807
    },
    "AmazonScience__massive_hy-AM": {
        "name": "AmazonScience/massive",
        "subset": "hy-AM",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "argilla__alpaca-gigo-detector_default": {
        "name": "argilla/alpaca-gigo-detector",
        "subset": "default",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 697
    },
    "metrec_plain_text": {
        "name": "metrec",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 47124
    },
    "urdu_fake_news_default": {
        "name": "urdu_fake_news",
        "subset": "default",
        "input": "news",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 638
    },
    "datacommons_factcheck_weekly_standard": {
        "name": "datacommons_factcheck",
        "subset": "weekly_standard",
        "input": "claim_text",
        "label": "review_rating",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 132
    },
    "pragmeval_persuasiveness-eloquence": {
        "name": "pragmeval",
        "subset": "persuasiveness-eloquence",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 725
    },
    "jlh-ibm__earnings_call_transcript-sentiment": {
        "name": "jlh-ibm/earnings_call",
        "subset": "transcript-sentiment",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6851
    },
    "tasksource__multilingual-zero-shot-label-nli_default": {
        "name": "tasksource/multilingual-zero-shot-label-nli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 878967
    },
    "guardian_authorship_cross_topic_11": {
        "name": "guardian_authorship",
        "subset": "cross_topic_11",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 117
    },
    "mlsum_es": {
        "name": "mlsum",
        "subset": "es",
        "input": "text",
        "label": "date",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 266367
    },
    "nguha__legalbench_opp115_user_choice_control": {
        "name": "nguha/legalbench",
        "subset": "opp115_user_choice_control",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "senti_lex_ja": {
        "name": "senti_lex",
        "subset": "ja",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1017
    },
    "masakhane__masakhanews_swa": {
        "name": "masakhane/masakhanews",
        "subset": "swa",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1658
    },
    "MichiganNLP__TID-8_friends_qia-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "friends_qia-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13113
    },
    "kuroneko5943__weibo16_\u65e5\u672c\u6838\u6c61\u6c34": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u65e5\u672c\u6838\u6c61\u6c34",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 790
    },
    "lchakkei__OpenOrca-Traditional-Chinese_default": {
        "name": "lchakkei/OpenOrca-Traditional-Chinese",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4233915
    },
    "imppres_presupposition_both_presupposition": {
        "name": "imppres",
        "subset": "presupposition_both_presupposition",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "both_presupposition"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "xtreme_PAWS-X.ko": {
        "name": "xtreme",
        "subset": "PAWS-X.ko",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49164
    },
    "senti_lex_ky": {
        "name": "senti_lex",
        "subset": "ky",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 246
    },
    "Divyanshu__indicxnli_ta": {
        "name": "Divyanshu/indicxnli",
        "subset": "ta",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "tasksource__mmlu_formal_logic": {
        "name": "tasksource/mmlu",
        "subset": "formal_logic",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 126
    },
    "cnachteg__duvel_default": {
        "name": "cnachteg/duvel",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6553
    },
    "indic_glue_wstp.gu": {
        "name": "indic_glue",
        "subset": "wstp.gu",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10004
    },
    "nguha__legalbench_cuad_notice_period_to_terminate_renewal": {
        "name": "nguha/legalbench",
        "subset": "cuad_notice_period_to_terminate_renewal",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "senti_lex_sw": {
        "name": "senti_lex",
        "subset": "sw",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1314
    },
    "xtreme_PAWS-X.fr": {
        "name": "xtreme",
        "subset": "PAWS-X.fr",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49399
    },
    "sst_dictionary": {
        "name": "sst",
        "subset": "dictionary",
        "input": "phrase",
        "label": "label",
        "splits": {
            "train": "dictionary"
        },
        "task_type": "regression",
        "train_split_size": 239232
    },
    "joelniklaus__lextreme_swiss_judgment_prediction_xl_facts": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_judgment_prediction_xl_facts",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 197432
    },
    "xtreme_PAWS-X.de": {
        "name": "xtreme",
        "subset": "PAWS-X.de",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49380
    },
    "xtreme_PAWS-X.zh": {
        "name": "xtreme",
        "subset": "PAWS-X.zh",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "jrahn__yolochess_deepblue_default": {
        "name": "jrahn/yolochess_deepblue",
        "subset": "default",
        "input": "fen",
        "label": "result",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 511
    },
    "DBQ__Chloe.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Chloe.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "title",
        "label": "category2_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2431
    },
    "Isotonic__pii-masking-200k_default": {
        "name": "Isotonic/pii-masking-200k",
        "subset": "default",
        "input": "unmasked_text",
        "label": "language",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 209261
    },
    "joelniklaus__lextreme_swiss_law_area_prediction_facts": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_law_area_prediction_facts",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10475
    },
    "indic_glue_iitp-mr.hi": {
        "name": "indic_glue",
        "subset": "iitp-mr.hi",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2480
    },
    "maveriq__bigbenchhard_logical_deduction_seven_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "logical_deduction_seven_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "DBQ__Gucci.Product.prices.Qatar_default": {
        "name": "DBQ/Gucci.Product.prices.Qatar",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5067
    },
    "DBQ__Net.a.Porter.Product.prices.United.Kingdom_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.United.Kingdom",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 56980
    },
    "nguha__legalbench_opp115_policy_change": {
        "name": "nguha/legalbench",
        "subset": "opp115_policy_change",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "catalonia_independence_spanish": {
        "name": "catalonia_independence",
        "subset": "spanish",
        "input": "TWEET",
        "label": "LABEL",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6046
    },
    "senti_lex_et": {
        "name": "senti_lex",
        "subset": "et",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2105
    },
    "ro_sts_ro_sts": {
        "name": "ro_sts",
        "subset": "ro_sts",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "kuroneko5943__stock11_medical": {
        "name": "kuroneko5943/stock11",
        "subset": "medical",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6308
    },
    "MichiganNLP__TID-8_md-agreement-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "md-agreement-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 37077
    },
    "d0rj__dolphin-ru_default": {
        "name": "d0rj/dolphin-ru",
        "subset": "default",
        "input": "input",
        "label": "instruction",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2840090
    },
    "kuroneko5943__snap21_Office_Products_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Office_Products_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7025
    },
    "RussianNLP__tape_per_ethics.raw": {
        "name": "RussianNLP/tape",
        "subset": "per_ethics.raw",
        "input": "text",
        "label": "per_virtue",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 259
    },
    "tasksource__mmlu_moral_disputes": {
        "name": "tasksource/mmlu",
        "subset": "moral_disputes",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 346
    },
    "claritylab__utcd_aspect-normalized-out-of-domain": {
        "name": "claritylab/utcd",
        "subset": "aspect-normalized-out-of-domain",
        "input": "text",
        "label": "aspect",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 119167
    },
    "DBQ__Blickers.Product.prices.France_default": {
        "name": "DBQ/Blickers.Product.prices.France",
        "subset": "default",
        "input": "category3_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7489
    },
    "indic_glue_wnli.gu": {
        "name": "indic_glue",
        "subset": "wnli.gu",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 635
    },
    "bn_hate_speech_default": {
        "name": "bn_hate_speech",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3418
    },
    "yelp_polarity_plain_text": {
        "name": "yelp_polarity",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 560000
    },
    "klue_sts": {
        "name": "klue",
        "subset": "sts",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "source",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11668
    },
    "hda_nli_hindi_HDA nli hindi": {
        "name": "hda_nli_hindi",
        "subset": "HDA nli hindi",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 31892
    },
    "AI-Sweden__SuperLim_sweana": {
        "name": "AI-Sweden/SuperLim",
        "subset": "sweana",
        "input": "a",
        "label": "relation",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 20638
    },
    "strombergnlp__nordic_langid_50k": {
        "name": "strombergnlp/nordic_langid",
        "subset": "50k",
        "input": "sentence",
        "label": "language",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 284231
    },
    "ccdv__arxiv-classification_default": {
        "name": "ccdv/arxiv-classification",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28388
    },
    "AI-team-UoA__greek_legal_code_subject": {
        "name": "AI-team-UoA/greek_legal_code",
        "subset": "subject",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28536
    },
    "fake-news-UFG__FactChecksbr_FakeRecogna": {
        "name": "fake-news-UFG/FactChecksbr",
        "subset": "FakeRecogna",
        "input": "review_text",
        "label": "is_fake",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11773
    },
    "tasksource__mmlu_conceptual_physics": {
        "name": "tasksource/mmlu",
        "subset": "conceptual_physics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 235
    },
    "RussianNLP__russian_super_glue_muserc": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "muserc",
        "input": "paragraph",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11950
    },
    "tasksource__mmlu_high_school_geography": {
        "name": "tasksource/mmlu",
        "subset": "high_school_geography",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 198
    },
    "strombergnlp__polstance_PolStance": {
        "name": "strombergnlp/polstance",
        "subset": "PolStance",
        "input": "quote",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 900
    },
    "clarin-pl__polemo2-official_reviews_text": {
        "name": "clarin-pl/polemo2-official",
        "subset": "reviews_text",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 403
    },
    "shunk031__JGLUE_JSQuAD": {
        "name": "shunk031/JGLUE",
        "subset": "JSQuAD",
        "input": "context",
        "label": "title",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 62859
    },
    "tweet_eval_sentiment": {
        "name": "tweet_eval",
        "subset": "sentiment",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "classification",
        "train_split_size": 45615
    },
    "westphal-jan__mnli_matched_default": {
        "name": "westphal-jan/mnli_matched",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 392702
    },
    "MichiganNLP__TID-8_commitmentbank-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "commitmentbank-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7274
    },
    "senti_lex_is": {
        "name": "senti_lex",
        "subset": "is",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1770
    },
    "hyperpartisan_news_detection_bypublisher": {
        "name": "hyperpartisan_news_detection",
        "subset": "bypublisher",
        "input": "text",
        "label": "bias",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 600000
    },
    "glue_mnli_matched": {
        "name": "glue",
        "subset": "mnli_matched",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 9815
    },
    "MichiganNLP__TID-8_hs_brexit-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "hs_brexit-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4704
    },
    "tyqiangz__multilingual-sentiments_italian": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "italian",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "nguha__legalbench_opp115_international_and_specific_audiences": {
        "name": "nguha/legalbench",
        "subset": "opp115_international_and_specific_audiences",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "kuroneko5943__jd21_\u6d17\u9762\u5976": {
        "name": "kuroneko5943/jd21",
        "subset": "\u6d17\u9762\u5976",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5049
    },
    "md_gender_bias_light_inferred": {
        "name": "md_gender_bias",
        "subset": "light_inferred",
        "input": "text",
        "label": "binary_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 106122
    },
    "cartesinus__leyzer-fedcsis-translated_pl-PL": {
        "name": "cartesinus/leyzer-fedcsis-translated",
        "subset": "pl-PL",
        "input": "bio",
        "label": "domain",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13022
    },
    "yangwang825__reuters-21578_default": {
        "name": "yangwang825/reuters-21578",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5485
    },
    "kuroneko5943__amz20_Sandal": {
        "name": "kuroneko5943/amz20",
        "subset": "Sandal",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "hate_speech18_default": {
        "name": "hate_speech18",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10944
    },
    "llm-book__wrime-sentiment_default": {
        "name": "llm-book/wrime-sentiment",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20149
    },
    "DBQ__Louis.Vuitton.Product.prices.Australia_default": {
        "name": "DBQ/Louis.Vuitton.Product.prices.Australia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6525
    },
    "clarin-pl__polemo2-official_products_text": {
        "name": "clarin-pl/polemo2-official",
        "subset": "products_text",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 387
    },
    "kuroneko5943__weibo16_\u745e\u4e3d\u75ab\u60c5": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u745e\u4e3d\u75ab\u60c5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 663
    },
    "Isotonic__SlimOrca_default": {
        "name": "Isotonic/SlimOrca",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2409134
    },
    "nguha__legalbench_supply_chain_disclosure_disclosed_audits": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_disclosed_audits",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "claritylab__utcd_out-of-domain": {
        "name": "claritylab/utcd",
        "subset": "out-of-domain",
        "input": "text",
        "label": "aspect",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4996673
    },
    "tasksource__mmlu_security_studies": {
        "name": "tasksource/mmlu",
        "subset": "security_studies",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 245
    },
    "tasksource__mmlu_international_law": {
        "name": "tasksource/mmlu",
        "subset": "international_law",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 121
    },
    "metaeval__autotnli_default": {
        "name": "metaeval/autotnli",
        "subset": "default",
        "input": "premises",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 135000
    },
    "gwlms__germeval2018_germeval2018": {
        "name": "gwlms/germeval2018",
        "subset": "germeval2018",
        "input": "text",
        "label": "coarse-grained",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5009
    },
    "gsarti__magpie_magpie": {
        "name": "gsarti/magpie",
        "subset": "magpie",
        "input": "sentence",
        "label": "usage",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 44451
    },
    "Filippo__osdg_cd_main_config": {
        "name": "Filippo/osdg_cd",
        "subset": "main_config",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 42355
    },
    "webis__Touche23-ValueEval_ibm-meta": {
        "name": "webis/Touche23-ValueEval",
        "subset": "ibm-meta",
        "input": "Argument ID",
        "label": "WA",
        "splits": {
            "train": "meta"
        },
        "task_type": "regression",
        "train_split_size": 7368
    },
    "ought__raft_banking_77": {
        "name": "ought/raft",
        "subset": "banking_77",
        "input": "Query",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "senti_lex_ro": {
        "name": "senti_lex",
        "subset": "ro",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3329
    },
    "senti_lex_de": {
        "name": "senti_lex",
        "subset": "de",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3974
    },
    "luist18__ptparl_full": {
        "name": "luist18/ptparl",
        "subset": "full",
        "input": "text",
        "label": "wing",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5713
    },
    "imppres_implicature_gradable_adjective": {
        "name": "imppres",
        "subset": "implicature_gradable_adjective",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label_log",
        "splits": {
            "train": "gradable_adjective"
        },
        "task_type": "classification",
        "train_split_size": 1200
    },
    "financial_phrasebank_sentences_75agree": {
        "name": "financial_phrasebank",
        "subset": "sentences_75agree",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3453
    },
    "MilaNLProc__honest_fr_binary": {
        "name": "MilaNLProc/honest",
        "subset": "fr_binary",
        "input": "template_masked",
        "label": "number",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 810
    },
    "senti_lex_vo": {
        "name": "senti_lex",
        "subset": "vo",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 43
    },
    "medical_questions_pairs_default": {
        "name": "medical_questions_pairs",
        "subset": "default",
        "input": "question_2",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3048
    },
    "kuroneko5943__snap21_Tools_and_Home_Improvement_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Tools_and_Home_Improvement_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6997
    },
    "tasksource__mmlu_high_school_european_history": {
        "name": "tasksource/mmlu",
        "subset": "high_school_european_history",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 165
    },
    "imppres_presupposition_question_presupposition": {
        "name": "imppres",
        "subset": "presupposition_question_presupposition",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "gold_label",
        "splits": {
            "train": "question_presupposition"
        },
        "task_type": "classification",
        "train_split_size": 1900
    },
    "evaluate__glue-ci_mnli_matched": {
        "name": "evaluate/glue-ci",
        "subset": "mnli_matched",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 9815
    },
    "AiresPucrs__stanford-encyclopedia-philosophy_default": {
        "name": "AiresPucrs/stanford-encyclopedia-philosophy",
        "subset": "default",
        "input": "text",
        "label": "metadata",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 182531
    },
    "tasksource__mmlu_college_physics": {
        "name": "tasksource/mmlu",
        "subset": "college_physics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 102
    },
    "nguha__legalbench_cuad_insurance": {
        "name": "nguha/legalbench",
        "subset": "cuad_insurance",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "joelniklaus__lextreme_brazilian_court_decisions_judgment": {
        "name": "joelniklaus/lextreme",
        "subset": "brazilian_court_decisions_judgment",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3234
    },
    "polemo2_out": {
        "name": "polemo2",
        "subset": "out",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5783
    },
    "climate_fever_default": {
        "name": "climate_fever",
        "subset": "default",
        "input": "claim",
        "label": "claim_label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1535
    },
    "mrm8488__CHISTES_spanish_jokes_default": {
        "name": "mrm8488/CHISTES_spanish_jokes",
        "subset": "default",
        "input": "text",
        "label": "category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2419
    },
    "nala-cub__americas_nli_aym": {
        "name": "nala-cub/americas_nli",
        "subset": "aym",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "google_wellformed_query_default": {
        "name": "google_wellformed_query",
        "subset": "default",
        "input": "content",
        "label": "rating",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "regression",
        "train_split_size": 17500
    },
    "AmazonScience__massive_sv-SE": {
        "name": "AmazonScience/massive",
        "subset": "sv-SE",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "hackathon-somos-nlp-2023__informes_discriminacion_gitana_default": {
        "name": "hackathon-somos-nlp-2023/informes_discriminacion_gitana",
        "subset": "default",
        "input": "text",
        "label": "sintetico",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1791
    },
    "pietrolesci__dbpedia_14_indexed_default": {
        "name": "pietrolesci/dbpedia_14_indexed",
        "subset": "default",
        "input": "content",
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 560000
    },
    "DBQ__Burberry.Product.prices.Singapore_default": {
        "name": "DBQ/Burberry.Product.prices.Singapore",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2691
    },
    "senti_lex_ar": {
        "name": "senti_lex",
        "subset": "ar",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2794
    },
    "biu-nlp__alsqa_alsqa": {
        "name": "biu-nlp/alsqa",
        "subset": "alsqa",
        "input": "context",
        "label": "title",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 369
    },
    "poleval2019_cyberbullying_task02": {
        "name": "poleval2019_cyberbullying",
        "subset": "task02",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10041
    },
    "AI-Sweden__SuperLim_swediag": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swediag",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1104
    },
    "nguha__legalbench_cuad_affiliate_license-licensor": {
        "name": "nguha/legalbench",
        "subset": "cuad_affiliate_license-licensor",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nala-cub__americas_nli_quy": {
        "name": "nala-cub/americas_nli",
        "subset": "quy",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "indic_glue_md.hi": {
        "name": "indic_glue",
        "subset": "md.hi",
        "input": "sentence",
        "label": "discourse_mode",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7974
    },
    "kuroneko5943__snap21_Electronics_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Electronics_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7026
    },
    "kuroneko5943__jd21_\u5439\u98ce\u673a": {
        "name": "kuroneko5943/jd21",
        "subset": "\u5439\u98ce\u673a",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4458
    },
    "Divyanshu__indicxnli_as": {
        "name": "Divyanshu/indicxnli",
        "subset": "as",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "mrm8488__sst2-es-mt_default": {
        "name": "mrm8488/sst2-es-mt",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 67349
    },
    "nguha__legalbench_unfair_tos": {
        "name": "nguha/legalbench",
        "subset": "unfair_tos",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9
    },
    "strombergnlp__offenseval_2020_tr": {
        "name": "strombergnlp/offenseval_2020",
        "subset": "tr",
        "input": "text",
        "label": "subtask_a",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 31277
    },
    "dengue_filipino_default": {
        "name": "dengue_filipino",
        "subset": "default",
        "input": "text",
        "label": "absent",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4015
    },
    "DBQ__Balenciaga.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Balenciaga.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "category3_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2307
    },
    "imdb_plain_text": {
        "name": "imdb",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train",
            "validation": "test"
        },
        "task_type": "classification",
        "train_split_size": 25000
    },
    "paws-x_en": {
        "name": "paws-x",
        "subset": "en",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "OxAISH-AL-LLM__wiki_toxic_default": {
        "name": "OxAISH-AL-LLM/wiki_toxic",
        "subset": "default",
        "input": "comment_text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 127656
    },
    "DBQ__Net.a.Porter.Product.prices.Turkey_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Turkey",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 41170
    },
    "biglam__cultural_heritage_metadata_accuracy_default": {
        "name": "biglam/cultural_heritage_metadata_accuracy",
        "subset": "default",
        "input": "metadata_text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 100821
    },
    "senti_lex_bn": {
        "name": "senti_lex",
        "subset": "bn",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2393
    },
    "KBLab__overlim_mrpc_nb": {
        "name": "KBLab/overlim",
        "subset": "mrpc_nb",
        "input": "text_a",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3261
    },
    "pragmeval_emobank-dominance": {
        "name": "pragmeval",
        "subset": "emobank-dominance",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6392
    },
    "social_bias_frames_default": {
        "name": "social_bias_frames",
        "subset": "default",
        "input": "post",
        "label": "annotatorGender",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 112900
    },
    "zeio__auto-pale_quotes": {
        "name": "zeio/auto-pale",
        "subset": "quotes",
        "input": "text",
        "label": "champion",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 31001
    },
    "pn_summary_1.0.0": {
        "name": "pn_summary",
        "subset": "1.0.0",
        "input": "article",
        "label": "network",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 82022
    },
    "nguha__legalbench_cuad_post-termination_services": {
        "name": "nguha/legalbench",
        "subset": "cuad_post-termination_services",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "shunk031__jsnli_without-filtering": {
        "name": "shunk031/jsnli",
        "subset": "without-filtering",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 548014
    },
    "fever_v1.0": {
        "name": "fever",
        "subset": "v1.0",
        "input": "claim",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 311431
    },
    "roman_urdu_hate_speech_Fine_Grained": {
        "name": "roman_urdu_hate_speech",
        "subset": "Fine_Grained",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7208
    },
    "tweet_eval_irony": {
        "name": "tweet_eval",
        "subset": "irony",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2862
    },
    "AmazonScience__massive_sl-SL": {
        "name": "AmazonScience/massive",
        "subset": "sl-SL",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "webis__Touche23-ValueEval_main-level1": {
        "name": "webis/Touche23-ValueEval",
        "subset": "main-level1",
        "input": "Premise",
        "label": "Stance",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5393
    },
    "ohsumed_ohsumed": {
        "name": "ohsumed",
        "subset": "ohsumed",
        "input": "abstract",
        "label": "publication_type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 54709
    },
    "KBLab__overlim_cb_nb": {
        "name": "KBLab/overlim",
        "subset": "cb_nb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 201
    },
    "senti_lex_it": {
        "name": "senti_lex",
        "subset": "it",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4491
    },
    "klue_ynat": {
        "name": "klue",
        "subset": "ynat",
        "input": "title",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 45678
    },
    "kuroneko5943__stock11_traffic": {
        "name": "kuroneko5943/stock11",
        "subset": "traffic",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4919
    },
    "kor_hate_default": {
        "name": "kor_hate",
        "subset": "default",
        "input": "comments",
        "label": "contain_gender_bias",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7896
    },
    "claudios__cubert_ETHPy150Open_exception_datasets": {
        "name": "claudios/cubert_ETHPy150Open",
        "subset": "exception_datasets",
        "input": "function",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 18480
    },
    "tyqiangz__multilingual-sentiments_chinese": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "chinese",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120000
    },
    "bgglue__bgglue_fakenews": {
        "name": "bgglue/bgglue",
        "subset": "fakenews",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1990
    },
    "cjvt__sentinews_document_level": {
        "name": "cjvt/sentinews",
        "subset": "document_level",
        "input": "content",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10427
    },
    "jason9693__APEACH_default": {
        "name": "jason9693/APEACH",
        "subset": "default",
        "input": "text",
        "label": "class",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7896
    },
    "tasksource__crowdflower_airline-sentiment": {
        "name": "tasksource/crowdflower",
        "subset": "airline-sentiment",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 14640
    },
    "tweet_eval_emoji": {
        "name": "tweet_eval",
        "subset": "emoji",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 45000
    },
    "DBQ__Mr.Porter.Product.prices.Russia_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Russia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 26612
    },
    "nguha__legalbench_telemarketing_sales_rule": {
        "name": "nguha/legalbench",
        "subset": "telemarketing_sales_rule",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "ScandEval__scala-sv_default": {
        "name": "ScandEval/scala-sv",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "dbrd_plain_text": {
        "name": "dbrd",
        "subset": "plain_text",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 20028
    },
    "paws-x_zh": {
        "name": "paws-x",
        "subset": "zh",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "stsb_multi_mt_pl": {
        "name": "stsb_multi_mt",
        "subset": "pl",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "senti_lex_te": {
        "name": "senti_lex",
        "subset": "te",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2523
    },
    "senti_lex_gl": {
        "name": "senti_lex",
        "subset": "gl",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2714
    },
    "mlsum_tu": {
        "name": "mlsum",
        "subset": "tu",
        "input": "text",
        "label": "date",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 249277
    },
    "MichiganNLP__TID-8_friends_qia-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "friends_qia-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11238
    },
    "philschmid__emotion_unsplit": {
        "name": "philschmid/emotion",
        "subset": "unsplit",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 416809
    },
    "winddude__finacial_pharsebank_66agree_split_default": {
        "name": "winddude/finacial_pharsebank_66agree_split",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3795
    },
    "danish_political_comments_default": {
        "name": "danish_political_comments",
        "subset": "default",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9008
    },
    "symanto__autextification2023_attribution_en": {
        "name": "symanto/autextification2023",
        "subset": "attribution_en",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 22416
    },
    "KBLab__overlim_stsb_sv": {
        "name": "KBLab/overlim",
        "subset": "stsb_sv",
        "input": "text_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 4312
    },
    "nguha__legalbench_learned_hands_business": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_business",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "paws-x_fr": {
        "name": "paws-x",
        "subset": "fr",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "DBQ__Rue.La.La.Product.prices.United.States_default": {
        "name": "DBQ/Rue.La.La.Product.prices.United.States",
        "subset": "default",
        "input": "title",
        "label": "category2_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7594
    },
    "MilaNLProc__honest_en_binary": {
        "name": "MilaNLProc/honest",
        "subset": "en_binary",
        "input": "template_masked",
        "label": "number",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 810
    },
    "poem_sentiment_default": {
        "name": "poem_sentiment",
        "subset": "default",
        "input": "verse_text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 892
    },
    "KBLab__overlim_qnli_da": {
        "name": "KBLab/overlim",
        "subset": "qnli_da",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 99506
    },
    "senti_lex_zhw": {
        "name": "senti_lex",
        "subset": "zhw",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3828
    },
    "nguha__legalbench_cuad_non-disparagement": {
        "name": "nguha/legalbench",
        "subset": "cuad_non-disparagement",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "almanach__hc3_french_ood_hc3_en_full": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_en_full",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 68335
    },
    "THUDM__LongBench_lcc_e": {
        "name": "THUDM/LongBench",
        "subset": "lcc_e",
        "input": "input",
        "label": "language",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 300
    },
    "MichiganNLP__TID-8_pejorative-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "pejorative-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1112
    },
    "indic_glue_wstp.ml": {
        "name": "indic_glue",
        "subset": "wstp.ml",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27527
    },
    "ttc4900_ttc4900": {
        "name": "ttc4900",
        "subset": "ttc4900",
        "input": "text",
        "label": "category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4900
    },
    "DBQ__Mr.Porter.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27206
    },
    "indic_glue_wstp.hi": {
        "name": "indic_glue",
        "subset": "wstp.hi",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 44069
    },
    "AmazonScience__massive_bn-BD": {
        "name": "AmazonScience/massive",
        "subset": "bn-BD",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nguha__legalbench_diversity_4": {
        "name": "nguha/legalbench",
        "subset": "diversity_4",
        "input": "text",
        "label": "aic_is_met",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "MilaNLProc__honest_ro_binary": {
        "name": "MilaNLProc/honest",
        "subset": "ro_binary",
        "input": "template_masked",
        "label": "number",
        "splits": {
            "train": "honest"
        },
        "task_type": "classification",
        "train_split_size": 840
    },
    "DBQ__Mr.Porter.Product.prices.Romania_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Romania",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27826
    },
    "tasksource__mmlu_management": {
        "name": "tasksource/mmlu",
        "subset": "management",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 103
    },
    "strombergnlp__danfever_DanFever": {
        "name": "strombergnlp/danfever",
        "subset": "DanFever",
        "input": "evidence_extract",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6407
    },
    "zeio__auto-pale_annotated": {
        "name": "zeio/auto-pale",
        "subset": "annotated",
        "input": "text",
        "label": "champion",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 67575
    },
    "masakhane__masakhanews_lug": {
        "name": "masakhane/masakhanews",
        "subset": "lug",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 771
    },
    "paws-x_ja": {
        "name": "paws-x",
        "subset": "ja",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "indic_glue_inltkh.gu": {
        "name": "indic_glue",
        "subset": "inltkh.gu",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5269
    },
    "senti_lex_id": {
        "name": "senti_lex",
        "subset": "id",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2900
    },
    "claritylab__utcd_in-domain": {
        "name": "claritylab/utcd",
        "subset": "in-domain",
        "input": "text",
        "label": "aspect",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2192703
    },
    "biosses_default": {
        "name": "biosses",
        "subset": "default",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 100
    },
    "webis__Touche23-ValueEval_nahjalbalagha-meta": {
        "name": "webis/Touche23-ValueEval",
        "subset": "nahjalbalagha-meta",
        "input": "Premise English",
        "label": "Stance Farsi",
        "splits": {
            "train": "meta"
        },
        "task_type": "classification",
        "train_split_size": 1326
    },
    "md_gender_bias_yelp_inferred": {
        "name": "md_gender_bias",
        "subset": "yelp_inferred",
        "input": "text",
        "label": "binary_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2577862
    },
    "davebulaval__CSMD_meaning_holdout_identical": {
        "name": "davebulaval/CSMD",
        "subset": "meaning_holdout_identical",
        "input": "original",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "regression",
        "train_split_size": 359
    },
    "DBQ__Bottega.Veneta.Product.prices.Australia_default": {
        "name": "DBQ/Bottega.Veneta.Product.prices.Australia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4461
    },
    "nli_tr_snli_tr": {
        "name": "nli_tr",
        "subset": "snli_tr",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550152
    },
    "MichiganNLP__TID-8_goemotions-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "goemotions-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 135504
    },
    "senti_lex_wa": {
        "name": "senti_lex",
        "subset": "wa",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 193
    },
    "lama_google_re": {
        "name": "lama",
        "subset": "google_re",
        "input": "evidences",
        "label": "pred",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6106
    },
    "joelniklaus__lextreme_online_terms_of_service_unfairness_levels": {
        "name": "joelniklaus/lextreme",
        "subset": "online_terms_of_service_unfairness_levels",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2074
    },
    "DBQ__Loro.Piana.Product.prices.Japan_default": {
        "name": "DBQ/Loro.Piana.Product.prices.Japan",
        "subset": "default",
        "input": "website_name",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 685
    },
    "AmazonScience__massive_mn-MN": {
        "name": "AmazonScience/massive",
        "subset": "mn-MN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "rotten_tomatoes_default": {
        "name": "rotten_tomatoes",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "classification",
        "train_split_size": 8530
    },
    "DBQ__Mr.Porter.Product.prices.Sweden_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Sweden",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27695
    },
    "ScandEval__scala-nb_default": {
        "name": "ScandEval/scala-nb",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "indic_glue_csqa.mr": {
        "name": "indic_glue",
        "subset": "csqa.mr",
        "input": "question",
        "label": "category",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 11370
    },
    "disaster_response_messages_default": {
        "name": "disaster_response_messages",
        "subset": "default",
        "input": "message",
        "label": "request",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 21046
    },
    "kuroneko5943__jd21_\u62a4\u80a4\u54c1": {
        "name": "kuroneko5943/jd21",
        "subset": "\u62a4\u80a4\u54c1",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3629
    },
    "THUDM__LongBench_narrativeqa": {
        "name": "THUDM/LongBench",
        "subset": "narrativeqa",
        "input": "input",
        "label": "context",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 200
    },
    "kuroneko5943__weibo16_sex": {
        "name": "kuroneko5943/weibo16",
        "subset": "sex",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 414
    },
    "super_glue_cb": {
        "name": "super_glue",
        "subset": "cb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "omp_posts_labeled": {
        "name": "omp",
        "subset": "posts_labeled",
        "input": "Body",
        "label": "Category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 40567
    },
    "Noxturnix__blognone-20230430_default": {
        "name": "Noxturnix/blognone-20230430",
        "subset": "default",
        "input": "content",
        "label": "author",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 18623
    },
    "maveriq__bigbenchhard_tracking_shuffled_objects_five_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "tracking_shuffled_objects_five_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "sileod__probability_words_nli_usnli": {
        "name": "sileod/probability_words_nli",
        "subset": "usnli",
        "input": "context",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 55517
    },
    "AmazonScience__massive_all": {
        "name": "AmazonScience/massive",
        "subset": "all",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 587214
    },
    "offenseval_dravidian_tamil": {
        "name": "offenseval_dravidian",
        "subset": "tamil",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35139
    },
    "Divyanshu__indicxnli_mr": {
        "name": "Divyanshu/indicxnli",
        "subset": "mr",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "super_glue_rte": {
        "name": "super_glue",
        "subset": "rte",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2490
    },
    "nguha__legalbench_insurance_policy_interpretation": {
        "name": "nguha/legalbench",
        "subset": "insurance_policy_interpretation",
        "input": "claim",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5
    },
    "adv_glue_adv_mnli": {
        "name": "adv_glue",
        "subset": "adv_mnli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 121
    },
    "kuroneko5943__weibo16_\u97e9\u56fd\u5e7d\u7075\u6574\u5bb9": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u97e9\u56fd\u5e7d\u7075\u6574\u5bb9",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 523
    },
    "webis__Touche23-ValueEval_zhihu-meta": {
        "name": "webis/Touche23-ValueEval",
        "subset": "zhihu-meta",
        "input": "Premise Chinese",
        "label": "Conclusion Chinese",
        "splits": {
            "train": "meta"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "clarin-pl__polemo2-official_all_text": {
        "name": "clarin-pl/polemo2-official",
        "subset": "all_text",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6573
    },
    "masakhane__masakhanews_tir": {
        "name": "masakhane/masakhanews",
        "subset": "tir",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 947
    },
    "kuroneko5943__snap21_Kindle_Store_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Kindle_Store_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6990
    },
    "gnad10_default": {
        "name": "gnad10",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9245
    },
    "shmuhammad__AfriSenti-twitter-sentiment_ary": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "ary",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5583
    },
    "metaeval__ethics_virtue": {
        "name": "metaeval/ethics",
        "subset": "virtue",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28245
    },
    "PiC__phrase_similarity_PS-hard": {
        "name": "PiC/phrase_similarity",
        "subset": "PS-hard",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7004
    },
    "senti_lex_da": {
        "name": "senti_lex",
        "subset": "da",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3340
    },
    "stsb_multi_mt_en": {
        "name": "stsb_multi_mt",
        "subset": "en",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "indic_glue_wstp.mr": {
        "name": "indic_glue",
        "subset": "wstp.mr",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10446
    },
    "nguha__legalbench_cuad_price_restrictions": {
        "name": "nguha/legalbench",
        "subset": "cuad_price_restrictions",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "RussianNLP__tape_winograd.episodes": {
        "name": "RussianNLP/tape",
        "subset": "winograd.episodes",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 59
    },
    "tollefj__sts-concatenated-NOB_default": {
        "name": "tollefj/sts-concatenated-NOB",
        "subset": "default",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 2234
    },
    "biglam__clmet_3_1_plain": {
        "name": "biglam/clmet_3_1",
        "subset": "plain",
        "input": "text",
        "label": "period",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 333
    },
    "evaluate__glue-ci_qnli": {
        "name": "evaluate/glue-ci",
        "subset": "qnli",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 104743
    },
    "kuroneko5943__jd21_\u667a\u80fd\u624b\u73af": {
        "name": "kuroneko5943/jd21",
        "subset": "\u667a\u80fd\u624b\u73af",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3536
    },
    "DBQ__Celine.Product.prices.United.Kingdom_default": {
        "name": "DBQ/Celine.Product.prices.United.Kingdom",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 658
    },
    "joelniklaus__lextreme_brazilian_court_decisions_unanimity": {
        "name": "joelniklaus/lextreme",
        "subset": "brazilian_court_decisions_unanimity",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1715
    },
    "nguha__legalbench_citation_prediction_open": {
        "name": "nguha/legalbench",
        "subset": "citation_prediction_open",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2
    },
    "sbx__superlim-2_swesim_similarity": {
        "name": "sbx/superlim-2",
        "subset": "swesim_similarity",
        "input": "word_1",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 131
    },
    "nguha__legalbench_cuad_anti-assignment": {
        "name": "nguha/legalbench",
        "subset": "cuad_anti-assignment",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "hpprc__jsick_original": {
        "name": "hpprc/jsick",
        "subset": "original",
        "input": "sentence_A_En",
        "label": "entailment_label_Ja",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4500
    },
    "DDSC__lcc_default": {
        "name": "DDSC/lcc",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 349
    },
    "tasksource__mmlu_marketing": {
        "name": "tasksource/mmlu",
        "subset": "marketing",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 234
    },
    "jrahn__yolochess_lichess-elite_2211_default": {
        "name": "jrahn/yolochess_lichess-elite_2211",
        "subset": "default",
        "input": "fen",
        "label": "result",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 22116598
    },
    "emotone_ar_default": {
        "name": "emotone_ar",
        "subset": "default",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10065
    },
    "maveriq__bigbenchhard_logical_deduction_five_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "logical_deduction_five_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "kuroneko5943__jd21_\u86cb\u767d\u7c89": {
        "name": "kuroneko5943/jd21",
        "subset": "\u86cb\u767d\u7c89",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3632
    },
    "cartesinus__leyzer-fedcsis-translated_all": {
        "name": "cartesinus/leyzer-fedcsis-translated",
        "subset": "all",
        "input": "bio",
        "label": "domain",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13022
    },
    "glue_rte": {
        "name": "glue",
        "subset": "rte",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2490
    },
    "KBLab__overlim_rte_sv": {
        "name": "KBLab/overlim",
        "subset": "rte_sv",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2214
    },
    "TheBritishLibrary__blbooksgenre_title_genre_classifiction": {
        "name": "TheBritishLibrary/blbooksgenre",
        "subset": "title_genre_classifiction",
        "input": "title",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1736
    },
    "moroco_moroco": {
        "name": "moroco",
        "subset": "moroco",
        "input": "sample",
        "label": "category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 21719
    },
    "tweet_eval_emotion": {
        "name": "tweet_eval",
        "subset": "emotion",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "classification",
        "train_split_size": 3257
    },
    "senti_lex_fa": {
        "name": "senti_lex",
        "subset": "fa",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2477
    },
    "joelniklaus__lextreme_swiss_law_area_prediction_considerations": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_law_area_prediction_considerations",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10475
    },
    "UMCU__HealthAdvice_Dutch_translated_with_MariaNMT_default": {
        "name": "UMCU/HealthAdvice_Dutch_translated_with_MariaNMT",
        "subset": "default",
        "input": "input",
        "label": "output",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8384
    },
    "clarin-pl__polemo2-official_all_sentence": {
        "name": "clarin-pl/polemo2-official",
        "subset": "all_sentence",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 45974
    },
    "sbx__superlim-2_swewinogender": {
        "name": "sbx/superlim-2",
        "subset": "swewinogender",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 624
    },
    "pragmeval_switchboard": {
        "name": "pragmeval",
        "subset": "switchboard",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 18930
    },
    "RussianNLP__rucola_default": {
        "name": "RussianNLP/rucola",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7869
    },
    "hpprc__jsick_stress": {
        "name": "hpprc/jsick",
        "subset": "stress",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 900
    },
    "KBLab__overlim_mnli_nb": {
        "name": "KBLab/overlim",
        "subset": "mnli_nb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 383124
    },
    "classla__FRENK-hate-sl_binary": {
        "name": "classla/FRENK-hate-sl",
        "subset": "binary",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7189
    },
    "guardian_authorship_cross_genre_2": {
        "name": "guardian_authorship",
        "subset": "cross_genre_2",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63
    },
    "pragmeval_squinky-implicature": {
        "name": "pragmeval",
        "subset": "squinky-implicature",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3724
    },
    "AmazonScience__massive_zh-CN": {
        "name": "AmazonScience/massive",
        "subset": "zh-CN",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "yangwang825__marc-ja_default": {
        "name": "yangwang825/marc-ja",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 187528
    },
    "indonlp__NusaX-senti_eng": {
        "name": "indonlp/NusaX-senti",
        "subset": "eng",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "masakhane__masakhanews_eng": {
        "name": "masakhane/masakhanews",
        "subset": "eng",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3309
    },
    "DBQ__Louis.Vuitton.Product.prices.Russia_default": {
        "name": "DBQ/Louis.Vuitton.Product.prices.Russia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6543
    },
    "kuroneko5943__snap21_Industrial_and_Scientific_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Industrial_and_Scientific_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6932
    },
    "banking77_default": {
        "name": "banking77",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10003
    },
    "nguha__legalbench_cuad_competitive_restriction_exception": {
        "name": "nguha/legalbench",
        "subset": "cuad_competitive_restriction_exception",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "DBQ__Loro.Piana.Product.prices.United.States_default": {
        "name": "DBQ/Loro.Piana.Product.prices.United.States",
        "subset": "default",
        "input": "website_name",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1042
    },
    "hebrew_sentiment_token": {
        "name": "hebrew_sentiment",
        "subset": "token",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10244
    },
    "stsb_multi_mt_nl": {
        "name": "stsb_multi_mt",
        "subset": "nl",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "hate_offensive_default": {
        "name": "hate_offensive",
        "subset": "default",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 24783
    },
    "kuroneko5943__amz20_Dumbbell": {
        "name": "kuroneko5943/amz20",
        "subset": "Dumbbell",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "humicroedit_subtask-1": {
        "name": "humicroedit",
        "subset": "subtask-1",
        "input": "original",
        "label": "meanGrade",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 9652
    },
    "super_glue_axb": {
        "name": "super_glue",
        "subset": "axb",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1104
    },
    "indic_glue_bbca.hi": {
        "name": "indic_glue",
        "subset": "bbca.hi",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3467
    },
    "webis__Touche23-ValueEval_main": {
        "name": "webis/Touche23-ValueEval",
        "subset": "main",
        "input": "Premise",
        "label": "Stance",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5393
    },
    "nguha__legalbench_learned_hands_benefits": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_benefits",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "DBQ__Balenciaga.Product.prices.Singapore_default": {
        "name": "DBQ/Balenciaga.Product.prices.Singapore",
        "subset": "default",
        "input": "category3_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2308
    },
    "kuroneko5943__amz20_RiceCooker": {
        "name": "kuroneko5943/amz20",
        "subset": "RiceCooker",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 701
    },
    "yoruba_wordsim353_default": {
        "name": "yoruba_wordsim353",
        "subset": "default",
        "input": "yoruba2",
        "label": "similarity",
        "splits": {
            "train": "test"
        },
        "task_type": "regression",
        "train_split_size": 353
    },
    "shmuhammad__AfriSenti-twitter-sentiment_yor": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "yor",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8522
    },
    "guardian_authorship_cross_genre_1": {
        "name": "guardian_authorship",
        "subset": "cross_genre_1",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63
    },
    "nguha__legalbench_opp115_data_retention": {
        "name": "nguha/legalbench",
        "subset": "opp115_data_retention",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "DBQ__Prada.Product.prices.Spain_default": {
        "name": "DBQ/Prada.Product.prices.Spain",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2528
    },
    "kor_sarcasm_default": {
        "name": "kor_sarcasm",
        "subset": "default",
        "input": "tokens",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9000
    },
    "senti_lex_sl": {
        "name": "senti_lex",
        "subset": "sl",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2244
    },
    "nguha__legalbench_contract_nli_permissible_acquirement_of_similar_information": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_permissible_acquirement_of_similar_information",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "circa_default": {
        "name": "circa",
        "subset": "default",
        "input": "context",
        "label": "goldstandard2",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 34268
    },
    "tasksource__mmlu_computer_security": {
        "name": "tasksource/mmlu",
        "subset": "computer_security",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "maveriq__bigbenchhard_movie_recommendation": {
        "name": "maveriq/bigbenchhard",
        "subset": "movie_recommendation",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "DBQ__Bottega.Veneta.Product.prices.United.States_default": {
        "name": "DBQ/Bottega.Veneta.Product.prices.United.States",
        "subset": "default",
        "input": "title",
        "label": "category2_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4469
    },
    "KBLab__overlim_cb_da": {
        "name": "KBLab/overlim",
        "subset": "cb_da",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 201
    },
    "almanach__hc3_french_ood_qa_fr_binggpt": {
        "name": "almanach/hc3_french_ood",
        "subset": "qa_fr_binggpt",
        "input": "answer",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 106
    },
    "DBQ__Balenciaga.Product.prices.South.Korea_default": {
        "name": "DBQ/Balenciaga.Product.prices.South.Korea",
        "subset": "default",
        "input": "category3_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2038
    },
    "DBQ__Prada.Product.prices.Austria_default": {
        "name": "DBQ/Prada.Product.prices.Austria",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2545
    },
    "ar_sarcasm_default": {
        "name": "ar_sarcasm",
        "subset": "default",
        "input": "tweet",
        "label": "sarcasm",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8437
    },
    "scaredmeow__shopee-reviews-tl-binary_default": {
        "name": "scaredmeow/shopee-reviews-tl-binary",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28000
    },
    "stsb_multi_mt_zh": {
        "name": "stsb_multi_mt",
        "subset": "zh",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "md_gender_bias_name_genders": {
        "name": "md_gender_bias",
        "subset": "name_genders",
        "input": "name",
        "label": "assigned_gender",
        "splits": {
            "train": "yob2008"
        },
        "task_type": "classification",
        "train_split_size": 35079
    },
    "nguha__legalbench_cuad_non-transferable_license": {
        "name": "nguha/legalbench",
        "subset": "cuad_non-transferable_license",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "PNLPhub__FarsTail_FarsTail": {
        "name": "PNLPhub/FarsTail",
        "subset": "FarsTail",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7266
    },
    "stsb_multi_mt_es": {
        "name": "stsb_multi_mt",
        "subset": "es",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "super_glue_boolq": {
        "name": "super_glue",
        "subset": "boolq",
        "input": "passage",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 9427
    },
    "tasksource__mmlu_logical_fallacies": {
        "name": "tasksource/mmlu",
        "subset": "logical_fallacies",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 163
    },
    "nlp-thedeep__humset_2.0.0": {
        "name": "nlp-thedeep/humset",
        "subset": "2.0.0",
        "input": "excerpt",
        "label": "lang",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 129268
    },
    "clarin-pl__polemo2-official_products_sentence": {
        "name": "clarin-pl/polemo2-official",
        "subset": "products_sentence",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5942
    },
    "nguha__legalbench_ssla_company_defendants": {
        "name": "nguha/legalbench",
        "subset": "ssla_company_defendants",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3
    },
    "senti_lex_th": {
        "name": "senti_lex",
        "subset": "th",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1279
    },
    "pragmeval_gum": {
        "name": "pragmeval",
        "subset": "gum",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1700
    },
    "adv_glue_adv_qqp": {
        "name": "adv_glue",
        "subset": "adv_qqp",
        "input": [
            "question1",
            "question2"
        ],
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 78
    },
    "nguha__legalbench_contract_nli_permissible_copy": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_permissible_copy",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "maveriq__bigbenchhard_reasoning_about_colored_objects": {
        "name": "maveriq/bigbenchhard",
        "subset": "reasoning_about_colored_objects",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "kaist-ai__CoT-Collection_en": {
        "name": "kaist-ai/CoT-Collection",
        "subset": "en",
        "input": "source",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1837928
    },
    "sick_default": {
        "name": "sick",
        "subset": "default",
        "input": "sentence_A",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4439
    },
    "tasksource__mmlu_high_school_chemistry": {
        "name": "tasksource/mmlu",
        "subset": "high_school_chemistry",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 203
    },
    "guardian_authorship_cross_topic_4": {
        "name": "guardian_authorship",
        "subset": "cross_topic_4",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 62
    },
    "Divyanshu__indicxnli_ml": {
        "name": "Divyanshu/indicxnli",
        "subset": "ml",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "ckandemir__bitcoin_tweets_sentiment_kaggle_default": {
        "name": "ckandemir/bitcoin_tweets_sentiment_kaggle",
        "subset": "default",
        "input": "text",
        "label": "Sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 77791
    },
    "joelniklaus__lextreme_swiss_judgment_prediction_xl_considerations": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_judgment_prediction_xl_considerations",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 197432
    },
    "AI-Sweden__SuperLim_dalaj": {
        "name": "AI-Sweden/SuperLim",
        "subset": "dalaj",
        "input": "original_sentence",
        "label": "approximate_level",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3841
    },
    "NbAiLab__norwegian_parliament_default": {
        "name": "NbAiLab/norwegian_parliament",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3600
    },
    "DBQ__Mr.Porter.Product.prices.Netherlands_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Netherlands",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27680
    },
    "llm-book__JGLUE_JSTS": {
        "name": "llm-book/JGLUE",
        "subset": "JSTS",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train",
            "validation": "validation"
        },
        "task_type": "regression",
        "train_split_size": 12451
    },
    "scherrmann__financial_phrasebank_75agree_german_default": {
        "name": "scherrmann/financial_phrasebank_75agree_german",
        "subset": "default",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2763
    },
    "maveriq__bigbenchhard_web_of_lies": {
        "name": "maveriq/bigbenchhard",
        "subset": "web_of_lies",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "masakhane__masakhanews_pcm": {
        "name": "masakhane/masakhanews",
        "subset": "pcm",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1060
    },
    "kuroneko5943__weibo16_\u75ab\u82d7": {
        "name": "kuroneko5943/weibo16",
        "subset": "\u75ab\u82d7",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 928
    },
    "nguha__legalbench_consumer_contracts_qa": {
        "name": "nguha/legalbench",
        "subset": "consumer_contracts_qa",
        "input": "contract",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "dutch_social_dutch_social": {
        "name": "dutch_social",
        "subset": "dutch_social",
        "input": "text_translation",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 162805
    },
    "vietgpt__anli_r3_en_default": {
        "name": "vietgpt/anli_r3_en",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 100459
    },
    "glue_mnli": {
        "name": "glue",
        "subset": "mnli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 392702
    },
    "kuroneko5943__jd21_\u6e38\u620f\u673a": {
        "name": "kuroneko5943/jd21",
        "subset": "\u6e38\u620f\u673a",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4077
    },
    "AI-Sweden__SuperLim_swewic": {
        "name": "AI-Sweden/SuperLim",
        "subset": "swewic",
        "input": "sentence_1",
        "label": "same_sense",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1000
    },
    "ought__raft_tai_safety_research": {
        "name": "ought/raft",
        "subset": "tai_safety_research",
        "input": "Abstract Note",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 50
    },
    "cardiffnlp__tweet_sentiment_multilingual_spanish": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "spanish",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "nguha__legalbench_jcrew_blocker": {
        "name": "nguha/legalbench",
        "subset": "jcrew_blocker",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "evaluate__glue-ci_rte": {
        "name": "evaluate/glue-ci",
        "subset": "rte",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2490
    },
    "nsmc_default": {
        "name": "nsmc",
        "subset": "default",
        "input": "document",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 150000
    },
    "nguha__legalbench_learned_hands_family": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_family",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "tasksource__lingnli_default": {
        "name": "tasksource/lingnli",
        "subset": "default",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 44982
    },
    "indic_glue_wstp.kn": {
        "name": "indic_glue",
        "subset": "wstp.kn",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35379
    },
    "offcombr_offcombr-3": {
        "name": "offcombr",
        "subset": "offcombr-3",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1033
    },
    "trec_default": {
        "name": "trec",
        "subset": "default",
        "input": "text",
        "label": "coarse_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5452
    },
    "AmazonScience__massive_he-IL": {
        "name": "AmazonScience/massive",
        "subset": "he-IL",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "DBQ__Chanel.Product.prices.Germany_default": {
        "name": "DBQ/Chanel.Product.prices.Germany",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1428
    },
    "facebook__asset_ratings": {
        "name": "facebook/asset",
        "subset": "ratings",
        "input": "original",
        "label": "aspect",
        "splits": {
            "train": "full"
        },
        "task_type": "classification",
        "train_split_size": 4500
    },
    "webis__Touche23-ValueEval_zhihu-level1": {
        "name": "webis/Touche23-ValueEval",
        "subset": "zhihu-level1",
        "input": "Premise",
        "label": "Stance",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "DBQ__Louis.Vuitton.Product.prices.United.Kingdom_default": {
        "name": "DBQ/Louis.Vuitton.Product.prices.United.Kingdom",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7741
    },
    "KBLab__overlim_qnli_nb": {
        "name": "KBLab/overlim",
        "subset": "qnli_nb",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 99506
    },
    "DBQ__Blickers.Product.prices.United.Kingdom_default": {
        "name": "DBQ/Blickers.Product.prices.United.Kingdom",
        "subset": "default",
        "input": "category3_code",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 28256
    },
    "hausa_voa_topics_default": {
        "name": "hausa_voa_topics",
        "subset": "default",
        "input": "news_title",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2045
    },
    "RussianNLP__tape_ru_openbook.episodes": {
        "name": "RussianNLP/tape",
        "subset": "ru_openbook.episodes",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 48
    },
    "maveriq__bigbenchhard_dyck_languages": {
        "name": "maveriq/bigbenchhard",
        "subset": "dyck_languages",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "ohidaoui__darija-reviews_default": {
        "name": "ohidaoui/darija-reviews",
        "subset": "default",
        "input": "review",
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 851
    },
    "shmuhammad__AfriSenti-twitter-sentiment_kin": {
        "name": "shmuhammad/AfriSenti-twitter-sentiment",
        "subset": "kin",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3302
    },
    "facebook__anli_plain_text": {
        "name": "facebook/anli",
        "subset": "plain_text",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train_r3"
        },
        "task_type": "classification",
        "train_split_size": 100459
    },
    "tasksource__mmlu_global_facts": {
        "name": "tasksource/mmlu",
        "subset": "global_facts",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "TheBritishLibrary__blbooksgenre_annotated_raw": {
        "name": "TheBritishLibrary/blbooksgenre",
        "subset": "annotated_raw",
        "input": "Title",
        "label": "Type of resource",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4398
    },
    "peixian__equity_evaluation_corpus_first_domain": {
        "name": "peixian/equity_evaluation_corpus",
        "subset": "first_domain",
        "input": "sentence",
        "label": "template",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8640
    },
    "adv_glue_adv_qnli": {
        "name": "adv_glue",
        "subset": "adv_qnli",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "validation"
        },
        "task_type": "classification",
        "train_split_size": 148
    },
    "DBQ__Mr.Porter.Product.prices.Croatia_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Croatia",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27570
    },
    "senti_lex_el": {
        "name": "senti_lex",
        "subset": "el",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2703
    },
    "Zahra99__IEMOCAP_Text_default": {
        "name": "Zahra99/IEMOCAP_Text",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "session5"
        },
        "task_type": "classification",
        "train_split_size": 1241
    },
    "claritylab__utcd_aspect-normalized-in-domain": {
        "name": "claritylab/utcd",
        "subset": "aspect-normalized-in-domain",
        "input": "text",
        "label": "aspect",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 115127
    },
    "mwong__climatetext-evidence-claim-pair-related-evaluation_default": {
        "name": "mwong/climatetext-evidence-claim-pair-related-evaluation",
        "subset": "default",
        "input": "claim",
        "label": "evidence",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 1978000
    },
    "MichiganNLP__TID-8_sentiment-ann": {
        "name": "MichiganNLP/TID-8",
        "subset": "sentiment-ann",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 59235
    },
    "clue_cmnli": {
        "name": "clue",
        "subset": "cmnli",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 391783
    },
    "guardian_authorship_cross_topic_12": {
        "name": "guardian_authorship",
        "subset": "cross_topic_12",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 117
    },
    "DBQ__Prada.Product.prices.Hong.Kong_default": {
        "name": "DBQ/Prada.Product.prices.Hong.Kong",
        "subset": "default",
        "input": "title",
        "label": "brand",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1692
    },
    "clarin-pl__polemo2-official_reviews_sentence": {
        "name": "clarin-pl/polemo2-official",
        "subset": "reviews_sentence",
        "input": "text",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2025
    },
    "AmazonScience__massive_id-ID": {
        "name": "AmazonScience/massive",
        "subset": "id-ID",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "erhwenkuo__openorca-chinese-zhtw_default": {
        "name": "erhwenkuo/openorca-chinese-zhtw",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4233915
    },
    "KBLab__overlim_qqp_sv": {
        "name": "KBLab/overlim",
        "subset": "qqp_sv",
        "input": "text_b",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 323419
    },
    "RussianNLP__russian_super_glue_rwsd": {
        "name": "RussianNLP/russian_super_glue",
        "subset": "rwsd",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 606
    },
    "nguha__legalbench_cuad_most_favored_nation": {
        "name": "nguha/legalbench",
        "subset": "cuad_most_favored_nation",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_contract_nli_notice_on_compelled_disclosure": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_notice_on_compelled_disclosure",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "allocine_allocine": {
        "name": "allocine",
        "subset": "allocine",
        "input": "review",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 160000
    },
    "ScandEval__norec-mini_default": {
        "name": "ScandEval/norec-mini",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "onestop_english_default": {
        "name": "onestop_english",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 567
    },
    "nguha__legalbench_learned_hands_consumer": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_consumer",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_cuad_joint_ip_ownership": {
        "name": "nguha/legalbench",
        "subset": "cuad_joint_ip_ownership",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nkazi__SciEntsBank_default": {
        "name": "nkazi/SciEntsBank",
        "subset": "default",
        "input": "question",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4969
    },
    "senti_lex_br": {
        "name": "senti_lex",
        "subset": "br",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 184
    },
    "nguha__legalbench_sara_entailment": {
        "name": "nguha/legalbench",
        "subset": "sara_entailment",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "senti_lex_pl": {
        "name": "senti_lex",
        "subset": "pl",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3533
    },
    "nala-cub__americas_nli_cni": {
        "name": "nala-cub/americas_nli",
        "subset": "cni",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 750
    },
    "jakartaresearch__google-play-review_default": {
        "name": "jakartaresearch/google-play-review",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7028
    },
    "catalonia_independence_catalan": {
        "name": "catalonia_independence",
        "subset": "catalan",
        "input": "TWEET",
        "label": "LABEL",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6028
    },
    "ScandEval__absabank-imm-mini_default": {
        "name": "ScandEval/absabank-imm-mini",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "hans_plain_text": {
        "name": "hans",
        "subset": "plain_text",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 30000
    },
    "masakhane__masakhanews_run": {
        "name": "masakhane/masakhanews",
        "subset": "run",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1117
    },
    "MichiganNLP__TID-8_sentiment-atr": {
        "name": "MichiganNLP/TID-8",
        "subset": "sentiment-atr",
        "input": "question",
        "label": "answer_label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 42439
    },
    "guardian_authorship_cross_topic_5": {
        "name": "guardian_authorship",
        "subset": "cross_topic_5",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 62
    },
    "pragmeval_persuasiveness-strength": {
        "name": "pragmeval",
        "subset": "persuasiveness-strength",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 371
    },
    "nlp-thedeep__humset_1.0.0": {
        "name": "nlp-thedeep/humset",
        "subset": "1.0.0",
        "input": "excerpt",
        "label": "lang",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 117435
    },
    "KBLab__overlim_cb_sv": {
        "name": "KBLab/overlim",
        "subset": "cb_sv",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 201
    },
    "prachathai67k_prachathai67k": {
        "name": "prachathai67k",
        "subset": "prachathai67k",
        "input": "body_text",
        "label": "politics",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 54379
    },
    "paws_unlabeled_final": {
        "name": "paws",
        "subset": "unlabeled_final",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 645652
    },
    "ScandEval__swerec-mini_default": {
        "name": "ScandEval/swerec-mini",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "ilist_default": {
        "name": "ilist",
        "subset": "default",
        "input": "text",
        "label": "language_id",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 70351
    },
    "tasksource__mmlu_high_school_computer_science": {
        "name": "tasksource/mmlu",
        "subset": "high_school_computer_science",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 100
    },
    "DBQ__Loro.Piana.Product.prices.Germany_default": {
        "name": "DBQ/Loro.Piana.Product.prices.Germany",
        "subset": "default",
        "input": "website_name",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1133
    },
    "ethos_multilabel": {
        "name": "ethos",
        "subset": "multilabel",
        "input": "text",
        "label": "violence",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 433
    },
    "hate_speech_portuguese_default": {
        "name": "hate_speech_portuguese",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5670
    },
    "DBQ__Net.a.Porter.Product.prices.Austria_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Austria",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 43237
    },
    "kuroneko5943__snap21_Prime_Pantry_5": {
        "name": "kuroneko5943/snap21",
        "subset": "Prime_Pantry_5",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7002
    },
    "DBQ__Net.a.Porter.Product.prices.Canada_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Canada",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 43168
    },
    "HausaNLP__NaijaSenti-Twitter_hau": {
        "name": "HausaNLP/NaijaSenti-Twitter",
        "subset": "hau",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 14172
    },
    "tasksource__mmlu_high_school_government_and_politics": {
        "name": "tasksource/mmlu",
        "subset": "high_school_government_and_politics",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 193
    },
    "fake-news-UFG__FactChecksbr_fakebr": {
        "name": "fake-news-UFG/FactChecksbr",
        "subset": "fakebr",
        "input": "claim_text",
        "label": "is_fake",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7200
    },
    "twi_wordsim353_default": {
        "name": "twi_wordsim353",
        "subset": "default",
        "input": "twi2",
        "label": "similarity",
        "splits": {
            "train": "test"
        },
        "task_type": "regression",
        "train_split_size": 274
    },
    "tasksource__PLANE-ood_default": {
        "name": "tasksource/PLANE-ood",
        "subset": "default",
        "input": "seq",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 300132
    },
    "classla__FRENK-hate-hr_binary": {
        "name": "classla/FRENK-hate-hr",
        "subset": "binary",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7964
    },
    "DBQ__Mr.Porter.Product.prices.Qatar_default": {
        "name": "DBQ/Mr.Porter.Product.prices.Qatar",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27140
    },
    "miam_loria": {
        "name": "miam",
        "subset": "loria",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8465
    },
    "shunk031__jsnli_with-filtering": {
        "name": "shunk031/jsnli",
        "subset": "with-filtering",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 533005
    },
    "matthewfranglen__aste-v2_2014-laptop-sem-eval": {
        "name": "matthewfranglen/aste-v2",
        "subset": "2014-laptop-sem-eval",
        "input": "text",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1450
    },
    "kinnews_kirnews_kirnews_raw": {
        "name": "kinnews_kirnews",
        "subset": "kirnews_raw",
        "input": "content",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3689
    },
    "tasksource__mmlu_anatomy": {
        "name": "tasksource/mmlu",
        "subset": "anatomy",
        "input": "question",
        "label": "answer",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 135
    },
    "kor_sae_default": {
        "name": "kor_sae",
        "subset": "default",
        "input": "intent_pair1",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 30837
    },
    "bleugreen__typescript-instruct_default": {
        "name": "bleugreen/typescript-instruct",
        "subset": "default",
        "input": "content",
        "label": "type",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 41109
    },
    "nguha__legalbench_cuad_audit_rights": {
        "name": "nguha/legalbench",
        "subset": "cuad_audit_rights",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_learned_hands_traffic": {
        "name": "nguha/legalbench",
        "subset": "learned_hands_traffic",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "nguha__legalbench_cuad_volume_restriction": {
        "name": "nguha/legalbench",
        "subset": "cuad_volume_restriction",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "ScandEval__scala-is_default": {
        "name": "ScandEval/scala-is",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1024
    },
    "ag_news_default": {
        "name": "ag_news",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120000
    },
    "CATIE-AQ__fever_fr_prompt_textual_entailment_default": {
        "name": "CATIE-AQ/fever_fr_prompt_textual_entailment",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 550000
    },
    "senti_lex_ko": {
        "name": "senti_lex",
        "subset": "ko",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2118
    },
    "KETI-AIR__kor_ag_news_default": {
        "name": "KETI-AIR/kor_ag_news",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120000
    },
    "almanach__hc3_french_ood_hc3_en_qa": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_en_qa",
        "input": "answer",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 68335
    },
    "THUDM__LongBench_repobench-p": {
        "name": "THUDM/LongBench",
        "subset": "repobench-p",
        "input": "input",
        "label": "language",
        "splits": {
            "train": "test"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "DBQ__Chanel.Product.prices.Italy_default": {
        "name": "DBQ/Chanel.Product.prices.Italy",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1426
    },
    "librarian-bots__model_card_dataset_mentions_default": {
        "name": "librarian-bots/model_card_dataset_mentions",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 297
    },
    "dlb__plue_snli": {
        "name": "dlb/plue",
        "subset": "snli",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 510711
    },
    "senti_lex_tl": {
        "name": "senti_lex",
        "subset": "tl",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1858
    },
    "nguha__legalbench_contract_nli_explicit_identification": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_explicit_identification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "nguha__legalbench_contract_nli_inclusion_of_verbally_conveyed_information": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_inclusion_of_verbally_conveyed_information",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "indic_glue_wstp.te": {
        "name": "indic_glue",
        "subset": "wstp.te",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 80000
    },
    "wmt20_mlqe_task2_en-de": {
        "name": "wmt20_mlqe_task2",
        "subset": "en-de",
        "input": "pe",
        "label": "hter",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 7000
    },
    "shunk031__JGLUE_JCoLA": {
        "name": "shunk031/JGLUE",
        "subset": "JCoLA",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6919
    },
    "told-br_binary": {
        "name": "told-br",
        "subset": "binary",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 16800
    },
    "cardiffnlp__tweet_sentiment_multilingual_french": {
        "name": "cardiffnlp/tweet_sentiment_multilingual",
        "subset": "french",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "georgesung__OpenOrca_35k_default": {
        "name": "georgesung/OpenOrca_35k",
        "subset": "default",
        "input": "question",
        "label": "system_prompt",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 35000
    },
    "kuroneko5943__jd21_\u7ef4\u751f\u7d20": {
        "name": "kuroneko5943/jd21",
        "subset": "\u7ef4\u751f\u7d20",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3641
    },
    "clinc_oos_small": {
        "name": "clinc_oos",
        "subset": "small",
        "input": "text",
        "label": "intent",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7600
    },
    "tasksource__naturallogic_default": {
        "name": "tasksource/naturallogic",
        "subset": "default",
        "input": " sent2 ",
        "label": " original_label ",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6390
    },
    "senti_lex_fy": {
        "name": "senti_lex",
        "subset": "fy",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 224
    },
    "KBLab__overlim_sst_sv": {
        "name": "KBLab/overlim",
        "subset": "sst_sv",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 66486
    },
    "stsb_multi_mt_de": {
        "name": "stsb_multi_mt",
        "subset": "de",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "similarity_score",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 5749
    },
    "DBQ__Chloe.Product.prices.South.Korea_default": {
        "name": "DBQ/Chloe.Product.prices.South.Korea",
        "subset": "default",
        "input": "title",
        "label": "category2_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 2439
    },
    "joelniklaus__lextreme_swiss_judgment_prediction": {
        "name": "joelniklaus/lextreme",
        "subset": "swiss_judgment_prediction",
        "input": "input",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 59709
    },
    "nguha__legalbench_contract_nli_confidentiality_of_agreement": {
        "name": "nguha/legalbench",
        "subset": "contract_nli_confidentiality_of_agreement",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "almanach__hc3_french_ood_hc3_fr_sentence": {
        "name": "almanach/hc3_french_ood",
        "subset": "hc3_fr_sentence",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 464885
    },
    "tyqiangz__multilingual-sentiments_portuguese": {
        "name": "tyqiangz/multilingual-sentiments",
        "subset": "portuguese",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1839
    },
    "rexarski__TCFD_disclosure_default": {
        "name": "rexarski/TCFD_disclosure",
        "subset": "default",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 593
    },
    "TheBritishLibrary__web_archive_classification_default": {
        "name": "TheBritishLibrary/web_archive_classification",
        "subset": "default",
        "input": "title",
        "label": "primary_category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 26909
    },
    "paws-x_de": {
        "name": "paws-x",
        "subset": "de",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 49401
    },
    "super_glue_copa": {
        "name": "super_glue",
        "subset": "copa",
        "input": "premise",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 400
    },
    "indonlp__NusaX-senti_min": {
        "name": "indonlp/NusaX-senti",
        "subset": "min",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 500
    },
    "tasksource__crowdflower_political-media-message": {
        "name": "tasksource/crowdflower",
        "subset": "political-media-message",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4999
    },
    "AmazonScience__massive_es-ES": {
        "name": "AmazonScience/massive",
        "subset": "es-ES",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "silicone_swda": {
        "name": "silicone",
        "subset": "swda",
        "input": "Utterance",
        "label": "Label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 190709
    },
    "biglam__atypical_animacy_default": {
        "name": "biglam/atypical_animacy",
        "subset": "default",
        "input": "sentence",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 594
    },
    "CATIE-AQ__DFP_default": {
        "name": "CATIE-AQ/DFP",
        "subset": "default",
        "input": "inputs",
        "label": "targets",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 102720891
    },
    "pragmeval_persuasiveness-relevance": {
        "name": "pragmeval",
        "subset": "persuasiveness-relevance",
        "input": [
            "sentence1",
            "sentence2"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 725
    },
    "davebulaval__CSMD_meaning": {
        "name": "davebulaval/CSMD",
        "subset": "meaning",
        "input": "original",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "regression",
        "train_split_size": 853
    },
    "HausaNLP__NaijaSenti-Twitter_pcm": {
        "name": "HausaNLP/NaijaSenti-Twitter",
        "subset": "pcm",
        "input": "tweet",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 5121
    },
    "LennardZuendorf__openlegaldata-processed_default": {
        "name": "LennardZuendorf/openlegaldata-processed",
        "subset": "default",
        "input": "content",
        "label": "type",
        "splits": {
            "train": "two"
        },
        "task_type": "classification",
        "train_split_size": 4954
    },
    "metaeval__defeasible-nli_social": {
        "name": "metaeval/defeasible-nli",
        "subset": "social",
        "input": "Hypothesis",
        "label": "UpdateType",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 77016
    },
    "conceptnet5_conceptnet5": {
        "name": "conceptnet5",
        "subset": "conceptnet5",
        "input": "sentence",
        "label": "extra_info",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 34074917
    },
    "pietrolesci__agnews_default": {
        "name": "pietrolesci/agnews",
        "subset": "default",
        "input": "text",
        "label": "labels",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 120000
    },
    "KBLab__overlim_boolq_nb": {
        "name": "KBLab/overlim",
        "subset": "boolq_nb",
        "input": "passage",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6285
    },
    "fake-news-UFG__fakebr_size_normalized_texts": {
        "name": "fake-news-UFG/fakebr",
        "subset": "size_normalized_texts",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 7200
    },
    "hpprc__janli_base": {
        "name": "hpprc/janli",
        "subset": "base",
        "input": [
            "premise",
            "hypothesis"
        ],
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 13680
    },
    "AmazonScience__massive_en-US": {
        "name": "AmazonScience/massive",
        "subset": "en-US",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "nguha__legalbench_textualism_tool_dictionaries": {
        "name": "nguha/legalbench",
        "subset": "textualism_tool_dictionaries",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 4
    },
    "maveriq__bigbenchhard_navigate": {
        "name": "maveriq/bigbenchhard",
        "subset": "navigate",
        "input": "input",
        "label": "target",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 250
    },
    "DBQ__Net.a.Porter.Product.prices.Macao_default": {
        "name": "DBQ/Net.a.Porter.Product.prices.Macao",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 51420
    },
    "nguha__legalbench_diversity_3": {
        "name": "nguha/legalbench",
        "subset": "diversity_3",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "AmazonScience__massive_el-GR": {
        "name": "AmazonScience/massive",
        "subset": "el-GR",
        "input": "annot_utt",
        "label": "scenario",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 11514
    },
    "indic_glue_wstp.ta": {
        "name": "indic_glue",
        "subset": "wstp.ta",
        "input": "sectionText",
        "label": "correctTitle",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 48940
    },
    "poleval2019_cyberbullying_task01": {
        "name": "poleval2019_cyberbullying",
        "subset": "task01",
        "input": "text",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 10041
    },
    "senti_lex_mr": {
        "name": "senti_lex",
        "subset": "mr",
        "input": "word",
        "label": "sentiment",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 1825
    },
    "guardian_authorship_cross_genre_3": {
        "name": "guardian_authorship",
        "subset": "cross_genre_3",
        "input": "article",
        "label": "topic",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 63
    },
    "nguha__legalbench_cuad_minimum_commitment": {
        "name": "nguha/legalbench",
        "subset": "cuad_minimum_commitment",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 6
    },
    "DBQ__Mr.Porter.Product.prices.South.Africa_default": {
        "name": "DBQ/Mr.Porter.Product.prices.South.Africa",
        "subset": "default",
        "input": "title",
        "label": "category1_code",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 27354
    },
    "nguha__legalbench_supply_chain_disclosure_best_practice_certification": {
        "name": "nguha/legalbench",
        "subset": "supply_chain_disclosure_best_practice_certification",
        "input": "text",
        "label": "answer",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 8
    },
    "sbx__superlim-2_swedn": {
        "name": "sbx/superlim-2",
        "subset": "swedn",
        "input": "article",
        "label": "article_category",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 29847
    },
    "kuroneko5943__jd21_\u9152": {
        "name": "kuroneko5943/jd21",
        "subset": "\u9152",
        "input": "sentence",
        "label": "label",
        "splits": {
            "train": "train"
        },
        "task_type": "classification",
        "train_split_size": 3872
    }
}