{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c90ef0",
   "metadata": {},
   "source": [
    "# Tutorial 3: Train your own ESM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e2ed7",
   "metadata": {},
   "source": [
    "In this tutorial, we will train our own ESM and publish it on the Hugging Face Hub, such that others can use it for intermediate task selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b0230c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from hfselect import ESMTrainer, Dataset\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d35da1",
   "metadata": {},
   "source": [
    "ESMs should always be trained by embedding a dataset once with a base model and once with the base model after being fine-tuned on the same dataset. For example, we train BERT on the IMDB dataset. Then, we embed the IMDB dataset using BERT and the fine-tuned version of BERT. The ESM will be trained on the resulting pairs of embeddings.\n",
    "\n",
    "For the sake of this tutorial we will work around this, as we don't want to fine-tune a language model. We will use BERT, a fine-tuned version of BERT from the HF Hub, and the dataset that is lists as being fine-tuned on. Furthermore, we will not embed the complete train dataset but only a sample.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "<strong>*We advise you to embed the exact same dataset (including its length) for embedding as used for fine-tuning the language model. This way, other users know exactly what the ESM represents.\n",
    "</strong></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e47b58",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "base_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tuned_model = BertModel.from_pretrained(\"prithivMLmods/Spam-Bert-Uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b565dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_hugging_face(\n",
    "    name=\"prithivMLmods/Spam-Text-Detect-Analysis\",\n",
    "    split=\"train\",\n",
    "    text_col=\"Message\",\n",
    "    label_col=\"Category\",\n",
    "    is_regression=False,\n",
    "    num_examples=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942656ba",
   "metadata": {},
   "source": [
    "# Training the ESM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d0964",
   "metadata": {},
   "source": [
    "We train the ESM with the default parameter for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d1f2ce",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b48ed9a15f48b9808829eb37403afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embedding dataset:   0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7345f3c2f5664c0bb1089714b5000979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training ESM:   0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_name = \"cpu\" #Change this to cuda if you want to use a GPU\n",
    "\n",
    "trainer = ESMTrainer(device_name=device_name)\n",
    "\n",
    "esm = trainer.train_with_models(dataset=dataset, base_model=base_model, tuned_model=tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e33bdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM - Task ID: prithivMLmods/Spam-Text-Detect-Analysis - Subset: None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b7a15",
   "metadata": {},
   "source": [
    "The config of the ESM gets filled with as much metadata as possible from the training process. Feel free to supplement it with relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a96d1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMConfig {\n",
       "  \"base_model_name\": \"google-bert/bert-base-uncased\",\n",
       "  \"developers\": null,\n",
       "  \"esm_architecture\": null,\n",
       "  \"esm_batch_size\": 32,\n",
       "  \"esm_embedding_dim\": null,\n",
       "  \"esm_learning_rate\": 0.001,\n",
       "  \"esm_num_epochs\": 10,\n",
       "  \"esm_optimizer\": null,\n",
       "  \"esm_weight_decay\": 0.01,\n",
       "  \"label_column\": \"Category\",\n",
       "  \"language\": null,\n",
       "  \"lm_batch_size\": null,\n",
       "  \"lm_learning_rate\": null,\n",
       "  \"lm_num_epochs\": null,\n",
       "  \"lm_optimizer\": null,\n",
       "  \"lm_weight_decay\": null,\n",
       "  \"num_examples\": 1000,\n",
       "  \"seed\": null,\n",
       "  \"streamed\": false,\n",
       "  \"task_id\": \"prithivMLmods/Spam-Text-Detect-Analysis\",\n",
       "  \"task_split\": \"train\",\n",
       "  \"task_subset\": null,\n",
       "  \"text_column\": \"Message\",\n",
       "  \"transformers_version\": \"4.47.1\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa235c",
   "metadata": {},
   "source": [
    "# Testing the ESM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f89d1",
   "metadata": {},
   "source": [
    "This step is not necessary for using the ESM. We demonstrate that it succesfully transforms embedding from a 768-dimensional embeddings space to an embedding space in the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e69345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(\n",
    "    dataset[0][\"Message\"],\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "base_embedding = base_model(**tokenized_input)[1]\n",
    "transformed_embedding = esm(base_embedding)\n",
    "\n",
    "print(transformed_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603f06b",
   "metadata": {},
   "source": [
    "# Saving / Publishing the ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07dfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm.to_disk(\"esm.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c22461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb93851962a45ceb5fbd19ad6f313f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "esm.publish(\"davidschulte/ESM__prithivMLmods_Spam-Text-Detect-Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e28f9c",
   "metadata": {},
   "source": [
    "The ESM was succesfully published on the HF Hub and can now be accessed by other users to rank the *Spam-Text-Detect-Analysis* dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
